{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 128\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.14717960357666 | KNN Loss: 6.230565071105957 | BCE Loss: 1.916614294052124\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.178574562072754 | KNN Loss: 6.230663299560547 | BCE Loss: 1.947911262512207\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.182377815246582 | KNN Loss: 6.230414390563965 | BCE Loss: 1.9519634246826172\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.153038024902344 | KNN Loss: 6.230363368988037 | BCE Loss: 1.9226750135421753\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.175795555114746 | KNN Loss: 6.23012638092041 | BCE Loss: 1.945669174194336\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.15682315826416 | KNN Loss: 6.229822635650635 | BCE Loss: 1.9270007610321045\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.178810119628906 | KNN Loss: 6.229875564575195 | BCE Loss: 1.9489349126815796\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.180387496948242 | KNN Loss: 6.229690074920654 | BCE Loss: 1.950697422027588\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.13415813446045 | KNN Loss: 6.229626178741455 | BCE Loss: 1.9045315980911255\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.130496978759766 | KNN Loss: 6.229172229766846 | BCE Loss: 1.9013242721557617\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.138644218444824 | KNN Loss: 6.228997707366943 | BCE Loss: 1.9096461534500122\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.153178215026855 | KNN Loss: 6.228700160980225 | BCE Loss: 1.9244780540466309\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.12009334564209 | KNN Loss: 6.228850841522217 | BCE Loss: 1.8912427425384521\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.115408897399902 | KNN Loss: 6.228509902954102 | BCE Loss: 1.8868993520736694\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.115596771240234 | KNN Loss: 6.228169918060303 | BCE Loss: 1.8874266147613525\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.10545825958252 | KNN Loss: 6.228289604187012 | BCE Loss: 1.8771684169769287\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.080029487609863 | KNN Loss: 6.2283220291137695 | BCE Loss: 1.851707100868225\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.057646751403809 | KNN Loss: 6.228099822998047 | BCE Loss: 1.8295470476150513\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.090126037597656 | KNN Loss: 6.227980136871338 | BCE Loss: 1.8621456623077393\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.078302383422852 | KNN Loss: 6.2275848388671875 | BCE Loss: 1.8507177829742432\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.035868644714355 | KNN Loss: 6.227292537689209 | BCE Loss: 1.8085763454437256\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.1057710647583 | KNN Loss: 6.226873397827148 | BCE Loss: 1.8788975477218628\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.001089096069336 | KNN Loss: 6.226925849914551 | BCE Loss: 1.774163007736206\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.052105903625488 | KNN Loss: 6.226134777069092 | BCE Loss: 1.8259714841842651\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.010909080505371 | KNN Loss: 6.226844787597656 | BCE Loss: 1.7840641736984253\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.005355834960938 | KNN Loss: 6.22622537612915 | BCE Loss: 1.7791306972503662\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.992892265319824 | KNN Loss: 6.225478172302246 | BCE Loss: 1.7674140930175781\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.00381851196289 | KNN Loss: 6.225457668304443 | BCE Loss: 1.7783610820770264\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.971414089202881 | KNN Loss: 6.225132942199707 | BCE Loss: 1.7462810277938843\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 8.001564979553223 | KNN Loss: 6.224727630615234 | BCE Loss: 1.7768374681472778\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.961668968200684 | KNN Loss: 6.2248992919921875 | BCE Loss: 1.736769676208496\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.945729732513428 | KNN Loss: 6.22416877746582 | BCE Loss: 1.721561074256897\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.955648899078369 | KNN Loss: 6.224191188812256 | BCE Loss: 1.7314577102661133\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.928955078125 | KNN Loss: 6.2236647605896 | BCE Loss: 1.7052900791168213\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.862649917602539 | KNN Loss: 6.223042964935303 | BCE Loss: 1.6396067142486572\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.892776966094971 | KNN Loss: 6.223201274871826 | BCE Loss: 1.6695756912231445\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.911470413208008 | KNN Loss: 6.222220420837402 | BCE Loss: 1.689250111579895\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.873140811920166 | KNN Loss: 6.221384048461914 | BCE Loss: 1.651756763458252\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.88552713394165 | KNN Loss: 6.221457004547119 | BCE Loss: 1.6640700101852417\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.841317653656006 | KNN Loss: 6.220376491546631 | BCE Loss: 1.6209410429000854\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.847844123840332 | KNN Loss: 6.22021484375 | BCE Loss: 1.627629280090332\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.81057071685791 | KNN Loss: 6.220099925994873 | BCE Loss: 1.5904709100723267\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.78611421585083 | KNN Loss: 6.219849109649658 | BCE Loss: 1.5662651062011719\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.807696342468262 | KNN Loss: 6.2176833152771 | BCE Loss: 1.5900132656097412\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.735326766967773 | KNN Loss: 6.21823263168335 | BCE Loss: 1.5170942544937134\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.737616539001465 | KNN Loss: 6.21666145324707 | BCE Loss: 1.5209550857543945\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.713335990905762 | KNN Loss: 6.2158379554748535 | BCE Loss: 1.4974980354309082\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.6881608963012695 | KNN Loss: 6.215785980224609 | BCE Loss: 1.472374677658081\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.689878940582275 | KNN Loss: 6.21295166015625 | BCE Loss: 1.4769271612167358\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.632083892822266 | KNN Loss: 6.212264537811279 | BCE Loss: 1.4198194742202759\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.671483516693115 | KNN Loss: 6.212199687957764 | BCE Loss: 1.459283709526062\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.581158638000488 | KNN Loss: 6.209375858306885 | BCE Loss: 1.371782898902893\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.575686454772949 | KNN Loss: 6.2096428871154785 | BCE Loss: 1.3660438060760498\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.559433460235596 | KNN Loss: 6.208233833312988 | BCE Loss: 1.3511996269226074\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.534515380859375 | KNN Loss: 6.205262660980225 | BCE Loss: 1.3292524814605713\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.525074481964111 | KNN Loss: 6.203598976135254 | BCE Loss: 1.3214755058288574\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.490196228027344 | KNN Loss: 6.1988205909729 | BCE Loss: 1.2913756370544434\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.49190616607666 | KNN Loss: 6.196659564971924 | BCE Loss: 1.2952463626861572\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.414733409881592 | KNN Loss: 6.194004535675049 | BCE Loss: 1.2207289934158325\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.433696269989014 | KNN Loss: 6.190783500671387 | BCE Loss: 1.2429126501083374\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.384086608886719 | KNN Loss: 6.187679290771484 | BCE Loss: 1.1964075565338135\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.422611236572266 | KNN Loss: 6.185395240783691 | BCE Loss: 1.2372157573699951\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.34771728515625 | KNN Loss: 6.177994251251221 | BCE Loss: 1.1697232723236084\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.329245090484619 | KNN Loss: 6.173007011413574 | BCE Loss: 1.156238079071045\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.350931167602539 | KNN Loss: 6.168063640594482 | BCE Loss: 1.1828676462173462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.28761625289917 | KNN Loss: 6.1548004150390625 | BCE Loss: 1.1328157186508179\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.281442642211914 | KNN Loss: 6.142515659332275 | BCE Loss: 1.1389267444610596\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.249922275543213 | KNN Loss: 6.137760639190674 | BCE Loss: 1.1121617555618286\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.2522125244140625 | KNN Loss: 6.126208305358887 | BCE Loss: 1.1260042190551758\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.251084327697754 | KNN Loss: 6.1104416847229 | BCE Loss: 1.1406424045562744\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.217717170715332 | KNN Loss: 6.0983428955078125 | BCE Loss: 1.11937415599823\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.160482406616211 | KNN Loss: 6.068140983581543 | BCE Loss: 1.092341423034668\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.165571212768555 | KNN Loss: 6.057934284210205 | BCE Loss: 1.1076366901397705\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.156958103179932 | KNN Loss: 6.044973373413086 | BCE Loss: 1.1119847297668457\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.086623668670654 | KNN Loss: 5.999081134796143 | BCE Loss: 1.0875424146652222\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.055097579956055 | KNN Loss: 5.981715202331543 | BCE Loss: 1.0733823776245117\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.050708770751953 | KNN Loss: 5.960951328277588 | BCE Loss: 1.0897573232650757\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 7.024325370788574 | KNN Loss: 5.918849945068359 | BCE Loss: 1.1054754257202148\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 6.946768760681152 | KNN Loss: 5.879800319671631 | BCE Loss: 1.0669684410095215\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 6.938664436340332 | KNN Loss: 5.846262454986572 | BCE Loss: 1.0924022197723389\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 6.890018463134766 | KNN Loss: 5.8166961669921875 | BCE Loss: 1.0733224153518677\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 6.8462018966674805 | KNN Loss: 5.761923313140869 | BCE Loss: 1.0842785835266113\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 6.812685966491699 | KNN Loss: 5.735325813293457 | BCE Loss: 1.0773600339889526\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 6.747444152832031 | KNN Loss: 5.660495758056641 | BCE Loss: 1.0869483947753906\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 6.714875221252441 | KNN Loss: 5.6404218673706055 | BCE Loss: 1.0744531154632568\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 6.691127777099609 | KNN Loss: 5.603305339813232 | BCE Loss: 1.087822675704956\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 6.67041015625 | KNN Loss: 5.564624309539795 | BCE Loss: 1.1057859659194946\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 6.571804046630859 | KNN Loss: 5.492129325866699 | BCE Loss: 1.0796749591827393\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 6.531124114990234 | KNN Loss: 5.462510585784912 | BCE Loss: 1.0686137676239014\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 6.489626407623291 | KNN Loss: 5.411103248596191 | BCE Loss: 1.0785231590270996\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 6.458183288574219 | KNN Loss: 5.378645896911621 | BCE Loss: 1.0795373916625977\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 6.415783405303955 | KNN Loss: 5.347997665405273 | BCE Loss: 1.0677857398986816\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 6.421905517578125 | KNN Loss: 5.326234817504883 | BCE Loss: 1.0956708192825317\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 6.374550819396973 | KNN Loss: 5.2971320152282715 | BCE Loss: 1.077418565750122\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 6.342942237854004 | KNN Loss: 5.283337593078613 | BCE Loss: 1.0596048831939697\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 6.313749313354492 | KNN Loss: 5.2417426109313965 | BCE Loss: 1.0720065832138062\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.313758850097656 | KNN Loss: 5.233541965484619 | BCE Loss: 1.0802167654037476\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 6.2721757888793945 | KNN Loss: 5.202230453491211 | BCE Loss: 1.0699453353881836\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.298421859741211 | KNN Loss: 5.207606315612793 | BCE Loss: 1.090815544128418\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.261845111846924 | KNN Loss: 5.194321155548096 | BCE Loss: 1.0675239562988281\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.202640533447266 | KNN Loss: 5.166408538818359 | BCE Loss: 1.0362322330474854\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.197764873504639 | KNN Loss: 5.13615608215332 | BCE Loss: 1.0616086721420288\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.22462272644043 | KNN Loss: 5.1454973220825195 | BCE Loss: 1.0791256427764893\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.226205825805664 | KNN Loss: 5.126797199249268 | BCE Loss: 1.0994086265563965\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.185638427734375 | KNN Loss: 5.117987155914307 | BCE Loss: 1.0676510334014893\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.195756912231445 | KNN Loss: 5.13258171081543 | BCE Loss: 1.0631749629974365\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.187647342681885 | KNN Loss: 5.116058826446533 | BCE Loss: 1.0715885162353516\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.177751541137695 | KNN Loss: 5.102514743804932 | BCE Loss: 1.0752370357513428\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.142215728759766 | KNN Loss: 5.10687780380249 | BCE Loss: 1.0353381633758545\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.188324928283691 | KNN Loss: 5.111257553100586 | BCE Loss: 1.0770671367645264\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.1629319190979 | KNN Loss: 5.1082234382629395 | BCE Loss: 1.0547086000442505\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.1712117195129395 | KNN Loss: 5.096427917480469 | BCE Loss: 1.0747836828231812\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.17507791519165 | KNN Loss: 5.1168622970581055 | BCE Loss: 1.0582154989242554\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.131930828094482 | KNN Loss: 5.0840559005737305 | BCE Loss: 1.0478748083114624\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.194794654846191 | KNN Loss: 5.090527057647705 | BCE Loss: 1.1042673587799072\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.182655334472656 | KNN Loss: 5.110410213470459 | BCE Loss: 1.0722448825836182\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.133851051330566 | KNN Loss: 5.086709499359131 | BCE Loss: 1.0471413135528564\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.151904582977295 | KNN Loss: 5.087432861328125 | BCE Loss: 1.06447172164917\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.155475616455078 | KNN Loss: 5.08322286605835 | BCE Loss: 1.072252869606018\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.156773567199707 | KNN Loss: 5.0835466384887695 | BCE Loss: 1.0732271671295166\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.1602888107299805 | KNN Loss: 5.071141719818115 | BCE Loss: 1.0891468524932861\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.136353492736816 | KNN Loss: 5.080451488494873 | BCE Loss: 1.0559017658233643\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.138688564300537 | KNN Loss: 5.082777500152588 | BCE Loss: 1.0559110641479492\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.148664474487305 | KNN Loss: 5.072792053222656 | BCE Loss: 1.0758721828460693\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.136550426483154 | KNN Loss: 5.077690124511719 | BCE Loss: 1.0588603019714355\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.142131805419922 | KNN Loss: 5.069644451141357 | BCE Loss: 1.0724871158599854\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.138890266418457 | KNN Loss: 5.073009014129639 | BCE Loss: 1.0658811330795288\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.162422180175781 | KNN Loss: 5.085824012756348 | BCE Loss: 1.0765984058380127\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.132030487060547 | KNN Loss: 5.064746379852295 | BCE Loss: 1.0672838687896729\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.150122165679932 | KNN Loss: 5.102487087249756 | BCE Loss: 1.0476350784301758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.108476638793945 | KNN Loss: 5.061213493347168 | BCE Loss: 1.0472630262374878\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.104894161224365 | KNN Loss: 5.063118934631348 | BCE Loss: 1.0417752265930176\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.156625747680664 | KNN Loss: 5.079104900360107 | BCE Loss: 1.0775208473205566\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.1539106369018555 | KNN Loss: 5.085153102874756 | BCE Loss: 1.0687572956085205\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.137800216674805 | KNN Loss: 5.079092502593994 | BCE Loss: 1.058707594871521\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.13166618347168 | KNN Loss: 5.064792633056641 | BCE Loss: 1.066873550415039\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.091497421264648 | KNN Loss: 5.0570597648620605 | BCE Loss: 1.034437656402588\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.151091575622559 | KNN Loss: 5.051975250244141 | BCE Loss: 1.099116563796997\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.118348121643066 | KNN Loss: 5.059143543243408 | BCE Loss: 1.0592044591903687\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.1284918785095215 | KNN Loss: 5.07887077331543 | BCE Loss: 1.0496211051940918\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.080065727233887 | KNN Loss: 5.055420398712158 | BCE Loss: 1.0246450901031494\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.119192123413086 | KNN Loss: 5.054752826690674 | BCE Loss: 1.064439296722412\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.1125946044921875 | KNN Loss: 5.050389766693115 | BCE Loss: 1.0622045993804932\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.1073899269104 | KNN Loss: 5.058563709259033 | BCE Loss: 1.0488262176513672\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.0969462394714355 | KNN Loss: 5.0529327392578125 | BCE Loss: 1.0440136194229126\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.103913307189941 | KNN Loss: 5.054111957550049 | BCE Loss: 1.0498015880584717\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.143928050994873 | KNN Loss: 5.067213535308838 | BCE Loss: 1.0767145156860352\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.100867748260498 | KNN Loss: 5.060390472412109 | BCE Loss: 1.0404772758483887\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.125216960906982 | KNN Loss: 5.057624816894531 | BCE Loss: 1.0675921440124512\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.114813804626465 | KNN Loss: 5.052191257476807 | BCE Loss: 1.0626225471496582\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.114073753356934 | KNN Loss: 5.060897350311279 | BCE Loss: 1.0531761646270752\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.099362850189209 | KNN Loss: 5.053415298461914 | BCE Loss: 1.0459476709365845\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.112799167633057 | KNN Loss: 5.063555717468262 | BCE Loss: 1.049243450164795\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.144500255584717 | KNN Loss: 5.0976643562316895 | BCE Loss: 1.0468358993530273\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.1154704093933105 | KNN Loss: 5.055138111114502 | BCE Loss: 1.0603324174880981\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.108926296234131 | KNN Loss: 5.04703426361084 | BCE Loss: 1.0618921518325806\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.0991339683532715 | KNN Loss: 5.052910804748535 | BCE Loss: 1.0462231636047363\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.128941059112549 | KNN Loss: 5.0738444328308105 | BCE Loss: 1.0550965070724487\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.106311798095703 | KNN Loss: 5.067409515380859 | BCE Loss: 1.0389022827148438\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.103701114654541 | KNN Loss: 5.061666011810303 | BCE Loss: 1.0420352220535278\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.131917953491211 | KNN Loss: 5.064396858215332 | BCE Loss: 1.067521095275879\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.119407653808594 | KNN Loss: 5.0559587478637695 | BCE Loss: 1.0634489059448242\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.11741828918457 | KNN Loss: 5.047075271606445 | BCE Loss: 1.070343255996704\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.12155818939209 | KNN Loss: 5.051902770996094 | BCE Loss: 1.069655418395996\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.080246448516846 | KNN Loss: 5.048228740692139 | BCE Loss: 1.032017707824707\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.10737419128418 | KNN Loss: 5.053591728210449 | BCE Loss: 1.0537827014923096\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.094769477844238 | KNN Loss: 5.047030925750732 | BCE Loss: 1.0477383136749268\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.11978816986084 | KNN Loss: 5.073034763336182 | BCE Loss: 1.0467536449432373\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.103460788726807 | KNN Loss: 5.051334381103516 | BCE Loss: 1.052126407623291\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.090792179107666 | KNN Loss: 5.053929328918457 | BCE Loss: 1.0368627309799194\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.097726821899414 | KNN Loss: 5.052648067474365 | BCE Loss: 1.0450786352157593\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.102198600769043 | KNN Loss: 5.069789409637451 | BCE Loss: 1.0324090719223022\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.0971174240112305 | KNN Loss: 5.043797492980957 | BCE Loss: 1.0533196926116943\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.13845682144165 | KNN Loss: 5.0723090171813965 | BCE Loss: 1.0661479234695435\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.106948375701904 | KNN Loss: 5.047224521636963 | BCE Loss: 1.059723973274231\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.103740692138672 | KNN Loss: 5.050779342651367 | BCE Loss: 1.0529615879058838\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.109868049621582 | KNN Loss: 5.059504508972168 | BCE Loss: 1.050363302230835\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.145285606384277 | KNN Loss: 5.054509162902832 | BCE Loss: 1.0907765626907349\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.114326000213623 | KNN Loss: 5.041723251342773 | BCE Loss: 1.0726027488708496\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.116057395935059 | KNN Loss: 5.051858425140381 | BCE Loss: 1.0641989707946777\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.128212928771973 | KNN Loss: 5.045201778411865 | BCE Loss: 1.0830111503601074\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.11044979095459 | KNN Loss: 5.0593180656433105 | BCE Loss: 1.0511319637298584\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.13790225982666 | KNN Loss: 5.061446189880371 | BCE Loss: 1.07645583152771\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.109652519226074 | KNN Loss: 5.05148458480835 | BCE Loss: 1.0581681728363037\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.096798419952393 | KNN Loss: 5.047729015350342 | BCE Loss: 1.0490694046020508\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.087681770324707 | KNN Loss: 5.044200420379639 | BCE Loss: 1.0434811115264893\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.083728790283203 | KNN Loss: 5.041885852813721 | BCE Loss: 1.0418429374694824\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.117372512817383 | KNN Loss: 5.045923233032227 | BCE Loss: 1.0714491605758667\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.093327522277832 | KNN Loss: 5.04453182220459 | BCE Loss: 1.0487955808639526\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.109033584594727 | KNN Loss: 5.051684856414795 | BCE Loss: 1.0573489665985107\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.131660461425781 | KNN Loss: 5.0517072677612305 | BCE Loss: 1.0799534320831299\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.063628673553467 | KNN Loss: 5.040090560913086 | BCE Loss: 1.0235381126403809\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.102351188659668 | KNN Loss: 5.043825626373291 | BCE Loss: 1.058525800704956\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.072518348693848 | KNN Loss: 5.04636812210083 | BCE Loss: 1.0261499881744385\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.1111907958984375 | KNN Loss: 5.049165725708008 | BCE Loss: 1.0620250701904297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.107427597045898 | KNN Loss: 5.051238536834717 | BCE Loss: 1.0561891794204712\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.115278720855713 | KNN Loss: 5.04813814163208 | BCE Loss: 1.0671404600143433\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.083350658416748 | KNN Loss: 5.041615009307861 | BCE Loss: 1.0417355298995972\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.092284202575684 | KNN Loss: 5.055273532867432 | BCE Loss: 1.037010669708252\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.086962699890137 | KNN Loss: 5.048601150512695 | BCE Loss: 1.0383614301681519\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.096273422241211 | KNN Loss: 5.04991340637207 | BCE Loss: 1.0463597774505615\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.074934959411621 | KNN Loss: 5.046614646911621 | BCE Loss: 1.0283203125\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.096368789672852 | KNN Loss: 5.045067310333252 | BCE Loss: 1.0513012409210205\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.108048439025879 | KNN Loss: 5.055100917816162 | BCE Loss: 1.0529475212097168\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.097046852111816 | KNN Loss: 5.045405387878418 | BCE Loss: 1.0516414642333984\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.101112365722656 | KNN Loss: 5.069579124450684 | BCE Loss: 1.0315334796905518\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.085559844970703 | KNN Loss: 5.0515642166137695 | BCE Loss: 1.0339956283569336\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.08096981048584 | KNN Loss: 5.049998760223389 | BCE Loss: 1.030970811843872\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.070062637329102 | KNN Loss: 5.042183876037598 | BCE Loss: 1.027878761291504\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.11933708190918 | KNN Loss: 5.0413384437561035 | BCE Loss: 1.0779988765716553\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.118109703063965 | KNN Loss: 5.066016674041748 | BCE Loss: 1.0520927906036377\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.0868306159973145 | KNN Loss: 5.056480407714844 | BCE Loss: 1.0303503274917603\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.109395980834961 | KNN Loss: 5.067877769470215 | BCE Loss: 1.0415184497833252\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.105348587036133 | KNN Loss: 5.0608229637146 | BCE Loss: 1.0445257425308228\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.1156744956970215 | KNN Loss: 5.04640531539917 | BCE Loss: 1.0692691802978516\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.101474761962891 | KNN Loss: 5.044010639190674 | BCE Loss: 1.0574638843536377\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.110111236572266 | KNN Loss: 5.051167011260986 | BCE Loss: 1.0589444637298584\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.102442264556885 | KNN Loss: 5.055775165557861 | BCE Loss: 1.046667218208313\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.092616558074951 | KNN Loss: 5.052194595336914 | BCE Loss: 1.040421962738037\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.092113971710205 | KNN Loss: 5.04520320892334 | BCE Loss: 1.0469107627868652\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.090242862701416 | KNN Loss: 5.046869277954102 | BCE Loss: 1.0433735847473145\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.105067729949951 | KNN Loss: 5.042906284332275 | BCE Loss: 1.0621615648269653\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.080386161804199 | KNN Loss: 5.0398712158203125 | BCE Loss: 1.0405149459838867\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.12681770324707 | KNN Loss: 5.050756931304932 | BCE Loss: 1.0760606527328491\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.098977088928223 | KNN Loss: 5.055689334869385 | BCE Loss: 1.0432876348495483\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.089102268218994 | KNN Loss: 5.050619125366211 | BCE Loss: 1.0384831428527832\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.081780433654785 | KNN Loss: 5.049314022064209 | BCE Loss: 1.0324662923812866\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.09702205657959 | KNN Loss: 5.0456037521362305 | BCE Loss: 1.0514183044433594\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.100210189819336 | KNN Loss: 5.050122261047363 | BCE Loss: 1.0500876903533936\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.12580680847168 | KNN Loss: 5.0476908683776855 | BCE Loss: 1.0781159400939941\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.124327659606934 | KNN Loss: 5.052317142486572 | BCE Loss: 1.0720107555389404\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.107443809509277 | KNN Loss: 5.0439653396606445 | BCE Loss: 1.0634782314300537\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.104124069213867 | KNN Loss: 5.050309658050537 | BCE Loss: 1.053814172744751\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.114683151245117 | KNN Loss: 5.045074462890625 | BCE Loss: 1.0696086883544922\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.060358047485352 | KNN Loss: 5.043758392333984 | BCE Loss: 1.0165996551513672\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.093903064727783 | KNN Loss: 5.055004119873047 | BCE Loss: 1.0388988256454468\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.097949981689453 | KNN Loss: 5.045958518981934 | BCE Loss: 1.0519917011260986\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.0849785804748535 | KNN Loss: 5.044410228729248 | BCE Loss: 1.040568232536316\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.098881721496582 | KNN Loss: 5.052095890045166 | BCE Loss: 1.0467859506607056\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.0964250564575195 | KNN Loss: 5.042277812957764 | BCE Loss: 1.0541470050811768\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.1120734214782715 | KNN Loss: 5.060776233673096 | BCE Loss: 1.0512971878051758\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.136571884155273 | KNN Loss: 5.048612594604492 | BCE Loss: 1.0879592895507812\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.11898946762085 | KNN Loss: 5.05837869644165 | BCE Loss: 1.0606106519699097\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.0915446281433105 | KNN Loss: 5.054651737213135 | BCE Loss: 1.0368928909301758\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.071722984313965 | KNN Loss: 5.043115139007568 | BCE Loss: 1.0286080837249756\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.091336250305176 | KNN Loss: 5.040008068084717 | BCE Loss: 1.051328182220459\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.101302146911621 | KNN Loss: 5.04403018951416 | BCE Loss: 1.0572717189788818\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.116147994995117 | KNN Loss: 5.046219348907471 | BCE Loss: 1.069928526878357\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.082724571228027 | KNN Loss: 5.043936252593994 | BCE Loss: 1.0387883186340332\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.071752071380615 | KNN Loss: 5.043332099914551 | BCE Loss: 1.0284199714660645\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.106503486633301 | KNN Loss: 5.050835609436035 | BCE Loss: 1.0556681156158447\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.101776123046875 | KNN Loss: 5.046468257904053 | BCE Loss: 1.0553081035614014\n",
      "Epoch    42: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.114842414855957 | KNN Loss: 5.056535243988037 | BCE Loss: 1.0583069324493408\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.125096797943115 | KNN Loss: 5.053639888763428 | BCE Loss: 1.071457028388977\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.084655284881592 | KNN Loss: 5.051424026489258 | BCE Loss: 1.0332313776016235\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.076232433319092 | KNN Loss: 5.043359279632568 | BCE Loss: 1.032873272895813\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.078438758850098 | KNN Loss: 5.044858455657959 | BCE Loss: 1.0335805416107178\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.0600690841674805 | KNN Loss: 5.044887065887451 | BCE Loss: 1.0151818990707397\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.092328071594238 | KNN Loss: 5.045749664306641 | BCE Loss: 1.0465786457061768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.062371730804443 | KNN Loss: 5.03947639465332 | BCE Loss: 1.0228954553604126\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.0904436111450195 | KNN Loss: 5.043878555297852 | BCE Loss: 1.0465651750564575\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.092868804931641 | KNN Loss: 5.044665336608887 | BCE Loss: 1.048203468322754\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.085016250610352 | KNN Loss: 5.041781425476074 | BCE Loss: 1.0432350635528564\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.1180877685546875 | KNN Loss: 5.065965175628662 | BCE Loss: 1.0521225929260254\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.081908226013184 | KNN Loss: 5.034112930297852 | BCE Loss: 1.0477955341339111\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.106303691864014 | KNN Loss: 5.046926498413086 | BCE Loss: 1.0593771934509277\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.1107258796691895 | KNN Loss: 5.0440263748168945 | BCE Loss: 1.066699504852295\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.106687545776367 | KNN Loss: 5.043073654174805 | BCE Loss: 1.0636138916015625\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.079429626464844 | KNN Loss: 5.046481132507324 | BCE Loss: 1.0329484939575195\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.09352445602417 | KNN Loss: 5.0404253005981445 | BCE Loss: 1.0530990362167358\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.124999046325684 | KNN Loss: 5.053914546966553 | BCE Loss: 1.0710844993591309\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.063513278961182 | KNN Loss: 5.039020538330078 | BCE Loss: 1.0244927406311035\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.071615219116211 | KNN Loss: 5.050938129425049 | BCE Loss: 1.0206772089004517\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.0810322761535645 | KNN Loss: 5.054370880126953 | BCE Loss: 1.0266613960266113\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.076165199279785 | KNN Loss: 5.042309284210205 | BCE Loss: 1.033855676651001\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.107868194580078 | KNN Loss: 5.036910057067871 | BCE Loss: 1.0709582567214966\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.067939758300781 | KNN Loss: 5.036278247833252 | BCE Loss: 1.0316613912582397\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.124249458312988 | KNN Loss: 5.057435035705566 | BCE Loss: 1.066814661026001\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.1006340980529785 | KNN Loss: 5.049172401428223 | BCE Loss: 1.0514616966247559\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.076643943786621 | KNN Loss: 5.043228626251221 | BCE Loss: 1.0334151983261108\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.099465370178223 | KNN Loss: 5.042459011077881 | BCE Loss: 1.0570061206817627\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.076132297515869 | KNN Loss: 5.040218830108643 | BCE Loss: 1.0359135866165161\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.095102310180664 | KNN Loss: 5.0440287590026855 | BCE Loss: 1.051073670387268\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.113397121429443 | KNN Loss: 5.044825077056885 | BCE Loss: 1.0685720443725586\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.123626708984375 | KNN Loss: 5.052833557128906 | BCE Loss: 1.0707932710647583\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.0724897384643555 | KNN Loss: 5.059383392333984 | BCE Loss: 1.013106346130371\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.108464241027832 | KNN Loss: 5.04878568649292 | BCE Loss: 1.059678554534912\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.073935508728027 | KNN Loss: 5.032492160797119 | BCE Loss: 1.0414435863494873\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.086282253265381 | KNN Loss: 5.042815208435059 | BCE Loss: 1.0434670448303223\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.119868278503418 | KNN Loss: 5.0453200340271 | BCE Loss: 1.0745480060577393\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.090722560882568 | KNN Loss: 5.044340133666992 | BCE Loss: 1.0463825464248657\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.104197025299072 | KNN Loss: 5.05085563659668 | BCE Loss: 1.053341269493103\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.099294662475586 | KNN Loss: 5.035881996154785 | BCE Loss: 1.0634124279022217\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.141468048095703 | KNN Loss: 5.0411858558654785 | BCE Loss: 1.1002819538116455\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.086300849914551 | KNN Loss: 5.0444746017456055 | BCE Loss: 1.0418262481689453\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.082015037536621 | KNN Loss: 5.040383815765381 | BCE Loss: 1.0416312217712402\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.102724552154541 | KNN Loss: 5.051120281219482 | BCE Loss: 1.0516042709350586\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.084682464599609 | KNN Loss: 5.042384147644043 | BCE Loss: 1.042298436164856\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.10966157913208 | KNN Loss: 5.044230937957764 | BCE Loss: 1.0654305219650269\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.103437900543213 | KNN Loss: 5.041418552398682 | BCE Loss: 1.0620192289352417\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.081006050109863 | KNN Loss: 5.045822620391846 | BCE Loss: 1.0351835489273071\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.100947380065918 | KNN Loss: 5.040278434753418 | BCE Loss: 1.060669183731079\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.093416690826416 | KNN Loss: 5.047950267791748 | BCE Loss: 1.0454663038253784\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.105460166931152 | KNN Loss: 5.03956413269043 | BCE Loss: 1.0658961534500122\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.0915021896362305 | KNN Loss: 5.044320106506348 | BCE Loss: 1.0471819639205933\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.1054229736328125 | KNN Loss: 5.040519714355469 | BCE Loss: 1.0649032592773438\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.1142449378967285 | KNN Loss: 5.051594257354736 | BCE Loss: 1.0626506805419922\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.111589431762695 | KNN Loss: 5.050264358520508 | BCE Loss: 1.0613253116607666\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.078471660614014 | KNN Loss: 5.052475452423096 | BCE Loss: 1.0259960889816284\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.069132328033447 | KNN Loss: 5.041967391967773 | BCE Loss: 1.0271649360656738\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.078143119812012 | KNN Loss: 5.0433807373046875 | BCE Loss: 1.0347626209259033\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.071322441101074 | KNN Loss: 5.0365729331970215 | BCE Loss: 1.0347497463226318\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.07862663269043 | KNN Loss: 5.041586875915527 | BCE Loss: 1.0370399951934814\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.085261344909668 | KNN Loss: 5.044658184051514 | BCE Loss: 1.0406033992767334\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.109833240509033 | KNN Loss: 5.0695319175720215 | BCE Loss: 1.0403012037277222\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.134398460388184 | KNN Loss: 5.064855098724365 | BCE Loss: 1.0695433616638184\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.144025802612305 | KNN Loss: 5.044066905975342 | BCE Loss: 1.0999586582183838\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.069746971130371 | KNN Loss: 5.043554782867432 | BCE Loss: 1.0261924266815186\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.100706577301025 | KNN Loss: 5.043272972106934 | BCE Loss: 1.0574337244033813\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.051299095153809 | KNN Loss: 5.039450168609619 | BCE Loss: 1.0118486881256104\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.0908660888671875 | KNN Loss: 5.0391459465026855 | BCE Loss: 1.0517202615737915\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.077577590942383 | KNN Loss: 5.037694454193115 | BCE Loss: 1.0398833751678467\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.095368385314941 | KNN Loss: 5.043476581573486 | BCE Loss: 1.051891565322876\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.112925052642822 | KNN Loss: 5.044351100921631 | BCE Loss: 1.0685738325119019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.096177577972412 | KNN Loss: 5.045533180236816 | BCE Loss: 1.0506443977355957\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.091556549072266 | KNN Loss: 5.043697834014893 | BCE Loss: 1.0478585958480835\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.100545883178711 | KNN Loss: 5.044763565063477 | BCE Loss: 1.0557821989059448\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.096068382263184 | KNN Loss: 5.0420660972595215 | BCE Loss: 1.0540025234222412\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.108269691467285 | KNN Loss: 5.038491249084473 | BCE Loss: 1.0697782039642334\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.089432716369629 | KNN Loss: 5.039303779602051 | BCE Loss: 1.050128698348999\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.113760471343994 | KNN Loss: 5.051718235015869 | BCE Loss: 1.062042236328125\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.082190990447998 | KNN Loss: 5.049178123474121 | BCE Loss: 1.0330129861831665\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.0546674728393555 | KNN Loss: 5.037201404571533 | BCE Loss: 1.0174663066864014\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.100749492645264 | KNN Loss: 5.038026809692383 | BCE Loss: 1.0627228021621704\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.0752739906311035 | KNN Loss: 5.0478692054748535 | BCE Loss: 1.02740478515625\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.076272487640381 | KNN Loss: 5.039253234863281 | BCE Loss: 1.0370193719863892\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.081066131591797 | KNN Loss: 5.041024208068848 | BCE Loss: 1.0400419235229492\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.132837295532227 | KNN Loss: 5.054145812988281 | BCE Loss: 1.0786916017532349\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.091150283813477 | KNN Loss: 5.047372817993164 | BCE Loss: 1.043777585029602\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.08575963973999 | KNN Loss: 5.04749870300293 | BCE Loss: 1.03826105594635\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.088492393493652 | KNN Loss: 5.041684627532959 | BCE Loss: 1.0468080043792725\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.055944442749023 | KNN Loss: 5.033078193664551 | BCE Loss: 1.0228664875030518\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.108344078063965 | KNN Loss: 5.045399188995361 | BCE Loss: 1.0629446506500244\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.083896636962891 | KNN Loss: 5.034940719604492 | BCE Loss: 1.0489561557769775\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.082845687866211 | KNN Loss: 5.049814224243164 | BCE Loss: 1.033031702041626\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.094777584075928 | KNN Loss: 5.036060810089111 | BCE Loss: 1.0587167739868164\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.086265563964844 | KNN Loss: 5.047142505645752 | BCE Loss: 1.039123296737671\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.062568664550781 | KNN Loss: 5.034961223602295 | BCE Loss: 1.0276076793670654\n",
      "Epoch    58: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.081633567810059 | KNN Loss: 5.031994342803955 | BCE Loss: 1.049639344215393\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.0716986656188965 | KNN Loss: 5.036316394805908 | BCE Loss: 1.0353822708129883\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.12474250793457 | KNN Loss: 5.058398723602295 | BCE Loss: 1.0663440227508545\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.100174427032471 | KNN Loss: 5.052872180938721 | BCE Loss: 1.0473021268844604\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.1114420890808105 | KNN Loss: 5.039499282836914 | BCE Loss: 1.071942687034607\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.079150199890137 | KNN Loss: 5.0346221923828125 | BCE Loss: 1.0445282459259033\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.085927963256836 | KNN Loss: 5.037878513336182 | BCE Loss: 1.0480494499206543\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.100858688354492 | KNN Loss: 5.041381359100342 | BCE Loss: 1.0594772100448608\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.076892852783203 | KNN Loss: 5.044233322143555 | BCE Loss: 1.0326595306396484\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.049638271331787 | KNN Loss: 5.037785053253174 | BCE Loss: 1.0118533372879028\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.112058162689209 | KNN Loss: 5.042779445648193 | BCE Loss: 1.069278597831726\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.0807037353515625 | KNN Loss: 5.032764434814453 | BCE Loss: 1.0479393005371094\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.100419998168945 | KNN Loss: 5.0494489669799805 | BCE Loss: 1.0509707927703857\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.093132019042969 | KNN Loss: 5.036046981811523 | BCE Loss: 1.0570849180221558\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.112118244171143 | KNN Loss: 5.052891731262207 | BCE Loss: 1.059226632118225\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.067313194274902 | KNN Loss: 5.037203311920166 | BCE Loss: 1.0301098823547363\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.109723091125488 | KNN Loss: 5.047419548034668 | BCE Loss: 1.0623036623001099\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.097464561462402 | KNN Loss: 5.041873455047607 | BCE Loss: 1.0555908679962158\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.079754829406738 | KNN Loss: 5.038034439086914 | BCE Loss: 1.0417202711105347\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.144550323486328 | KNN Loss: 5.061225414276123 | BCE Loss: 1.083324909210205\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.096278667449951 | KNN Loss: 5.043405055999756 | BCE Loss: 1.0528736114501953\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.0805487632751465 | KNN Loss: 5.058273792266846 | BCE Loss: 1.0222749710083008\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.0863447189331055 | KNN Loss: 5.041175842285156 | BCE Loss: 1.0451691150665283\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.064626693725586 | KNN Loss: 5.039196014404297 | BCE Loss: 1.0254309177398682\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.071840286254883 | KNN Loss: 5.029677867889404 | BCE Loss: 1.0421621799468994\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.088013648986816 | KNN Loss: 5.041860580444336 | BCE Loss: 1.0461533069610596\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.098118782043457 | KNN Loss: 5.032510280609131 | BCE Loss: 1.0656087398529053\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.082371234893799 | KNN Loss: 5.056350231170654 | BCE Loss: 1.0260210037231445\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.152955055236816 | KNN Loss: 5.084573268890381 | BCE Loss: 1.0683817863464355\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.0999836921691895 | KNN Loss: 5.048908233642578 | BCE Loss: 1.0510755777359009\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.106874465942383 | KNN Loss: 5.040475845336914 | BCE Loss: 1.0663983821868896\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.072300910949707 | KNN Loss: 5.034426212310791 | BCE Loss: 1.0378749370574951\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.100076675415039 | KNN Loss: 5.047184467315674 | BCE Loss: 1.0528922080993652\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.126673698425293 | KNN Loss: 5.051682949066162 | BCE Loss: 1.0749908685684204\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.1283183097839355 | KNN Loss: 5.046923637390137 | BCE Loss: 1.0813946723937988\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.073044776916504 | KNN Loss: 5.0393385887146 | BCE Loss: 1.0337059497833252\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.072940349578857 | KNN Loss: 5.038411617279053 | BCE Loss: 1.0345286130905151\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.074825286865234 | KNN Loss: 5.035009384155273 | BCE Loss: 1.0398156642913818\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.078561782836914 | KNN Loss: 5.034733772277832 | BCE Loss: 1.0438278913497925\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.111866474151611 | KNN Loss: 5.048316478729248 | BCE Loss: 1.0635499954223633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.087122917175293 | KNN Loss: 5.052177906036377 | BCE Loss: 1.0349452495574951\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.084155082702637 | KNN Loss: 5.036402702331543 | BCE Loss: 1.0477523803710938\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.0978593826293945 | KNN Loss: 5.036871910095215 | BCE Loss: 1.0609874725341797\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.061302661895752 | KNN Loss: 5.0426812171936035 | BCE Loss: 1.0186213254928589\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.076380729675293 | KNN Loss: 5.037388801574707 | BCE Loss: 1.038992166519165\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.095370292663574 | KNN Loss: 5.056882381439209 | BCE Loss: 1.0384880304336548\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.104027271270752 | KNN Loss: 5.064815998077393 | BCE Loss: 1.0392112731933594\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.106443405151367 | KNN Loss: 5.036929607391357 | BCE Loss: 1.0695140361785889\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.073327541351318 | KNN Loss: 5.048219680786133 | BCE Loss: 1.025107741355896\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.05534553527832 | KNN Loss: 5.036223411560059 | BCE Loss: 1.0191223621368408\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.089069366455078 | KNN Loss: 5.03153657913208 | BCE Loss: 1.057532787322998\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.077350616455078 | KNN Loss: 5.043325901031494 | BCE Loss: 1.034024715423584\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.10254430770874 | KNN Loss: 5.028481960296631 | BCE Loss: 1.0740623474121094\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.099799156188965 | KNN Loss: 5.043481826782227 | BCE Loss: 1.0563174486160278\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.085319519042969 | KNN Loss: 5.046732425689697 | BCE Loss: 1.0385868549346924\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.121886253356934 | KNN Loss: 5.050197601318359 | BCE Loss: 1.0716888904571533\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.087121486663818 | KNN Loss: 5.035882472991943 | BCE Loss: 1.0512388944625854\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.100837230682373 | KNN Loss: 5.042068004608154 | BCE Loss: 1.0587691068649292\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.128414154052734 | KNN Loss: 5.062965393066406 | BCE Loss: 1.0654489994049072\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.075284481048584 | KNN Loss: 5.04039192199707 | BCE Loss: 1.0348924398422241\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.080299377441406 | KNN Loss: 5.050793647766113 | BCE Loss: 1.029505968093872\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.096331596374512 | KNN Loss: 5.044673442840576 | BCE Loss: 1.0516579151153564\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.0990095138549805 | KNN Loss: 5.036596775054932 | BCE Loss: 1.0624126195907593\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.128805637359619 | KNN Loss: 5.07138204574585 | BCE Loss: 1.0574235916137695\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.0865607261657715 | KNN Loss: 5.042069435119629 | BCE Loss: 1.0444914102554321\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.059784412384033 | KNN Loss: 5.031617641448975 | BCE Loss: 1.0281667709350586\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.0680131912231445 | KNN Loss: 5.04703426361084 | BCE Loss: 1.0209790468215942\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.092284679412842 | KNN Loss: 5.0387091636657715 | BCE Loss: 1.0535753965377808\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.069291114807129 | KNN Loss: 5.04066801071167 | BCE Loss: 1.0286232233047485\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.123435020446777 | KNN Loss: 5.047757148742676 | BCE Loss: 1.0756781101226807\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.107263088226318 | KNN Loss: 5.041251182556152 | BCE Loss: 1.066011905670166\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.107444763183594 | KNN Loss: 5.032092094421387 | BCE Loss: 1.0753529071807861\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.103892803192139 | KNN Loss: 5.058532238006592 | BCE Loss: 1.0453605651855469\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.0930585861206055 | KNN Loss: 5.039165496826172 | BCE Loss: 1.0538930892944336\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.100276470184326 | KNN Loss: 5.0383219718933105 | BCE Loss: 1.0619544982910156\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.105175971984863 | KNN Loss: 5.033970355987549 | BCE Loss: 1.0712058544158936\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.065688133239746 | KNN Loss: 5.029192924499512 | BCE Loss: 1.0364954471588135\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.085141181945801 | KNN Loss: 5.037394046783447 | BCE Loss: 1.0477471351623535\n",
      "Epoch    71: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.110979080200195 | KNN Loss: 5.03162145614624 | BCE Loss: 1.0793578624725342\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.089992523193359 | KNN Loss: 5.03983736038208 | BCE Loss: 1.0501549243927002\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.067666053771973 | KNN Loss: 5.031771659851074 | BCE Loss: 1.0358946323394775\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.078231334686279 | KNN Loss: 5.039942264556885 | BCE Loss: 1.0382890701293945\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.06403112411499 | KNN Loss: 5.037436008453369 | BCE Loss: 1.026595115661621\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.0849995613098145 | KNN Loss: 5.033907413482666 | BCE Loss: 1.051092267036438\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.062819480895996 | KNN Loss: 5.033910751342773 | BCE Loss: 1.0289088487625122\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.051570892333984 | KNN Loss: 5.029475688934326 | BCE Loss: 1.0220954418182373\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.107593536376953 | KNN Loss: 5.032397747039795 | BCE Loss: 1.0751960277557373\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.073699951171875 | KNN Loss: 5.032296180725098 | BCE Loss: 1.0414037704467773\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.0755462646484375 | KNN Loss: 5.032754421234131 | BCE Loss: 1.0427918434143066\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.085007667541504 | KNN Loss: 5.050962448120117 | BCE Loss: 1.0340449810028076\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.06418514251709 | KNN Loss: 5.034546375274658 | BCE Loss: 1.0296390056610107\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.0809431076049805 | KNN Loss: 5.043069839477539 | BCE Loss: 1.0378735065460205\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.07639217376709 | KNN Loss: 5.039239883422852 | BCE Loss: 1.0371522903442383\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.077818393707275 | KNN Loss: 5.041889190673828 | BCE Loss: 1.0359290838241577\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.071164608001709 | KNN Loss: 5.048989295959473 | BCE Loss: 1.0221751928329468\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.077536106109619 | KNN Loss: 5.054511070251465 | BCE Loss: 1.0230251550674438\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.094855308532715 | KNN Loss: 5.0383076667785645 | BCE Loss: 1.0565475225448608\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.105772018432617 | KNN Loss: 5.030942440032959 | BCE Loss: 1.0748294591903687\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.052511692047119 | KNN Loss: 5.046781539916992 | BCE Loss: 1.005730152130127\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.087771892547607 | KNN Loss: 5.033281326293945 | BCE Loss: 1.054490566253662\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.077448844909668 | KNN Loss: 5.033425807952881 | BCE Loss: 1.044023036956787\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.098994255065918 | KNN Loss: 5.046968460083008 | BCE Loss: 1.0520259141921997\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.100628852844238 | KNN Loss: 5.032475471496582 | BCE Loss: 1.0681532621383667\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.073862552642822 | KNN Loss: 5.036123275756836 | BCE Loss: 1.0377391576766968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.0768232345581055 | KNN Loss: 5.051542282104492 | BCE Loss: 1.0252811908721924\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.061604976654053 | KNN Loss: 5.036231517791748 | BCE Loss: 1.0253734588623047\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.075700759887695 | KNN Loss: 5.031344413757324 | BCE Loss: 1.0443565845489502\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.0973687171936035 | KNN Loss: 5.049161434173584 | BCE Loss: 1.048207402229309\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.086277961730957 | KNN Loss: 5.052222728729248 | BCE Loss: 1.0340549945831299\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.085200309753418 | KNN Loss: 5.037827014923096 | BCE Loss: 1.0473735332489014\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.088960647583008 | KNN Loss: 5.034401893615723 | BCE Loss: 1.0545589923858643\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.091104507446289 | KNN Loss: 5.034475326538086 | BCE Loss: 1.0566293001174927\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.054037094116211 | KNN Loss: 5.031445503234863 | BCE Loss: 1.0225913524627686\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.079694747924805 | KNN Loss: 5.053164958953857 | BCE Loss: 1.0265299081802368\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.101530075073242 | KNN Loss: 5.033092498779297 | BCE Loss: 1.0684373378753662\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.133140563964844 | KNN Loss: 5.077838897705078 | BCE Loss: 1.0553019046783447\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.075037956237793 | KNN Loss: 5.033457279205322 | BCE Loss: 1.0415807962417603\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.087411403656006 | KNN Loss: 5.037132740020752 | BCE Loss: 1.0502785444259644\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.0839972496032715 | KNN Loss: 5.036244869232178 | BCE Loss: 1.0477524995803833\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.076424598693848 | KNN Loss: 5.036552906036377 | BCE Loss: 1.0398714542388916\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.090846538543701 | KNN Loss: 5.044483661651611 | BCE Loss: 1.0463629961013794\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.082763195037842 | KNN Loss: 5.035858154296875 | BCE Loss: 1.0469049215316772\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.090707778930664 | KNN Loss: 5.040414333343506 | BCE Loss: 1.0502934455871582\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.086443901062012 | KNN Loss: 5.038200378417969 | BCE Loss: 1.048243761062622\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.045608997344971 | KNN Loss: 5.0332231521606445 | BCE Loss: 1.0123858451843262\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.120330810546875 | KNN Loss: 5.044830322265625 | BCE Loss: 1.075500249862671\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.052463054656982 | KNN Loss: 5.029046058654785 | BCE Loss: 1.0234169960021973\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.110432147979736 | KNN Loss: 5.063683032989502 | BCE Loss: 1.0467491149902344\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.112137317657471 | KNN Loss: 5.038473606109619 | BCE Loss: 1.0736637115478516\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.061907768249512 | KNN Loss: 5.0403523445129395 | BCE Loss: 1.0215554237365723\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.092551231384277 | KNN Loss: 5.037692546844482 | BCE Loss: 1.054858684539795\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.076120376586914 | KNN Loss: 5.039388179779053 | BCE Loss: 1.0367321968078613\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.032750129699707 | KNN Loss: 5.040858745574951 | BCE Loss: 0.9918912053108215\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.055555820465088 | KNN Loss: 5.028291702270508 | BCE Loss: 1.02726411819458\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.114052772521973 | KNN Loss: 5.027191638946533 | BCE Loss: 1.086861252784729\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.070744514465332 | KNN Loss: 5.029599189758301 | BCE Loss: 1.0411453247070312\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.083569049835205 | KNN Loss: 5.042574405670166 | BCE Loss: 1.040994644165039\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.074491500854492 | KNN Loss: 5.032362461090088 | BCE Loss: 1.0421289205551147\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.11776065826416 | KNN Loss: 5.0599045753479 | BCE Loss: 1.0578563213348389\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.082655906677246 | KNN Loss: 5.038528919219971 | BCE Loss: 1.044127106666565\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.063426971435547 | KNN Loss: 5.037224292755127 | BCE Loss: 1.0262027978897095\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.113120079040527 | KNN Loss: 5.035506248474121 | BCE Loss: 1.0776138305664062\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.083471298217773 | KNN Loss: 5.036525249481201 | BCE Loss: 1.0469462871551514\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.057729721069336 | KNN Loss: 5.035045623779297 | BCE Loss: 1.0226843357086182\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.046784400939941 | KNN Loss: 5.028000831604004 | BCE Loss: 1.0187833309173584\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.068141937255859 | KNN Loss: 5.040158271789551 | BCE Loss: 1.0279834270477295\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.064464092254639 | KNN Loss: 5.039237022399902 | BCE Loss: 1.0252269506454468\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.0871686935424805 | KNN Loss: 5.038000106811523 | BCE Loss: 1.049168348312378\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.074873924255371 | KNN Loss: 5.031773090362549 | BCE Loss: 1.0431008338928223\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.0692057609558105 | KNN Loss: 5.030173301696777 | BCE Loss: 1.0390324592590332\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.054629325866699 | KNN Loss: 5.031661510467529 | BCE Loss: 1.022968053817749\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.073970317840576 | KNN Loss: 5.042634010314941 | BCE Loss: 1.0313363075256348\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.077775001525879 | KNN Loss: 5.031854629516602 | BCE Loss: 1.0459201335906982\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.082730293273926 | KNN Loss: 5.0330281257629395 | BCE Loss: 1.0497021675109863\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.089025497436523 | KNN Loss: 5.03975248336792 | BCE Loss: 1.0492730140686035\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.114129066467285 | KNN Loss: 5.067646503448486 | BCE Loss: 1.0464825630187988\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.046693801879883 | KNN Loss: 5.0347161293029785 | BCE Loss: 1.0119776725769043\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.09276819229126 | KNN Loss: 5.040646076202393 | BCE Loss: 1.0521221160888672\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.0740251541137695 | KNN Loss: 5.038339138031006 | BCE Loss: 1.0356861352920532\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.094309329986572 | KNN Loss: 5.038758277893066 | BCE Loss: 1.0555510520935059\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.087601184844971 | KNN Loss: 5.041341304779053 | BCE Loss: 1.046259880065918\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.083282947540283 | KNN Loss: 5.032841682434082 | BCE Loss: 1.0504413843154907\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.083418846130371 | KNN Loss: 5.055907249450684 | BCE Loss: 1.0275118350982666\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.094093322753906 | KNN Loss: 5.034482002258301 | BCE Loss: 1.0596115589141846\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.081459999084473 | KNN Loss: 5.044580936431885 | BCE Loss: 1.036879301071167\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.043838977813721 | KNN Loss: 5.034994125366211 | BCE Loss: 1.0088448524475098\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.097537517547607 | KNN Loss: 5.052519798278809 | BCE Loss: 1.0450177192687988\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.111672401428223 | KNN Loss: 5.036819934844971 | BCE Loss: 1.074852705001831\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.071584224700928 | KNN Loss: 5.029376983642578 | BCE Loss: 1.0422072410583496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.069293022155762 | KNN Loss: 5.03639554977417 | BCE Loss: 1.0328973531723022\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.09158182144165 | KNN Loss: 5.032209873199463 | BCE Loss: 1.059372067451477\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.06585693359375 | KNN Loss: 5.03915548324585 | BCE Loss: 1.0267012119293213\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.083051681518555 | KNN Loss: 5.036562919616699 | BCE Loss: 1.0464890003204346\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.079998016357422 | KNN Loss: 5.03474235534668 | BCE Loss: 1.0452557802200317\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.086360931396484 | KNN Loss: 5.0373311042785645 | BCE Loss: 1.0490297079086304\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.09236478805542 | KNN Loss: 5.035271644592285 | BCE Loss: 1.0570932626724243\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.050549507141113 | KNN Loss: 5.02980899810791 | BCE Loss: 1.0207405090332031\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.088337421417236 | KNN Loss: 5.044297218322754 | BCE Loss: 1.0440402030944824\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.090482234954834 | KNN Loss: 5.0341877937316895 | BCE Loss: 1.056294322013855\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.10558557510376 | KNN Loss: 5.038113117218018 | BCE Loss: 1.0674724578857422\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.114214897155762 | KNN Loss: 5.034573078155518 | BCE Loss: 1.0796419382095337\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.057574272155762 | KNN Loss: 5.039064884185791 | BCE Loss: 1.0185091495513916\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.087636947631836 | KNN Loss: 5.044355392456055 | BCE Loss: 1.0432813167572021\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.071101665496826 | KNN Loss: 5.033807754516602 | BCE Loss: 1.0372939109802246\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.047190189361572 | KNN Loss: 5.031914710998535 | BCE Loss: 1.015275478363037\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.085268497467041 | KNN Loss: 5.031219005584717 | BCE Loss: 1.0540494918823242\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.052452087402344 | KNN Loss: 5.032414436340332 | BCE Loss: 1.0200375318527222\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.079252243041992 | KNN Loss: 5.050922870635986 | BCE Loss: 1.0283291339874268\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.094263076782227 | KNN Loss: 5.042764663696289 | BCE Loss: 1.0514984130859375\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.08305549621582 | KNN Loss: 5.033921241760254 | BCE Loss: 1.0491342544555664\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.055893898010254 | KNN Loss: 5.032617092132568 | BCE Loss: 1.0232770442962646\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.085074424743652 | KNN Loss: 5.0333991050720215 | BCE Loss: 1.05167555809021\n",
      "Epoch    90: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.040576934814453 | KNN Loss: 5.0318145751953125 | BCE Loss: 1.0087623596191406\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.083528518676758 | KNN Loss: 5.033430576324463 | BCE Loss: 1.050098180770874\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.103156089782715 | KNN Loss: 5.078292369842529 | BCE Loss: 1.0248639583587646\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.072984218597412 | KNN Loss: 5.040778636932373 | BCE Loss: 1.0322057008743286\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.051608562469482 | KNN Loss: 5.030703544616699 | BCE Loss: 1.0209050178527832\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.081599235534668 | KNN Loss: 5.033332824707031 | BCE Loss: 1.0482664108276367\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.09122371673584 | KNN Loss: 5.029196262359619 | BCE Loss: 1.0620275735855103\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.090239524841309 | KNN Loss: 5.046676158905029 | BCE Loss: 1.0435631275177002\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.074102401733398 | KNN Loss: 5.034531116485596 | BCE Loss: 1.0395712852478027\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.076903343200684 | KNN Loss: 5.040530204772949 | BCE Loss: 1.0363733768463135\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.096593856811523 | KNN Loss: 5.052308559417725 | BCE Loss: 1.0442850589752197\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.0869855880737305 | KNN Loss: 5.036736488342285 | BCE Loss: 1.0502488613128662\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.125432968139648 | KNN Loss: 5.040405750274658 | BCE Loss: 1.0850274562835693\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.0667009353637695 | KNN Loss: 5.0303053855896 | BCE Loss: 1.0363953113555908\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.087867736816406 | KNN Loss: 5.047426223754883 | BCE Loss: 1.0404415130615234\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.0877685546875 | KNN Loss: 5.037988662719727 | BCE Loss: 1.0497801303863525\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.053988456726074 | KNN Loss: 5.033139705657959 | BCE Loss: 1.0208487510681152\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.078307151794434 | KNN Loss: 5.033535957336426 | BCE Loss: 1.044771432876587\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.067332744598389 | KNN Loss: 5.03144645690918 | BCE Loss: 1.035886287689209\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.117396354675293 | KNN Loss: 5.039936065673828 | BCE Loss: 1.0774600505828857\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.103732585906982 | KNN Loss: 5.0394182205200195 | BCE Loss: 1.064314365386963\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.082015514373779 | KNN Loss: 5.02998685836792 | BCE Loss: 1.0520286560058594\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.058104038238525 | KNN Loss: 5.031991958618164 | BCE Loss: 1.0261120796203613\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.077508926391602 | KNN Loss: 5.0308918952941895 | BCE Loss: 1.0466171503067017\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.06824254989624 | KNN Loss: 5.030520915985107 | BCE Loss: 1.0377216339111328\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.085885047912598 | KNN Loss: 5.036558628082275 | BCE Loss: 1.0493265390396118\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.102298736572266 | KNN Loss: 5.046928882598877 | BCE Loss: 1.0553696155548096\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.079966068267822 | KNN Loss: 5.037118434906006 | BCE Loss: 1.0428476333618164\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.066564083099365 | KNN Loss: 5.032908916473389 | BCE Loss: 1.0336551666259766\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.085258483886719 | KNN Loss: 5.0301055908203125 | BCE Loss: 1.0551526546478271\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.0981316566467285 | KNN Loss: 5.0460920333862305 | BCE Loss: 1.0520397424697876\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.11851692199707 | KNN Loss: 5.046196937561035 | BCE Loss: 1.0723199844360352\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.0707268714904785 | KNN Loss: 5.02908182144165 | BCE Loss: 1.0416449308395386\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.047920227050781 | KNN Loss: 5.029923439025879 | BCE Loss: 1.017996907234192\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.040081024169922 | KNN Loss: 5.034768581390381 | BCE Loss: 1.0053126811981201\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.086756229400635 | KNN Loss: 5.038487434387207 | BCE Loss: 1.0482687950134277\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.067074298858643 | KNN Loss: 5.025641918182373 | BCE Loss: 1.0414323806762695\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.04046106338501 | KNN Loss: 5.029689788818359 | BCE Loss: 1.0107711553573608\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.050899505615234 | KNN Loss: 5.0308380126953125 | BCE Loss: 1.0200614929199219\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.072311878204346 | KNN Loss: 5.031050682067871 | BCE Loss: 1.0412611961364746\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.097344398498535 | KNN Loss: 5.03886079788208 | BCE Loss: 1.0584834814071655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.084042549133301 | KNN Loss: 5.027827262878418 | BCE Loss: 1.056215524673462\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.056000709533691 | KNN Loss: 5.038578033447266 | BCE Loss: 1.0174229145050049\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.107810020446777 | KNN Loss: 5.057973861694336 | BCE Loss: 1.0498363971710205\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.077352523803711 | KNN Loss: 5.030940055847168 | BCE Loss: 1.046412467956543\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.089074611663818 | KNN Loss: 5.038368225097656 | BCE Loss: 1.050706386566162\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.097002983093262 | KNN Loss: 5.050445556640625 | BCE Loss: 1.0465574264526367\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.034694671630859 | KNN Loss: 5.024302005767822 | BCE Loss: 1.0103929042816162\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.089261054992676 | KNN Loss: 5.045051097869873 | BCE Loss: 1.0442101955413818\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.078973770141602 | KNN Loss: 5.038985729217529 | BCE Loss: 1.0399881601333618\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.120975017547607 | KNN Loss: 5.03732967376709 | BCE Loss: 1.0836453437805176\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.09664249420166 | KNN Loss: 5.032984733581543 | BCE Loss: 1.0636577606201172\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.079239845275879 | KNN Loss: 5.033971786499023 | BCE Loss: 1.0452680587768555\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.072930812835693 | KNN Loss: 5.036184310913086 | BCE Loss: 1.036746621131897\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.09481143951416 | KNN Loss: 5.029839992523193 | BCE Loss: 1.0649712085723877\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.075769424438477 | KNN Loss: 5.032147407531738 | BCE Loss: 1.0436218976974487\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.086766242980957 | KNN Loss: 5.037199020385742 | BCE Loss: 1.0495672225952148\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.1069111824035645 | KNN Loss: 5.0500922203063965 | BCE Loss: 1.0568188428878784\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.096870422363281 | KNN Loss: 5.029804229736328 | BCE Loss: 1.067065954208374\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.057584762573242 | KNN Loss: 5.041471004486084 | BCE Loss: 1.0161137580871582\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.087657928466797 | KNN Loss: 5.061994552612305 | BCE Loss: 1.0256633758544922\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.0692901611328125 | KNN Loss: 5.031140327453613 | BCE Loss: 1.0381498336791992\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.099992275238037 | KNN Loss: 5.039221286773682 | BCE Loss: 1.0607709884643555\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.095243453979492 | KNN Loss: 5.039804935455322 | BCE Loss: 1.0554386377334595\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.046220302581787 | KNN Loss: 5.038963794708252 | BCE Loss: 1.0072565078735352\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.062338829040527 | KNN Loss: 5.031261444091797 | BCE Loss: 1.0310771465301514\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.0900654792785645 | KNN Loss: 5.029733180999756 | BCE Loss: 1.0603324174880981\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.083327770233154 | KNN Loss: 5.0376105308532715 | BCE Loss: 1.0457172393798828\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.108501434326172 | KNN Loss: 5.066306114196777 | BCE Loss: 1.0421953201293945\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.071503639221191 | KNN Loss: 5.034984588623047 | BCE Loss: 1.036519169807434\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.072975158691406 | KNN Loss: 5.042763710021973 | BCE Loss: 1.0302115678787231\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.0905442237854 | KNN Loss: 5.030947208404541 | BCE Loss: 1.059597134590149\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.10014533996582 | KNN Loss: 5.0443878173828125 | BCE Loss: 1.0557575225830078\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.113800525665283 | KNN Loss: 5.0367326736450195 | BCE Loss: 1.0770677328109741\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.080307960510254 | KNN Loss: 5.036718368530273 | BCE Loss: 1.0435898303985596\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.0590434074401855 | KNN Loss: 5.0280985832214355 | BCE Loss: 1.03094482421875\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.077389240264893 | KNN Loss: 5.035928249359131 | BCE Loss: 1.0414609909057617\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.079348087310791 | KNN Loss: 5.0397748947143555 | BCE Loss: 1.0395731925964355\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.0793070793151855 | KNN Loss: 5.044379711151123 | BCE Loss: 1.0349273681640625\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.063046455383301 | KNN Loss: 5.026468753814697 | BCE Loss: 1.0365774631500244\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.091329097747803 | KNN Loss: 5.03530216217041 | BCE Loss: 1.0560269355773926\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.059436321258545 | KNN Loss: 5.033989906311035 | BCE Loss: 1.0254462957382202\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.0947771072387695 | KNN Loss: 5.041948318481445 | BCE Loss: 1.0528287887573242\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.073705673217773 | KNN Loss: 5.029016017913818 | BCE Loss: 1.0446895360946655\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.090402126312256 | KNN Loss: 5.02855110168457 | BCE Loss: 1.0618510246276855\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.050227165222168 | KNN Loss: 5.03679895401001 | BCE Loss: 1.0134282112121582\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.072760581970215 | KNN Loss: 5.034050464630127 | BCE Loss: 1.038710355758667\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.0636186599731445 | KNN Loss: 5.02876615524292 | BCE Loss: 1.0348525047302246\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.106257438659668 | KNN Loss: 5.033779144287109 | BCE Loss: 1.0724784135818481\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.0827178955078125 | KNN Loss: 5.041329383850098 | BCE Loss: 1.041388750076294\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.080253601074219 | KNN Loss: 5.049820423126221 | BCE Loss: 1.030433177947998\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.074985027313232 | KNN Loss: 5.0370354652404785 | BCE Loss: 1.0379496812820435\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.0763678550720215 | KNN Loss: 5.039222240447998 | BCE Loss: 1.037145733833313\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.101319313049316 | KNN Loss: 5.047551155090332 | BCE Loss: 1.0537683963775635\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.106904029846191 | KNN Loss: 5.047397136688232 | BCE Loss: 1.0595066547393799\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.069323539733887 | KNN Loss: 5.0291337966918945 | BCE Loss: 1.0401898622512817\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.079999923706055 | KNN Loss: 5.032063961029053 | BCE Loss: 1.0479357242584229\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.103791236877441 | KNN Loss: 5.038941860198975 | BCE Loss: 1.0648493766784668\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.072981834411621 | KNN Loss: 5.035181999206543 | BCE Loss: 1.0378000736236572\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.112868309020996 | KNN Loss: 5.053866386413574 | BCE Loss: 1.0590019226074219\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.11021614074707 | KNN Loss: 5.047778606414795 | BCE Loss: 1.0624372959136963\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.0542497634887695 | KNN Loss: 5.036783218383789 | BCE Loss: 1.0174667835235596\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.092746257781982 | KNN Loss: 5.03507661819458 | BCE Loss: 1.0576696395874023\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.073984622955322 | KNN Loss: 5.036859035491943 | BCE Loss: 1.037125587463379\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.104783058166504 | KNN Loss: 5.064416885375977 | BCE Loss: 1.040366291999817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.052009582519531 | KNN Loss: 5.028120517730713 | BCE Loss: 1.0238889455795288\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.063867568969727 | KNN Loss: 5.033749103546143 | BCE Loss: 1.030118465423584\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.080366134643555 | KNN Loss: 5.0443501472473145 | BCE Loss: 1.0360157489776611\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.073193550109863 | KNN Loss: 5.034810543060303 | BCE Loss: 1.0383827686309814\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.060562610626221 | KNN Loss: 5.0455241203308105 | BCE Loss: 1.0150386095046997\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.066206455230713 | KNN Loss: 5.034526348114014 | BCE Loss: 1.0316799879074097\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.068295001983643 | KNN Loss: 5.027552127838135 | BCE Loss: 1.0407429933547974\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.078431606292725 | KNN Loss: 5.03693962097168 | BCE Loss: 1.0414918661117554\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.071252822875977 | KNN Loss: 5.030850887298584 | BCE Loss: 1.040401816368103\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.068639755249023 | KNN Loss: 5.03612756729126 | BCE Loss: 1.0325124263763428\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.093067646026611 | KNN Loss: 5.040058612823486 | BCE Loss: 1.0530091524124146\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.095438003540039 | KNN Loss: 5.04899263381958 | BCE Loss: 1.046445369720459\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.09974479675293 | KNN Loss: 5.045553684234619 | BCE Loss: 1.0541908740997314\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.0514750480651855 | KNN Loss: 5.030518531799316 | BCE Loss: 1.0209563970565796\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.063893795013428 | KNN Loss: 5.035143852233887 | BCE Loss: 1.0287500619888306\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.075006484985352 | KNN Loss: 5.040520668029785 | BCE Loss: 1.0344855785369873\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.071126937866211 | KNN Loss: 5.045485496520996 | BCE Loss: 1.0256414413452148\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.114750385284424 | KNN Loss: 5.038659572601318 | BCE Loss: 1.076090693473816\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.0821638107299805 | KNN Loss: 5.032205581665039 | BCE Loss: 1.0499584674835205\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.041426658630371 | KNN Loss: 5.031056880950928 | BCE Loss: 1.0103696584701538\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.069682598114014 | KNN Loss: 5.0567803382873535 | BCE Loss: 1.0129022598266602\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.119841575622559 | KNN Loss: 5.030165672302246 | BCE Loss: 1.0896756649017334\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.1015777587890625 | KNN Loss: 5.055899143218994 | BCE Loss: 1.0456786155700684\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.082433700561523 | KNN Loss: 5.031716346740723 | BCE Loss: 1.0507171154022217\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.087913990020752 | KNN Loss: 5.036314487457275 | BCE Loss: 1.0515995025634766\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.115992546081543 | KNN Loss: 5.052263259887695 | BCE Loss: 1.063729166984558\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.093919277191162 | KNN Loss: 5.05988883972168 | BCE Loss: 1.0340304374694824\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.079792499542236 | KNN Loss: 5.035309791564941 | BCE Loss: 1.044482707977295\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.057210922241211 | KNN Loss: 5.030797958374023 | BCE Loss: 1.0264127254486084\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.059201240539551 | KNN Loss: 5.037735939025879 | BCE Loss: 1.021465539932251\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.07710599899292 | KNN Loss: 5.033261299133301 | BCE Loss: 1.0438446998596191\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.099158763885498 | KNN Loss: 5.033789157867432 | BCE Loss: 1.065369725227356\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.10262393951416 | KNN Loss: 5.0438008308410645 | BCE Loss: 1.0588231086730957\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.08392333984375 | KNN Loss: 5.054574966430664 | BCE Loss: 1.029348373413086\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.0896501541137695 | KNN Loss: 5.031091690063477 | BCE Loss: 1.058558464050293\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.129908561706543 | KNN Loss: 5.06339693069458 | BCE Loss: 1.066511631011963\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.093877792358398 | KNN Loss: 5.035679817199707 | BCE Loss: 1.0581978559494019\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.085020065307617 | KNN Loss: 5.032524585723877 | BCE Loss: 1.0524952411651611\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.096420764923096 | KNN Loss: 5.0498576164245605 | BCE Loss: 1.0465631484985352\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.108172416687012 | KNN Loss: 5.044701099395752 | BCE Loss: 1.0634711980819702\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.080605506896973 | KNN Loss: 5.039319038391113 | BCE Loss: 1.0412862300872803\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.078876972198486 | KNN Loss: 5.040408134460449 | BCE Loss: 1.038468837738037\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.044847011566162 | KNN Loss: 5.037781238555908 | BCE Loss: 1.007065773010254\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.082075119018555 | KNN Loss: 5.03810977935791 | BCE Loss: 1.043965220451355\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.089268684387207 | KNN Loss: 5.038305759429932 | BCE Loss: 1.0509631633758545\n",
      "Epoch   115: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.066738128662109 | KNN Loss: 5.033576488494873 | BCE Loss: 1.0331614017486572\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.060418128967285 | KNN Loss: 5.032402515411377 | BCE Loss: 1.0280158519744873\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.087010860443115 | KNN Loss: 5.037505626678467 | BCE Loss: 1.0495052337646484\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.077849864959717 | KNN Loss: 5.029242515563965 | BCE Loss: 1.0486074686050415\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.092850685119629 | KNN Loss: 5.03802490234375 | BCE Loss: 1.054825782775879\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.082371711730957 | KNN Loss: 5.047069072723389 | BCE Loss: 1.0353024005889893\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.076352119445801 | KNN Loss: 5.031421184539795 | BCE Loss: 1.0449308156967163\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.0546183586120605 | KNN Loss: 5.035615921020508 | BCE Loss: 1.0190024375915527\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.089024543762207 | KNN Loss: 5.049829959869385 | BCE Loss: 1.0391943454742432\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.03908634185791 | KNN Loss: 5.026942729949951 | BCE Loss: 1.012143850326538\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.075949668884277 | KNN Loss: 5.03085470199585 | BCE Loss: 1.0450949668884277\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.068841457366943 | KNN Loss: 5.032216548919678 | BCE Loss: 1.0366249084472656\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.067422389984131 | KNN Loss: 5.045490264892578 | BCE Loss: 1.0219321250915527\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.068265914916992 | KNN Loss: 5.0274739265441895 | BCE Loss: 1.0407922267913818\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.065526485443115 | KNN Loss: 5.033973693847656 | BCE Loss: 1.031552791595459\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.083514213562012 | KNN Loss: 5.039722919464111 | BCE Loss: 1.0437915325164795\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.078708648681641 | KNN Loss: 5.042737007141113 | BCE Loss: 1.0359718799591064\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.055002212524414 | KNN Loss: 5.032626628875732 | BCE Loss: 1.0223757028579712\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.068689346313477 | KNN Loss: 5.035546779632568 | BCE Loss: 1.0331426858901978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.082198619842529 | KNN Loss: 5.041184425354004 | BCE Loss: 1.0410141944885254\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.091071605682373 | KNN Loss: 5.0430731773376465 | BCE Loss: 1.0479984283447266\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.050337791442871 | KNN Loss: 5.035341739654541 | BCE Loss: 1.0149962902069092\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.0671281814575195 | KNN Loss: 5.032009124755859 | BCE Loss: 1.035118818283081\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.057733535766602 | KNN Loss: 5.030279159545898 | BCE Loss: 1.0274542570114136\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.140458106994629 | KNN Loss: 5.109394550323486 | BCE Loss: 1.0310635566711426\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.093816757202148 | KNN Loss: 5.037773132324219 | BCE Loss: 1.0560436248779297\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.059527397155762 | KNN Loss: 5.027854919433594 | BCE Loss: 1.031672477722168\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.0993332862854 | KNN Loss: 5.030411243438721 | BCE Loss: 1.0689220428466797\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.074504852294922 | KNN Loss: 5.03537130355835 | BCE Loss: 1.0391335487365723\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.064096927642822 | KNN Loss: 5.037453651428223 | BCE Loss: 1.0266432762145996\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.084914207458496 | KNN Loss: 5.049896717071533 | BCE Loss: 1.0350172519683838\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.071228981018066 | KNN Loss: 5.031455039978027 | BCE Loss: 1.0397741794586182\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.060050964355469 | KNN Loss: 5.02750301361084 | BCE Loss: 1.032547950744629\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.099316596984863 | KNN Loss: 5.031439781188965 | BCE Loss: 1.0678766965866089\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.075958728790283 | KNN Loss: 5.046024322509766 | BCE Loss: 1.0299344062805176\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.077301025390625 | KNN Loss: 5.049432754516602 | BCE Loss: 1.0278685092926025\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.094970703125 | KNN Loss: 5.051433563232422 | BCE Loss: 1.0435373783111572\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.1209306716918945 | KNN Loss: 5.036787509918213 | BCE Loss: 1.0841429233551025\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.099791526794434 | KNN Loss: 5.039921760559082 | BCE Loss: 1.059869647026062\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.082717418670654 | KNN Loss: 5.032487392425537 | BCE Loss: 1.0502301454544067\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.0604987144470215 | KNN Loss: 5.02920389175415 | BCE Loss: 1.0312947034835815\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.088897705078125 | KNN Loss: 5.031826019287109 | BCE Loss: 1.0570719242095947\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.090877532958984 | KNN Loss: 5.025050640106201 | BCE Loss: 1.0658271312713623\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.058071613311768 | KNN Loss: 5.034013271331787 | BCE Loss: 1.02405846118927\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.069582462310791 | KNN Loss: 5.032970905303955 | BCE Loss: 1.036611557006836\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.080018043518066 | KNN Loss: 5.034732818603516 | BCE Loss: 1.0452853441238403\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.085349082946777 | KNN Loss: 5.033843994140625 | BCE Loss: 1.0515050888061523\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.060837745666504 | KNN Loss: 5.027515411376953 | BCE Loss: 1.0333223342895508\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.0542120933532715 | KNN Loss: 5.038477420806885 | BCE Loss: 1.0157346725463867\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.084152698516846 | KNN Loss: 5.029939651489258 | BCE Loss: 1.0542129278182983\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.070229530334473 | KNN Loss: 5.034956455230713 | BCE Loss: 1.0352733135223389\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.0751118659973145 | KNN Loss: 5.036423683166504 | BCE Loss: 1.0386883020401\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.057635307312012 | KNN Loss: 5.027999401092529 | BCE Loss: 1.029636025428772\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.058995246887207 | KNN Loss: 5.0358428955078125 | BCE Loss: 1.0231523513793945\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.091073989868164 | KNN Loss: 5.042468070983887 | BCE Loss: 1.0486056804656982\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.069877624511719 | KNN Loss: 5.037421703338623 | BCE Loss: 1.0324561595916748\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.078090667724609 | KNN Loss: 5.041511058807373 | BCE Loss: 1.0365796089172363\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.121959209442139 | KNN Loss: 5.037501335144043 | BCE Loss: 1.0844579935073853\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.093526840209961 | KNN Loss: 5.038402557373047 | BCE Loss: 1.055124044418335\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.087708473205566 | KNN Loss: 5.050075531005859 | BCE Loss: 1.0376330614089966\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.090834617614746 | KNN Loss: 5.035407066345215 | BCE Loss: 1.0554273128509521\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.0925984382629395 | KNN Loss: 5.037505626678467 | BCE Loss: 1.0550928115844727\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.077373504638672 | KNN Loss: 5.027265548706055 | BCE Loss: 1.0501080751419067\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.046027660369873 | KNN Loss: 5.028133869171143 | BCE Loss: 1.01789391040802\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.103839874267578 | KNN Loss: 5.040160179138184 | BCE Loss: 1.0636794567108154\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.059347152709961 | KNN Loss: 5.033705234527588 | BCE Loss: 1.0256421566009521\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.048317909240723 | KNN Loss: 5.030168533325195 | BCE Loss: 1.0181493759155273\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.061824321746826 | KNN Loss: 5.030511856079102 | BCE Loss: 1.0313125848770142\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.0290751457214355 | KNN Loss: 5.032207489013672 | BCE Loss: 0.9968676567077637\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.091521739959717 | KNN Loss: 5.034294605255127 | BCE Loss: 1.0572270154953003\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.080020904541016 | KNN Loss: 5.042549133300781 | BCE Loss: 1.0374720096588135\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.098972320556641 | KNN Loss: 5.025553226470947 | BCE Loss: 1.0734188556671143\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.112689971923828 | KNN Loss: 5.051758766174316 | BCE Loss: 1.0609314441680908\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.076587200164795 | KNN Loss: 5.031225681304932 | BCE Loss: 1.0453613996505737\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.063100337982178 | KNN Loss: 5.027912139892578 | BCE Loss: 1.0351881980895996\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.044983386993408 | KNN Loss: 5.034368515014648 | BCE Loss: 1.0106148719787598\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.071728706359863 | KNN Loss: 5.028674602508545 | BCE Loss: 1.0430539846420288\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.0461225509643555 | KNN Loss: 5.035544395446777 | BCE Loss: 1.0105782747268677\n",
      "Epoch   128: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.08436393737793 | KNN Loss: 5.043149471282959 | BCE Loss: 1.0412147045135498\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.120804309844971 | KNN Loss: 5.0809645652771 | BCE Loss: 1.0398396253585815\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.05775785446167 | KNN Loss: 5.027777671813965 | BCE Loss: 1.029980182647705\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.060493469238281 | KNN Loss: 5.030304431915283 | BCE Loss: 1.030189037322998\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.11431884765625 | KNN Loss: 5.038575172424316 | BCE Loss: 1.0757439136505127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.070181846618652 | KNN Loss: 5.041353225708008 | BCE Loss: 1.0288288593292236\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.087186813354492 | KNN Loss: 5.0392279624938965 | BCE Loss: 1.0479586124420166\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.069699764251709 | KNN Loss: 5.028414249420166 | BCE Loss: 1.0412856340408325\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.072876930236816 | KNN Loss: 5.037140846252441 | BCE Loss: 1.0357359647750854\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.043679714202881 | KNN Loss: 5.039313793182373 | BCE Loss: 1.0043660402297974\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.062193870544434 | KNN Loss: 5.0416789054870605 | BCE Loss: 1.0205152034759521\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.106677532196045 | KNN Loss: 5.035653591156006 | BCE Loss: 1.0710238218307495\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.092171669006348 | KNN Loss: 5.035059928894043 | BCE Loss: 1.0571118593215942\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.0541672706604 | KNN Loss: 5.034473896026611 | BCE Loss: 1.0196934938430786\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.112750053405762 | KNN Loss: 5.037519454956055 | BCE Loss: 1.075230360031128\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.111236095428467 | KNN Loss: 5.051718235015869 | BCE Loss: 1.0595179796218872\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.091588973999023 | KNN Loss: 5.049157619476318 | BCE Loss: 1.042431116104126\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.069865703582764 | KNN Loss: 5.03412389755249 | BCE Loss: 1.0357418060302734\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.094934463500977 | KNN Loss: 5.0347089767456055 | BCE Loss: 1.0602257251739502\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.094837665557861 | KNN Loss: 5.036616325378418 | BCE Loss: 1.0582212209701538\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.095756530761719 | KNN Loss: 5.041914463043213 | BCE Loss: 1.0538418292999268\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.049371719360352 | KNN Loss: 5.030330181121826 | BCE Loss: 1.0190412998199463\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.087778091430664 | KNN Loss: 5.037526607513428 | BCE Loss: 1.0502513647079468\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.040848731994629 | KNN Loss: 5.036762237548828 | BCE Loss: 1.0040866136550903\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.10507869720459 | KNN Loss: 5.055713176727295 | BCE Loss: 1.049365520477295\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.082510948181152 | KNN Loss: 5.042985439300537 | BCE Loss: 1.0395255088806152\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.077356815338135 | KNN Loss: 5.042234420776367 | BCE Loss: 1.035122275352478\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.098669528961182 | KNN Loss: 5.0305938720703125 | BCE Loss: 1.0680757761001587\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.101253509521484 | KNN Loss: 5.065463066101074 | BCE Loss: 1.0357903242111206\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.065521717071533 | KNN Loss: 5.045373439788818 | BCE Loss: 1.0201482772827148\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.086277008056641 | KNN Loss: 5.032728672027588 | BCE Loss: 1.0535480976104736\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.089930057525635 | KNN Loss: 5.041466236114502 | BCE Loss: 1.0484638214111328\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.063437461853027 | KNN Loss: 5.031367778778076 | BCE Loss: 1.0320696830749512\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.053831577301025 | KNN Loss: 5.029659748077393 | BCE Loss: 1.0241719484329224\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.071158409118652 | KNN Loss: 5.031880855560303 | BCE Loss: 1.0392773151397705\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.044791221618652 | KNN Loss: 5.025411128997803 | BCE Loss: 1.0193798542022705\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.060100078582764 | KNN Loss: 5.03064489364624 | BCE Loss: 1.0294551849365234\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.0476508140563965 | KNN Loss: 5.031623363494873 | BCE Loss: 1.0160274505615234\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.094644546508789 | KNN Loss: 5.043187618255615 | BCE Loss: 1.051457166671753\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.072044372558594 | KNN Loss: 5.034258842468262 | BCE Loss: 1.037785291671753\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.07093620300293 | KNN Loss: 5.036301136016846 | BCE Loss: 1.0346348285675049\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.075546741485596 | KNN Loss: 5.043096542358398 | BCE Loss: 1.0324501991271973\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.098888874053955 | KNN Loss: 5.048556804656982 | BCE Loss: 1.0503321886062622\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.076776027679443 | KNN Loss: 5.039881706237793 | BCE Loss: 1.0368942022323608\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.11931037902832 | KNN Loss: 5.065800666809082 | BCE Loss: 1.0535097122192383\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.10886812210083 | KNN Loss: 5.0474772453308105 | BCE Loss: 1.06139075756073\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.071357250213623 | KNN Loss: 5.030081272125244 | BCE Loss: 1.041275978088379\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.076384544372559 | KNN Loss: 5.038360595703125 | BCE Loss: 1.0380237102508545\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.058361530303955 | KNN Loss: 5.044989109039307 | BCE Loss: 1.013372540473938\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.0738115310668945 | KNN Loss: 5.0320563316345215 | BCE Loss: 1.0417554378509521\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.073233604431152 | KNN Loss: 5.033736705780029 | BCE Loss: 1.0394967794418335\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.114356994628906 | KNN Loss: 5.054938793182373 | BCE Loss: 1.0594182014465332\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.090321063995361 | KNN Loss: 5.0424346923828125 | BCE Loss: 1.0478862524032593\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.072025299072266 | KNN Loss: 5.037631511688232 | BCE Loss: 1.034393548965454\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.095236778259277 | KNN Loss: 5.036495685577393 | BCE Loss: 1.0587408542633057\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.063521385192871 | KNN Loss: 5.030695915222168 | BCE Loss: 1.0328257083892822\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.070054054260254 | KNN Loss: 5.034026145935059 | BCE Loss: 1.0360277891159058\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.056596279144287 | KNN Loss: 5.039862632751465 | BCE Loss: 1.0167335271835327\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.075511455535889 | KNN Loss: 5.030721187591553 | BCE Loss: 1.0447903871536255\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.106142997741699 | KNN Loss: 5.053419589996338 | BCE Loss: 1.0527231693267822\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.078461647033691 | KNN Loss: 5.03999662399292 | BCE Loss: 1.0384647846221924\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.138563632965088 | KNN Loss: 5.0612874031066895 | BCE Loss: 1.0772762298583984\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.0871500968933105 | KNN Loss: 5.031633377075195 | BCE Loss: 1.0555166006088257\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.073398590087891 | KNN Loss: 5.043594837188721 | BCE Loss: 1.02980375289917\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.10959005355835 | KNN Loss: 5.03885555267334 | BCE Loss: 1.0707345008850098\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.066182613372803 | KNN Loss: 5.046003818511963 | BCE Loss: 1.0201789140701294\n",
      "Epoch   139: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.047943115234375 | KNN Loss: 5.027473449707031 | BCE Loss: 1.0204699039459229\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.038221836090088 | KNN Loss: 5.034348487854004 | BCE Loss: 1.0038734674453735\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.074734687805176 | KNN Loss: 5.034634113311768 | BCE Loss: 1.0401004552841187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.100985050201416 | KNN Loss: 5.057323455810547 | BCE Loss: 1.0436615943908691\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.089192867279053 | KNN Loss: 5.036113262176514 | BCE Loss: 1.0530794858932495\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.054187774658203 | KNN Loss: 5.035865783691406 | BCE Loss: 1.018322229385376\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.083724498748779 | KNN Loss: 5.05403995513916 | BCE Loss: 1.0296844244003296\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.070745468139648 | KNN Loss: 5.034542560577393 | BCE Loss: 1.0362030267715454\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.087112903594971 | KNN Loss: 5.041832447052002 | BCE Loss: 1.0452804565429688\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.118324279785156 | KNN Loss: 5.039999485015869 | BCE Loss: 1.078324794769287\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.079291820526123 | KNN Loss: 5.036701679229736 | BCE Loss: 1.0425901412963867\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.084518909454346 | KNN Loss: 5.065674781799316 | BCE Loss: 1.0188441276550293\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.0828704833984375 | KNN Loss: 5.04067850112915 | BCE Loss: 1.0421922206878662\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.095156669616699 | KNN Loss: 5.0372138023376465 | BCE Loss: 1.0579428672790527\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.076630592346191 | KNN Loss: 5.038344383239746 | BCE Loss: 1.0382863283157349\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.086503028869629 | KNN Loss: 5.036108016967773 | BCE Loss: 1.0503950119018555\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.088240623474121 | KNN Loss: 5.030442714691162 | BCE Loss: 1.0577976703643799\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.084914207458496 | KNN Loss: 5.041171073913574 | BCE Loss: 1.0437430143356323\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.0742902755737305 | KNN Loss: 5.031182765960693 | BCE Loss: 1.043107509613037\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.0658278465271 | KNN Loss: 5.043078422546387 | BCE Loss: 1.022749423980713\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.06874418258667 | KNN Loss: 5.027059555053711 | BCE Loss: 1.041684627532959\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.082300662994385 | KNN Loss: 5.042006015777588 | BCE Loss: 1.0402946472167969\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.048489570617676 | KNN Loss: 5.027781963348389 | BCE Loss: 1.0207074880599976\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.101307392120361 | KNN Loss: 5.0364789962768555 | BCE Loss: 1.0648282766342163\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.0704545974731445 | KNN Loss: 5.031828880310059 | BCE Loss: 1.0386254787445068\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.081212043762207 | KNN Loss: 5.033932209014893 | BCE Loss: 1.0472798347473145\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.09891939163208 | KNN Loss: 5.042747974395752 | BCE Loss: 1.0561714172363281\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.083835601806641 | KNN Loss: 5.040144443511963 | BCE Loss: 1.0436911582946777\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.079316139221191 | KNN Loss: 5.037622451782227 | BCE Loss: 1.0416934490203857\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.072263717651367 | KNN Loss: 5.045873165130615 | BCE Loss: 1.0263903141021729\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.052581310272217 | KNN Loss: 5.032511234283447 | BCE Loss: 1.0200700759887695\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.06382942199707 | KNN Loss: 5.047395706176758 | BCE Loss: 1.0164339542388916\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.074925422668457 | KNN Loss: 5.046246528625488 | BCE Loss: 1.0286790132522583\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.0616960525512695 | KNN Loss: 5.039361476898193 | BCE Loss: 1.0223344564437866\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.0462822914123535 | KNN Loss: 5.029494762420654 | BCE Loss: 1.0167875289916992\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.066457271575928 | KNN Loss: 5.032800674438477 | BCE Loss: 1.0336565971374512\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.053204536437988 | KNN Loss: 5.036320209503174 | BCE Loss: 1.0168845653533936\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.07804012298584 | KNN Loss: 5.035012722015381 | BCE Loss: 1.0430271625518799\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.05084228515625 | KNN Loss: 5.029708385467529 | BCE Loss: 1.0211340188980103\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.087199687957764 | KNN Loss: 5.036762237548828 | BCE Loss: 1.0504374504089355\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.052592754364014 | KNN Loss: 5.043144702911377 | BCE Loss: 1.0094479322433472\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.109660625457764 | KNN Loss: 5.05224084854126 | BCE Loss: 1.0574196577072144\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.057555198669434 | KNN Loss: 5.028308391571045 | BCE Loss: 1.0292465686798096\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.070226192474365 | KNN Loss: 5.04154109954834 | BCE Loss: 1.0286850929260254\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.067871570587158 | KNN Loss: 5.049211025238037 | BCE Loss: 1.018660545349121\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.092220306396484 | KNN Loss: 5.045084476470947 | BCE Loss: 1.047135591506958\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.098539352416992 | KNN Loss: 5.04988956451416 | BCE Loss: 1.048649787902832\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.088823318481445 | KNN Loss: 5.045858860015869 | BCE Loss: 1.0429645776748657\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.0888566970825195 | KNN Loss: 5.051475524902344 | BCE Loss: 1.0373810529708862\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.0441389083862305 | KNN Loss: 5.028697490692139 | BCE Loss: 1.0154414176940918\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.079455375671387 | KNN Loss: 5.029048442840576 | BCE Loss: 1.0504066944122314\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.073246955871582 | KNN Loss: 5.028776168823242 | BCE Loss: 1.044471025466919\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.072727203369141 | KNN Loss: 5.032963752746582 | BCE Loss: 1.0397632122039795\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.063699722290039 | KNN Loss: 5.031339168548584 | BCE Loss: 1.032360315322876\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.078161716461182 | KNN Loss: 5.0317487716674805 | BCE Loss: 1.0464129447937012\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.074191093444824 | KNN Loss: 5.030680179595947 | BCE Loss: 1.0435106754302979\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.0690507888793945 | KNN Loss: 5.03076696395874 | BCE Loss: 1.0382838249206543\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.04765510559082 | KNN Loss: 5.035743236541748 | BCE Loss: 1.0119121074676514\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.056191921234131 | KNN Loss: 5.033535957336426 | BCE Loss: 1.022655963897705\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.073139667510986 | KNN Loss: 5.041172027587891 | BCE Loss: 1.0319677591323853\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.075573444366455 | KNN Loss: 5.038342475891113 | BCE Loss: 1.0372308492660522\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.119550704956055 | KNN Loss: 5.041322231292725 | BCE Loss: 1.078228235244751\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.072587490081787 | KNN Loss: 5.0369391441345215 | BCE Loss: 1.0356484651565552\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.041206359863281 | KNN Loss: 5.0290913581848145 | BCE Loss: 1.0121150016784668\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.078007221221924 | KNN Loss: 5.039007663726807 | BCE Loss: 1.0389996767044067\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.069673538208008 | KNN Loss: 5.029965877532959 | BCE Loss: 1.039707899093628\n",
      "Epoch   150: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.063235759735107 | KNN Loss: 5.030491828918457 | BCE Loss: 1.0327439308166504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.071098327636719 | KNN Loss: 5.041639804840088 | BCE Loss: 1.0294585227966309\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.099087715148926 | KNN Loss: 5.037156105041504 | BCE Loss: 1.061931848526001\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.077216148376465 | KNN Loss: 5.041511058807373 | BCE Loss: 1.0357049703598022\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.088779449462891 | KNN Loss: 5.032336235046387 | BCE Loss: 1.056443214416504\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.077906608581543 | KNN Loss: 5.03281831741333 | BCE Loss: 1.0450884103775024\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.090187072753906 | KNN Loss: 5.032061576843262 | BCE Loss: 1.058125376701355\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.081716537475586 | KNN Loss: 5.032903671264648 | BCE Loss: 1.0488128662109375\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.104953765869141 | KNN Loss: 5.046867370605469 | BCE Loss: 1.058086633682251\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.080545902252197 | KNN Loss: 5.051091194152832 | BCE Loss: 1.0294547080993652\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.056995391845703 | KNN Loss: 5.033372402191162 | BCE Loss: 1.023622989654541\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.083661079406738 | KNN Loss: 5.031364440917969 | BCE Loss: 1.052296757698059\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.069267749786377 | KNN Loss: 5.032620906829834 | BCE Loss: 1.036646842956543\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.053761959075928 | KNN Loss: 5.030806064605713 | BCE Loss: 1.0229558944702148\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.077122688293457 | KNN Loss: 5.049249649047852 | BCE Loss: 1.0278730392456055\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.083434104919434 | KNN Loss: 5.026749134063721 | BCE Loss: 1.0566850900650024\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.0363335609436035 | KNN Loss: 5.031506061553955 | BCE Loss: 1.0048273801803589\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.058887004852295 | KNN Loss: 5.029500961303711 | BCE Loss: 1.029386043548584\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.070136070251465 | KNN Loss: 5.027541160583496 | BCE Loss: 1.0425951480865479\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.063510417938232 | KNN Loss: 5.029278755187988 | BCE Loss: 1.0342316627502441\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.1104278564453125 | KNN Loss: 5.035286903381348 | BCE Loss: 1.0751410722732544\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.072697639465332 | KNN Loss: 5.026925086975098 | BCE Loss: 1.0457725524902344\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.068899154663086 | KNN Loss: 5.048182964324951 | BCE Loss: 1.0207161903381348\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.079988479614258 | KNN Loss: 5.042393207550049 | BCE Loss: 1.0375950336456299\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.065557479858398 | KNN Loss: 5.036793231964111 | BCE Loss: 1.028764247894287\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.088927268981934 | KNN Loss: 5.033423900604248 | BCE Loss: 1.0555031299591064\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.0858354568481445 | KNN Loss: 5.040224075317383 | BCE Loss: 1.0456111431121826\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.092171669006348 | KNN Loss: 5.034465312957764 | BCE Loss: 1.0577061176300049\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.084109306335449 | KNN Loss: 5.050255298614502 | BCE Loss: 1.0338537693023682\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.064955711364746 | KNN Loss: 5.040768146514893 | BCE Loss: 1.0241875648498535\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.082397937774658 | KNN Loss: 5.051761627197266 | BCE Loss: 1.0306364297866821\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.098380088806152 | KNN Loss: 5.039156913757324 | BCE Loss: 1.0592232942581177\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.0954742431640625 | KNN Loss: 5.0376715660095215 | BCE Loss: 1.057802677154541\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.087225914001465 | KNN Loss: 5.033342361450195 | BCE Loss: 1.05388343334198\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.066013336181641 | KNN Loss: 5.023731231689453 | BCE Loss: 1.0422818660736084\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.033758163452148 | KNN Loss: 5.028975009918213 | BCE Loss: 1.0047829151153564\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.0763654708862305 | KNN Loss: 5.033895015716553 | BCE Loss: 1.0424703359603882\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.093438148498535 | KNN Loss: 5.04172420501709 | BCE Loss: 1.0517137050628662\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.060880661010742 | KNN Loss: 5.031923770904541 | BCE Loss: 1.0289571285247803\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.082484245300293 | KNN Loss: 5.045441627502441 | BCE Loss: 1.0370426177978516\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.07779598236084 | KNN Loss: 5.034767150878906 | BCE Loss: 1.0430288314819336\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.104291915893555 | KNN Loss: 5.0386810302734375 | BCE Loss: 1.0656108856201172\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.0437116622924805 | KNN Loss: 5.029768943786621 | BCE Loss: 1.0139427185058594\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.052437782287598 | KNN Loss: 5.031896591186523 | BCE Loss: 1.0205409526824951\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.0465593338012695 | KNN Loss: 5.027680397033691 | BCE Loss: 1.0188789367675781\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.079816818237305 | KNN Loss: 5.030341148376465 | BCE Loss: 1.049475908279419\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.05972957611084 | KNN Loss: 5.053176403045654 | BCE Loss: 1.0065529346466064\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.090519905090332 | KNN Loss: 5.033237934112549 | BCE Loss: 1.0572819709777832\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.064215660095215 | KNN Loss: 5.041970729827881 | BCE Loss: 1.0222446918487549\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.039412498474121 | KNN Loss: 5.035109043121338 | BCE Loss: 1.0043034553527832\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.089611053466797 | KNN Loss: 5.0382843017578125 | BCE Loss: 1.0513265132904053\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.07263708114624 | KNN Loss: 5.0313897132873535 | BCE Loss: 1.0412473678588867\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.061184883117676 | KNN Loss: 5.034068584442139 | BCE Loss: 1.0271165370941162\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.082401275634766 | KNN Loss: 5.035991668701172 | BCE Loss: 1.0464093685150146\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.063529968261719 | KNN Loss: 5.028277397155762 | BCE Loss: 1.035252571105957\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.04646110534668 | KNN Loss: 5.04053258895874 | BCE Loss: 1.0059285163879395\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.065648078918457 | KNN Loss: 5.035575866699219 | BCE Loss: 1.0300723314285278\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.0829854011535645 | KNN Loss: 5.0396342277526855 | BCE Loss: 1.0433512926101685\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.087677001953125 | KNN Loss: 5.029453754425049 | BCE Loss: 1.058223009109497\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.054174423217773 | KNN Loss: 5.03586483001709 | BCE Loss: 1.0183095932006836\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.098597526550293 | KNN Loss: 5.035020351409912 | BCE Loss: 1.0635769367218018\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.095919132232666 | KNN Loss: 5.028115749359131 | BCE Loss: 1.0678033828735352\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.085084915161133 | KNN Loss: 5.038934707641602 | BCE Loss: 1.0461499691009521\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.06218147277832 | KNN Loss: 5.032675743103027 | BCE Loss: 1.0295058488845825\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.075387477874756 | KNN Loss: 5.034908771514893 | BCE Loss: 1.0404788255691528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.068413734436035 | KNN Loss: 5.038077354431152 | BCE Loss: 1.0303362607955933\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.089548587799072 | KNN Loss: 5.028110504150391 | BCE Loss: 1.0614380836486816\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.0749616622924805 | KNN Loss: 5.030587673187256 | BCE Loss: 1.0443741083145142\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.071917533874512 | KNN Loss: 5.03441047668457 | BCE Loss: 1.0375070571899414\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.086417198181152 | KNN Loss: 5.027633190155029 | BCE Loss: 1.058783769607544\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.042969703674316 | KNN Loss: 5.027047634124756 | BCE Loss: 1.0159218311309814\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.066227912902832 | KNN Loss: 5.030499458312988 | BCE Loss: 1.0357283353805542\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.105352401733398 | KNN Loss: 5.026792049407959 | BCE Loss: 1.0785603523254395\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.053370952606201 | KNN Loss: 5.0262932777404785 | BCE Loss: 1.0270776748657227\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.065948009490967 | KNN Loss: 5.028696537017822 | BCE Loss: 1.0372514724731445\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.066357612609863 | KNN Loss: 5.030584335327148 | BCE Loss: 1.0357731580734253\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.092543601989746 | KNN Loss: 5.036900043487549 | BCE Loss: 1.0556435585021973\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.089224815368652 | KNN Loss: 5.048911094665527 | BCE Loss: 1.040313959121704\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.100306034088135 | KNN Loss: 5.038512706756592 | BCE Loss: 1.0617934465408325\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.110383033752441 | KNN Loss: 5.057766437530518 | BCE Loss: 1.0526163578033447\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.064996719360352 | KNN Loss: 5.031010627746582 | BCE Loss: 1.03398597240448\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.062122344970703 | KNN Loss: 5.038448333740234 | BCE Loss: 1.0236740112304688\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.113280773162842 | KNN Loss: 5.037437438964844 | BCE Loss: 1.075843334197998\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.042809009552002 | KNN Loss: 5.025698661804199 | BCE Loss: 1.0171102285385132\n",
      "Epoch   164: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.104748249053955 | KNN Loss: 5.0365471839904785 | BCE Loss: 1.068200945854187\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.124009132385254 | KNN Loss: 5.054777145385742 | BCE Loss: 1.0692322254180908\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.055632591247559 | KNN Loss: 5.039113521575928 | BCE Loss: 1.01651930809021\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.059750080108643 | KNN Loss: 5.0461344718933105 | BCE Loss: 1.0136154890060425\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.1116437911987305 | KNN Loss: 5.065728664398193 | BCE Loss: 1.0459152460098267\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.093698501586914 | KNN Loss: 5.030369758605957 | BCE Loss: 1.063328504562378\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.050492286682129 | KNN Loss: 5.030004024505615 | BCE Loss: 1.0204880237579346\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.0989179611206055 | KNN Loss: 5.041304111480713 | BCE Loss: 1.0576138496398926\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.085391044616699 | KNN Loss: 5.033158779144287 | BCE Loss: 1.0522325038909912\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.076673984527588 | KNN Loss: 5.0411577224731445 | BCE Loss: 1.035516381263733\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.067692756652832 | KNN Loss: 5.031492233276367 | BCE Loss: 1.036200761795044\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.10094690322876 | KNN Loss: 5.052098274230957 | BCE Loss: 1.0488487482070923\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.040920257568359 | KNN Loss: 5.03080940246582 | BCE Loss: 1.01011061668396\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.066225528717041 | KNN Loss: 5.048418998718262 | BCE Loss: 1.0178064107894897\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.059460163116455 | KNN Loss: 5.0344319343566895 | BCE Loss: 1.0250282287597656\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.0717291831970215 | KNN Loss: 5.02939510345459 | BCE Loss: 1.0423340797424316\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.0828728675842285 | KNN Loss: 5.034387588500977 | BCE Loss: 1.0484851598739624\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.109407901763916 | KNN Loss: 5.039475917816162 | BCE Loss: 1.069931983947754\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.071483135223389 | KNN Loss: 5.032224655151367 | BCE Loss: 1.0392584800720215\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.054550647735596 | KNN Loss: 5.028684616088867 | BCE Loss: 1.025866150856018\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.065685749053955 | KNN Loss: 5.04679536819458 | BCE Loss: 1.0188902616500854\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.087603569030762 | KNN Loss: 5.040866374969482 | BCE Loss: 1.0467374324798584\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.0214152336120605 | KNN Loss: 5.025674343109131 | BCE Loss: 0.9957407712936401\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.067404747009277 | KNN Loss: 5.0252485275268555 | BCE Loss: 1.0421563386917114\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.061522483825684 | KNN Loss: 5.030992031097412 | BCE Loss: 1.0305302143096924\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.080604553222656 | KNN Loss: 5.027318477630615 | BCE Loss: 1.053286075592041\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.074886798858643 | KNN Loss: 5.0333733558654785 | BCE Loss: 1.0415135622024536\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.073638439178467 | KNN Loss: 5.029808044433594 | BCE Loss: 1.0438302755355835\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.097715377807617 | KNN Loss: 5.0311384201049805 | BCE Loss: 1.0665770769119263\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.108241081237793 | KNN Loss: 5.048572540283203 | BCE Loss: 1.0596683025360107\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.049067497253418 | KNN Loss: 5.036252975463867 | BCE Loss: 1.0128146409988403\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.0902509689331055 | KNN Loss: 5.043207168579102 | BCE Loss: 1.047043800354004\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.068833351135254 | KNN Loss: 5.03188419342041 | BCE Loss: 1.0369493961334229\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.100226402282715 | KNN Loss: 5.02817440032959 | BCE Loss: 1.072051763534546\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.085147857666016 | KNN Loss: 5.0333685874938965 | BCE Loss: 1.0517795085906982\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.088044166564941 | KNN Loss: 5.035694599151611 | BCE Loss: 1.0523498058319092\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.078920364379883 | KNN Loss: 5.054337024688721 | BCE Loss: 1.024583101272583\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.062525749206543 | KNN Loss: 5.026758193969727 | BCE Loss: 1.0357675552368164\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.116396903991699 | KNN Loss: 5.035231590270996 | BCE Loss: 1.081165075302124\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.054848670959473 | KNN Loss: 5.037341117858887 | BCE Loss: 1.017507553100586\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.112491130828857 | KNN Loss: 5.0655999183654785 | BCE Loss: 1.0468913316726685\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.094268798828125 | KNN Loss: 5.0312700271606445 | BCE Loss: 1.0629987716674805\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.092698574066162 | KNN Loss: 5.028820037841797 | BCE Loss: 1.0638786554336548\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.071845531463623 | KNN Loss: 5.0264177322387695 | BCE Loss: 1.0454277992248535\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.080975532531738 | KNN Loss: 5.0316901206970215 | BCE Loss: 1.049285650253296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.078695297241211 | KNN Loss: 5.036757946014404 | BCE Loss: 1.0419371128082275\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.061735153198242 | KNN Loss: 5.024326801300049 | BCE Loss: 1.0374085903167725\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.0650248527526855 | KNN Loss: 5.032693386077881 | BCE Loss: 1.0323314666748047\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.087860584259033 | KNN Loss: 5.029001712799072 | BCE Loss: 1.0588587522506714\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.058897018432617 | KNN Loss: 5.031718730926514 | BCE Loss: 1.0271785259246826\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.109053134918213 | KNN Loss: 5.049563407897949 | BCE Loss: 1.0594897270202637\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.071697235107422 | KNN Loss: 5.035006999969482 | BCE Loss: 1.036690354347229\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.093623161315918 | KNN Loss: 5.033542633056641 | BCE Loss: 1.0600807666778564\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.103703498840332 | KNN Loss: 5.05019998550415 | BCE Loss: 1.0535036325454712\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.065890312194824 | KNN Loss: 5.041126728057861 | BCE Loss: 1.0247633457183838\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.078213691711426 | KNN Loss: 5.0332417488098145 | BCE Loss: 1.0449717044830322\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.077162742614746 | KNN Loss: 5.0522780418396 | BCE Loss: 1.0248847007751465\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.071200847625732 | KNN Loss: 5.031309604644775 | BCE Loss: 1.0398911237716675\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.075934410095215 | KNN Loss: 5.031895637512207 | BCE Loss: 1.0440387725830078\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.097573280334473 | KNN Loss: 5.036718368530273 | BCE Loss: 1.0608551502227783\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.082370281219482 | KNN Loss: 5.0284833908081055 | BCE Loss: 1.053886890411377\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.12026309967041 | KNN Loss: 5.033397197723389 | BCE Loss: 1.086865782737732\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.108837127685547 | KNN Loss: 5.065789222717285 | BCE Loss: 1.0430479049682617\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.060240745544434 | KNN Loss: 5.043296813964844 | BCE Loss: 1.0169439315795898\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.060956001281738 | KNN Loss: 5.031126022338867 | BCE Loss: 1.029829978942871\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.078169345855713 | KNN Loss: 5.046205997467041 | BCE Loss: 1.0319632291793823\n",
      "Epoch   175: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.045832633972168 | KNN Loss: 5.0323052406311035 | BCE Loss: 1.0135271549224854\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.038584232330322 | KNN Loss: 5.03796911239624 | BCE Loss: 1.000615119934082\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.0699357986450195 | KNN Loss: 5.0348286628723145 | BCE Loss: 1.035106897354126\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.087803840637207 | KNN Loss: 5.058868885040283 | BCE Loss: 1.0289349555969238\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.07675313949585 | KNN Loss: 5.039553642272949 | BCE Loss: 1.0371994972229004\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.056733131408691 | KNN Loss: 5.034911632537842 | BCE Loss: 1.0218212604522705\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.0773115158081055 | KNN Loss: 5.040586948394775 | BCE Loss: 1.0367246866226196\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.059376239776611 | KNN Loss: 5.031295299530029 | BCE Loss: 1.028080940246582\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.0870256423950195 | KNN Loss: 5.048279762268066 | BCE Loss: 1.0387461185455322\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.067188262939453 | KNN Loss: 5.029059410095215 | BCE Loss: 1.0381288528442383\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.081747531890869 | KNN Loss: 5.042344570159912 | BCE Loss: 1.039402961730957\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.072488784790039 | KNN Loss: 5.03225564956665 | BCE Loss: 1.0402331352233887\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.06988525390625 | KNN Loss: 5.026034832000732 | BCE Loss: 1.0438501834869385\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.077860355377197 | KNN Loss: 5.041306018829346 | BCE Loss: 1.0365543365478516\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.086156368255615 | KNN Loss: 5.029689311981201 | BCE Loss: 1.056467056274414\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.072303771972656 | KNN Loss: 5.0295538902282715 | BCE Loss: 1.0427497625350952\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.0870561599731445 | KNN Loss: 5.0448899269104 | BCE Loss: 1.042165994644165\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.076272964477539 | KNN Loss: 5.046631813049316 | BCE Loss: 1.0296409130096436\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.045674800872803 | KNN Loss: 5.034193992614746 | BCE Loss: 1.0114808082580566\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.0673394203186035 | KNN Loss: 5.027601718902588 | BCE Loss: 1.039737582206726\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.0708909034729 | KNN Loss: 5.046414852142334 | BCE Loss: 1.0244759321212769\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.078816890716553 | KNN Loss: 5.032967567443848 | BCE Loss: 1.045849323272705\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.096889019012451 | KNN Loss: 5.030327796936035 | BCE Loss: 1.066561222076416\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.0713605880737305 | KNN Loss: 5.039421558380127 | BCE Loss: 1.0319387912750244\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.072622776031494 | KNN Loss: 5.031559467315674 | BCE Loss: 1.0410634279251099\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.094656944274902 | KNN Loss: 5.0419921875 | BCE Loss: 1.0526645183563232\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.079959869384766 | KNN Loss: 5.031870365142822 | BCE Loss: 1.0480897426605225\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.079719066619873 | KNN Loss: 5.030098915100098 | BCE Loss: 1.0496201515197754\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.104158878326416 | KNN Loss: 5.041818618774414 | BCE Loss: 1.062340259552002\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.108514785766602 | KNN Loss: 5.060892581939697 | BCE Loss: 1.0476219654083252\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.046458721160889 | KNN Loss: 5.030713081359863 | BCE Loss: 1.0157455205917358\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.064646244049072 | KNN Loss: 5.030348777770996 | BCE Loss: 1.0342975854873657\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.096199989318848 | KNN Loss: 5.055452346801758 | BCE Loss: 1.0407477617263794\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.070743083953857 | KNN Loss: 5.042229652404785 | BCE Loss: 1.0285135507583618\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.051913738250732 | KNN Loss: 5.038066387176514 | BCE Loss: 1.0138472318649292\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.10603666305542 | KNN Loss: 5.045493125915527 | BCE Loss: 1.0605435371398926\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.075495719909668 | KNN Loss: 5.035162925720215 | BCE Loss: 1.0403329133987427\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.040770530700684 | KNN Loss: 5.024763584136963 | BCE Loss: 1.0160068273544312\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.056927680969238 | KNN Loss: 5.037393569946289 | BCE Loss: 1.0195341110229492\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.101147651672363 | KNN Loss: 5.037778854370117 | BCE Loss: 1.063368797302246\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.0645432472229 | KNN Loss: 5.038017272949219 | BCE Loss: 1.0265260934829712\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.0930681228637695 | KNN Loss: 5.038781642913818 | BCE Loss: 1.0542863607406616\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.0694684982299805 | KNN Loss: 5.031447410583496 | BCE Loss: 1.0380208492279053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.048887252807617 | KNN Loss: 5.03870964050293 | BCE Loss: 1.0101778507232666\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.09234619140625 | KNN Loss: 5.06146764755249 | BCE Loss: 1.0308787822723389\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.08812952041626 | KNN Loss: 5.026316165924072 | BCE Loss: 1.061813235282898\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.041009426116943 | KNN Loss: 5.030131816864014 | BCE Loss: 1.0108776092529297\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.067170143127441 | KNN Loss: 5.027286529541016 | BCE Loss: 1.0398838520050049\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.075752258300781 | KNN Loss: 5.033078670501709 | BCE Loss: 1.0426733493804932\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.061255931854248 | KNN Loss: 5.03233003616333 | BCE Loss: 1.028925895690918\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.058967590332031 | KNN Loss: 5.030661582946777 | BCE Loss: 1.0283061265945435\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.0811238288879395 | KNN Loss: 5.033664226531982 | BCE Loss: 1.047459602355957\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.047393798828125 | KNN Loss: 5.031461715698242 | BCE Loss: 1.0159320831298828\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.08253288269043 | KNN Loss: 5.030338764190674 | BCE Loss: 1.052194356918335\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.0425567626953125 | KNN Loss: 5.027032375335693 | BCE Loss: 1.0155246257781982\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.051767349243164 | KNN Loss: 5.0270209312438965 | BCE Loss: 1.0247465372085571\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.036149024963379 | KNN Loss: 5.025721073150635 | BCE Loss: 1.0104281902313232\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.084272384643555 | KNN Loss: 5.035098552703857 | BCE Loss: 1.0491737127304077\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.081509590148926 | KNN Loss: 5.036184310913086 | BCE Loss: 1.0453252792358398\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.091057300567627 | KNN Loss: 5.03126859664917 | BCE Loss: 1.059788703918457\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.065817832946777 | KNN Loss: 5.054535865783691 | BCE Loss: 1.0112818479537964\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.07111930847168 | KNN Loss: 5.031006336212158 | BCE Loss: 1.040112853050232\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.085127353668213 | KNN Loss: 5.027284622192383 | BCE Loss: 1.05784273147583\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.077667236328125 | KNN Loss: 5.041616916656494 | BCE Loss: 1.0360503196716309\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.053648948669434 | KNN Loss: 5.033380031585693 | BCE Loss: 1.0202691555023193\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.091527938842773 | KNN Loss: 5.029098033905029 | BCE Loss: 1.0624301433563232\n",
      "Epoch   186: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.0652923583984375 | KNN Loss: 5.027308940887451 | BCE Loss: 1.0379836559295654\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.063326835632324 | KNN Loss: 5.0291924476623535 | BCE Loss: 1.0341343879699707\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.078669548034668 | KNN Loss: 5.034023284912109 | BCE Loss: 1.0446460247039795\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.059229850769043 | KNN Loss: 5.029272079467773 | BCE Loss: 1.029957890510559\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.083198547363281 | KNN Loss: 5.057356834411621 | BCE Loss: 1.0258419513702393\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.076120376586914 | KNN Loss: 5.038591384887695 | BCE Loss: 1.0375292301177979\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.066389560699463 | KNN Loss: 5.030594348907471 | BCE Loss: 1.0357950925827026\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.115083694458008 | KNN Loss: 5.046729564666748 | BCE Loss: 1.0683541297912598\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.0942158699035645 | KNN Loss: 5.043201446533203 | BCE Loss: 1.0510145425796509\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.0754804611206055 | KNN Loss: 5.033766269683838 | BCE Loss: 1.0417141914367676\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.0663909912109375 | KNN Loss: 5.034348487854004 | BCE Loss: 1.0320422649383545\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.066588401794434 | KNN Loss: 5.028961181640625 | BCE Loss: 1.0376274585723877\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.084242820739746 | KNN Loss: 5.025060653686523 | BCE Loss: 1.0591821670532227\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.089607238769531 | KNN Loss: 5.0309038162231445 | BCE Loss: 1.0587034225463867\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.090516567230225 | KNN Loss: 5.032351493835449 | BCE Loss: 1.0581650733947754\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.028796195983887 | KNN Loss: 5.028267860412598 | BCE Loss: 1.000528335571289\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.035302639007568 | KNN Loss: 5.0347723960876465 | BCE Loss: 1.0005302429199219\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.04557991027832 | KNN Loss: 5.0292792320251465 | BCE Loss: 1.016300916671753\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.046724319458008 | KNN Loss: 5.03113317489624 | BCE Loss: 1.015591025352478\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.0779852867126465 | KNN Loss: 5.0254716873168945 | BCE Loss: 1.0525137186050415\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.071948528289795 | KNN Loss: 5.033895492553711 | BCE Loss: 1.038053035736084\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.05239725112915 | KNN Loss: 5.036924839019775 | BCE Loss: 1.0154722929000854\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.0594000816345215 | KNN Loss: 5.030035495758057 | BCE Loss: 1.0293647050857544\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.068214416503906 | KNN Loss: 5.028008460998535 | BCE Loss: 1.040205955505371\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.049873352050781 | KNN Loss: 5.0361528396606445 | BCE Loss: 1.0137203931808472\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.070349216461182 | KNN Loss: 5.03660774230957 | BCE Loss: 1.0337415933609009\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.062943935394287 | KNN Loss: 5.0290961265563965 | BCE Loss: 1.0338478088378906\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.0721611976623535 | KNN Loss: 5.035636901855469 | BCE Loss: 1.0365244150161743\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.081622123718262 | KNN Loss: 5.0286455154418945 | BCE Loss: 1.0529766082763672\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.060399055480957 | KNN Loss: 5.031724452972412 | BCE Loss: 1.028674602508545\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.070621490478516 | KNN Loss: 5.042128086090088 | BCE Loss: 1.0284931659698486\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.067596435546875 | KNN Loss: 5.029898643493652 | BCE Loss: 1.0376980304718018\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.089493751525879 | KNN Loss: 5.034054279327393 | BCE Loss: 1.0554397106170654\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.128771781921387 | KNN Loss: 5.040343284606934 | BCE Loss: 1.088428258895874\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.064158916473389 | KNN Loss: 5.032577991485596 | BCE Loss: 1.0315810441970825\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.078328609466553 | KNN Loss: 5.050904273986816 | BCE Loss: 1.0274242162704468\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.061278343200684 | KNN Loss: 5.037835597991943 | BCE Loss: 1.0234426259994507\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.048501491546631 | KNN Loss: 5.033952236175537 | BCE Loss: 1.0145493745803833\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.063861846923828 | KNN Loss: 5.037290573120117 | BCE Loss: 1.026571273803711\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.093350887298584 | KNN Loss: 5.04379415512085 | BCE Loss: 1.049556851387024\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.0802507400512695 | KNN Loss: 5.0270891189575195 | BCE Loss: 1.053161859512329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.0494842529296875 | KNN Loss: 5.026808261871338 | BCE Loss: 1.0226757526397705\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.111610412597656 | KNN Loss: 5.039653778076172 | BCE Loss: 1.0719566345214844\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.069733142852783 | KNN Loss: 5.057985305786133 | BCE Loss: 1.0117478370666504\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.081287384033203 | KNN Loss: 5.043806552886963 | BCE Loss: 1.0374810695648193\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.087515830993652 | KNN Loss: 5.041334629058838 | BCE Loss: 1.046181082725525\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.075685501098633 | KNN Loss: 5.03734016418457 | BCE Loss: 1.0383450984954834\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.0766096115112305 | KNN Loss: 5.027439117431641 | BCE Loss: 1.0491702556610107\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.062281608581543 | KNN Loss: 5.030760288238525 | BCE Loss: 1.0315215587615967\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.057251930236816 | KNN Loss: 5.027595043182373 | BCE Loss: 1.0296566486358643\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.084476947784424 | KNN Loss: 5.029341220855713 | BCE Loss: 1.055135726928711\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.065489292144775 | KNN Loss: 5.05029296875 | BCE Loss: 1.0151962041854858\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.104974746704102 | KNN Loss: 5.051284313201904 | BCE Loss: 1.0536901950836182\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.088714599609375 | KNN Loss: 5.03285551071167 | BCE Loss: 1.055859088897705\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.0731611251831055 | KNN Loss: 5.026467800140381 | BCE Loss: 1.0466930866241455\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.095855712890625 | KNN Loss: 5.0355706214904785 | BCE Loss: 1.0602853298187256\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.054364204406738 | KNN Loss: 5.037213325500488 | BCE Loss: 1.01715087890625\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.084629058837891 | KNN Loss: 5.040062427520752 | BCE Loss: 1.0445668697357178\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.072470664978027 | KNN Loss: 5.034850597381592 | BCE Loss: 1.0376203060150146\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.07111120223999 | KNN Loss: 5.0260910987854 | BCE Loss: 1.0450201034545898\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.064331531524658 | KNN Loss: 5.039003849029541 | BCE Loss: 1.0253275632858276\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.049955368041992 | KNN Loss: 5.027058124542236 | BCE Loss: 1.022897481918335\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.073263168334961 | KNN Loss: 5.032716274261475 | BCE Loss: 1.0405471324920654\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.104226589202881 | KNN Loss: 5.044408321380615 | BCE Loss: 1.0598182678222656\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.109748363494873 | KNN Loss: 5.043717384338379 | BCE Loss: 1.0660309791564941\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.0828938484191895 | KNN Loss: 5.029969215393066 | BCE Loss: 1.0529245138168335\n",
      "Epoch   197: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.097005844116211 | KNN Loss: 5.055041313171387 | BCE Loss: 1.0419647693634033\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.063796043395996 | KNN Loss: 5.03419828414917 | BCE Loss: 1.0295978784561157\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.044110298156738 | KNN Loss: 5.04054069519043 | BCE Loss: 1.0035696029663086\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.08212947845459 | KNN Loss: 5.03554105758667 | BCE Loss: 1.04658842086792\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.078012943267822 | KNN Loss: 5.032690525054932 | BCE Loss: 1.0453224182128906\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.067446708679199 | KNN Loss: 5.042978763580322 | BCE Loss: 1.024468183517456\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.04450798034668 | KNN Loss: 5.0372490882873535 | BCE Loss: 1.0072591304779053\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.09998083114624 | KNN Loss: 5.0406718254089355 | BCE Loss: 1.0593090057373047\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.05869722366333 | KNN Loss: 5.031965255737305 | BCE Loss: 1.0267319679260254\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.109095573425293 | KNN Loss: 5.048056125640869 | BCE Loss: 1.061039686203003\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.103374481201172 | KNN Loss: 5.070816516876221 | BCE Loss: 1.032557725906372\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.077944755554199 | KNN Loss: 5.044302940368652 | BCE Loss: 1.0336415767669678\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.06376314163208 | KNN Loss: 5.032077789306641 | BCE Loss: 1.0316853523254395\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.09614372253418 | KNN Loss: 5.051563739776611 | BCE Loss: 1.0445798635482788\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.077873229980469 | KNN Loss: 5.026486396789551 | BCE Loss: 1.051387071609497\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.089258193969727 | KNN Loss: 5.036790370941162 | BCE Loss: 1.0524678230285645\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.06382942199707 | KNN Loss: 5.037261962890625 | BCE Loss: 1.0265672206878662\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.074737548828125 | KNN Loss: 5.030583381652832 | BCE Loss: 1.044154405593872\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.045648574829102 | KNN Loss: 5.031013011932373 | BCE Loss: 1.0146353244781494\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.090215682983398 | KNN Loss: 5.033618927001953 | BCE Loss: 1.0565967559814453\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.0920634269714355 | KNN Loss: 5.055411338806152 | BCE Loss: 1.0366522073745728\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.074016094207764 | KNN Loss: 5.032392978668213 | BCE Loss: 1.0416231155395508\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.046724319458008 | KNN Loss: 5.031449317932129 | BCE Loss: 1.0152747631072998\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.101930618286133 | KNN Loss: 5.03454065322876 | BCE Loss: 1.067389965057373\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.059525966644287 | KNN Loss: 5.041605472564697 | BCE Loss: 1.0179203748703003\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.075171947479248 | KNN Loss: 5.03455114364624 | BCE Loss: 1.0406208038330078\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.085616588592529 | KNN Loss: 5.039575576782227 | BCE Loss: 1.0460408926010132\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.094720840454102 | KNN Loss: 5.039392948150635 | BCE Loss: 1.055328130722046\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.0925116539001465 | KNN Loss: 5.050705909729004 | BCE Loss: 1.041805624961853\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.054778099060059 | KNN Loss: 5.044168472290039 | BCE Loss: 1.0106098651885986\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.067737579345703 | KNN Loss: 5.045520305633545 | BCE Loss: 1.0222172737121582\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.035589218139648 | KNN Loss: 5.02951717376709 | BCE Loss: 1.0060720443725586\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.069763660430908 | KNN Loss: 5.038578987121582 | BCE Loss: 1.0311846733093262\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.0524725914001465 | KNN Loss: 5.028441905975342 | BCE Loss: 1.0240306854248047\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.078886985778809 | KNN Loss: 5.043347358703613 | BCE Loss: 1.0355397462844849\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.102775573730469 | KNN Loss: 5.040096759796143 | BCE Loss: 1.0626788139343262\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.098128318786621 | KNN Loss: 5.044558048248291 | BCE Loss: 1.05357027053833\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.049263954162598 | KNN Loss: 5.031670570373535 | BCE Loss: 1.017593264579773\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.082319259643555 | KNN Loss: 5.050786972045898 | BCE Loss: 1.0315324068069458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.103603839874268 | KNN Loss: 5.028852462768555 | BCE Loss: 1.0747514963150024\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.071135520935059 | KNN Loss: 5.041805744171143 | BCE Loss: 1.0293298959732056\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.050136566162109 | KNN Loss: 5.03346061706543 | BCE Loss: 1.0166759490966797\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.067919731140137 | KNN Loss: 5.04030704498291 | BCE Loss: 1.0276126861572266\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.064104080200195 | KNN Loss: 5.040430545806885 | BCE Loss: 1.0236736536026\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.06163215637207 | KNN Loss: 5.033855438232422 | BCE Loss: 1.0277769565582275\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.060523509979248 | KNN Loss: 5.0311737060546875 | BCE Loss: 1.02934992313385\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.084150314331055 | KNN Loss: 5.037420272827148 | BCE Loss: 1.0467299222946167\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.0862226486206055 | KNN Loss: 5.041951656341553 | BCE Loss: 1.0442712306976318\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.070358753204346 | KNN Loss: 5.027188301086426 | BCE Loss: 1.04317045211792\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.110284805297852 | KNN Loss: 5.0453643798828125 | BCE Loss: 1.0649206638336182\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.081358432769775 | KNN Loss: 5.035193920135498 | BCE Loss: 1.0461645126342773\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.07354736328125 | KNN Loss: 5.028451919555664 | BCE Loss: 1.045095443725586\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.053290367126465 | KNN Loss: 5.031330108642578 | BCE Loss: 1.0219602584838867\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.061581134796143 | KNN Loss: 5.03101921081543 | BCE Loss: 1.030561923980713\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.067873001098633 | KNN Loss: 5.027515888214111 | BCE Loss: 1.0403568744659424\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.065643787384033 | KNN Loss: 5.028564453125 | BCE Loss: 1.0370794534683228\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.070925712585449 | KNN Loss: 5.030816078186035 | BCE Loss: 1.040109634399414\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.088726043701172 | KNN Loss: 5.0398783683776855 | BCE Loss: 1.0488479137420654\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.071407318115234 | KNN Loss: 5.039984703063965 | BCE Loss: 1.0314223766326904\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.063478469848633 | KNN Loss: 5.0339250564575195 | BCE Loss: 1.0295531749725342\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.050119400024414 | KNN Loss: 5.031841278076172 | BCE Loss: 1.0182783603668213\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.098806381225586 | KNN Loss: 5.044102191925049 | BCE Loss: 1.054704189300537\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.039904594421387 | KNN Loss: 5.031831741333008 | BCE Loss: 1.0080726146697998\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.0889081954956055 | KNN Loss: 5.036796569824219 | BCE Loss: 1.0521117448806763\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.119800567626953 | KNN Loss: 5.042417049407959 | BCE Loss: 1.077383279800415\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.064408302307129 | KNN Loss: 5.027400016784668 | BCE Loss: 1.0370084047317505\n",
      "Epoch   208: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.070583343505859 | KNN Loss: 5.036590576171875 | BCE Loss: 1.0339925289154053\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.066212177276611 | KNN Loss: 5.03378438949585 | BCE Loss: 1.0324279069900513\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.0527215003967285 | KNN Loss: 5.040966987609863 | BCE Loss: 1.0117543935775757\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.0980143547058105 | KNN Loss: 5.033913612365723 | BCE Loss: 1.0641006231307983\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.07848596572876 | KNN Loss: 5.034591197967529 | BCE Loss: 1.0438947677612305\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.060516357421875 | KNN Loss: 5.040024280548096 | BCE Loss: 1.0204918384552002\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.053768634796143 | KNN Loss: 5.025473594665527 | BCE Loss: 1.0282950401306152\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.097801208496094 | KNN Loss: 5.069597244262695 | BCE Loss: 1.0282037258148193\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.081103324890137 | KNN Loss: 5.030066967010498 | BCE Loss: 1.0510365962982178\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.063715934753418 | KNN Loss: 5.0276994705200195 | BCE Loss: 1.0360167026519775\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.054409980773926 | KNN Loss: 5.030790328979492 | BCE Loss: 1.023619532585144\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.0905351638793945 | KNN Loss: 5.042265892028809 | BCE Loss: 1.0482690334320068\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.075058937072754 | KNN Loss: 5.024233818054199 | BCE Loss: 1.0508253574371338\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.050469398498535 | KNN Loss: 5.028707027435303 | BCE Loss: 1.0217626094818115\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.088171005249023 | KNN Loss: 5.036725044250488 | BCE Loss: 1.0514460802078247\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.077606678009033 | KNN Loss: 5.039577960968018 | BCE Loss: 1.038028597831726\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.074669361114502 | KNN Loss: 5.036257266998291 | BCE Loss: 1.0384119749069214\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.071212291717529 | KNN Loss: 5.033589839935303 | BCE Loss: 1.037622332572937\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.054996967315674 | KNN Loss: 5.0300679206848145 | BCE Loss: 1.0249290466308594\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.098634243011475 | KNN Loss: 5.03400182723999 | BCE Loss: 1.0646322965621948\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.126745223999023 | KNN Loss: 5.046974182128906 | BCE Loss: 1.079770803451538\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.101097106933594 | KNN Loss: 5.051220417022705 | BCE Loss: 1.0498766899108887\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.057938575744629 | KNN Loss: 5.044239521026611 | BCE Loss: 1.0136991739273071\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.054195404052734 | KNN Loss: 5.030030727386475 | BCE Loss: 1.0241644382476807\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.079246997833252 | KNN Loss: 5.032957077026367 | BCE Loss: 1.0462898015975952\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.045304298400879 | KNN Loss: 5.02701473236084 | BCE Loss: 1.0182898044586182\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.053338050842285 | KNN Loss: 5.026971817016602 | BCE Loss: 1.0263662338256836\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.074824810028076 | KNN Loss: 5.02772855758667 | BCE Loss: 1.0470963716506958\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.087978363037109 | KNN Loss: 5.03300666809082 | BCE Loss: 1.0549719333648682\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.054899215698242 | KNN Loss: 5.027481555938721 | BCE Loss: 1.0274176597595215\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.099891185760498 | KNN Loss: 5.040769100189209 | BCE Loss: 1.0591219663619995\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.093242168426514 | KNN Loss: 5.043189525604248 | BCE Loss: 1.050052523612976\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.067373752593994 | KNN Loss: 5.030129909515381 | BCE Loss: 1.0372438430786133\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.100208759307861 | KNN Loss: 5.042924880981445 | BCE Loss: 1.0572837591171265\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.0699028968811035 | KNN Loss: 5.0261454582214355 | BCE Loss: 1.0437573194503784\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.055561065673828 | KNN Loss: 5.02614164352417 | BCE Loss: 1.029419183731079\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.059090614318848 | KNN Loss: 5.035586833953857 | BCE Loss: 1.0235037803649902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.08603572845459 | KNN Loss: 5.027030944824219 | BCE Loss: 1.059004545211792\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.065065860748291 | KNN Loss: 5.033153533935547 | BCE Loss: 1.0319123268127441\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.076951026916504 | KNN Loss: 5.036865234375 | BCE Loss: 1.040086030960083\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.08395528793335 | KNN Loss: 5.040464878082275 | BCE Loss: 1.0434904098510742\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.080463886260986 | KNN Loss: 5.031987190246582 | BCE Loss: 1.0484766960144043\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.073736190795898 | KNN Loss: 5.039417743682861 | BCE Loss: 1.034318208694458\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.061346530914307 | KNN Loss: 5.032276630401611 | BCE Loss: 1.0290699005126953\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.0648908615112305 | KNN Loss: 5.046885967254639 | BCE Loss: 1.0180048942565918\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.066647052764893 | KNN Loss: 5.03830623626709 | BCE Loss: 1.0283408164978027\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.08205509185791 | KNN Loss: 5.025679111480713 | BCE Loss: 1.0563762187957764\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.065406322479248 | KNN Loss: 5.044227600097656 | BCE Loss: 1.0211788415908813\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.095558166503906 | KNN Loss: 5.036320209503174 | BCE Loss: 1.0592381954193115\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.085999488830566 | KNN Loss: 5.042362689971924 | BCE Loss: 1.0436370372772217\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.082345008850098 | KNN Loss: 5.032369136810303 | BCE Loss: 1.0499759912490845\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.048712730407715 | KNN Loss: 5.02755069732666 | BCE Loss: 1.0211620330810547\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.091877460479736 | KNN Loss: 5.040177345275879 | BCE Loss: 1.051700234413147\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.062368392944336 | KNN Loss: 5.033149242401123 | BCE Loss: 1.0292189121246338\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.077897548675537 | KNN Loss: 5.028442859649658 | BCE Loss: 1.049454689025879\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.057209014892578 | KNN Loss: 5.029915809631348 | BCE Loss: 1.027293086051941\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.085000038146973 | KNN Loss: 5.055574417114258 | BCE Loss: 1.0294256210327148\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.079063415527344 | KNN Loss: 5.025835037231445 | BCE Loss: 1.0532283782958984\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.061566352844238 | KNN Loss: 5.030665397644043 | BCE Loss: 1.0309009552001953\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.054563522338867 | KNN Loss: 5.041334629058838 | BCE Loss: 1.0132288932800293\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.065649032592773 | KNN Loss: 5.03667688369751 | BCE Loss: 1.0289721488952637\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.083061695098877 | KNN Loss: 5.030381679534912 | BCE Loss: 1.0526800155639648\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.025350570678711 | KNN Loss: 5.030744552612305 | BCE Loss: 0.9946062564849854\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.067082405090332 | KNN Loss: 5.037668704986572 | BCE Loss: 1.0294135808944702\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.080760955810547 | KNN Loss: 5.029565334320068 | BCE Loss: 1.051195740699768\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.141808032989502 | KNN Loss: 5.075057506561279 | BCE Loss: 1.0667505264282227\n",
      "Epoch   219: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.076456069946289 | KNN Loss: 5.040629863739014 | BCE Loss: 1.0358262062072754\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.054408073425293 | KNN Loss: 5.0294108390808105 | BCE Loss: 1.0249974727630615\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.065713882446289 | KNN Loss: 5.031895637512207 | BCE Loss: 1.0338184833526611\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.052338123321533 | KNN Loss: 5.031460762023926 | BCE Loss: 1.0208772420883179\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.070636749267578 | KNN Loss: 5.034199237823486 | BCE Loss: 1.0364375114440918\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.092918872833252 | KNN Loss: 5.028555870056152 | BCE Loss: 1.0643630027770996\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.1095099449157715 | KNN Loss: 5.049989700317383 | BCE Loss: 1.0595202445983887\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.091838836669922 | KNN Loss: 5.043341636657715 | BCE Loss: 1.0484973192214966\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.079768180847168 | KNN Loss: 5.034063816070557 | BCE Loss: 1.0457046031951904\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.084118843078613 | KNN Loss: 5.026247501373291 | BCE Loss: 1.0578711032867432\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.063399314880371 | KNN Loss: 5.031780242919922 | BCE Loss: 1.0316189527511597\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.08172082901001 | KNN Loss: 5.031929016113281 | BCE Loss: 1.0497918128967285\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.0558762550354 | KNN Loss: 5.034877300262451 | BCE Loss: 1.0209988355636597\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.117982864379883 | KNN Loss: 5.074637413024902 | BCE Loss: 1.0433452129364014\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.056482315063477 | KNN Loss: 5.035921096801758 | BCE Loss: 1.0205613374710083\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.086997985839844 | KNN Loss: 5.046713352203369 | BCE Loss: 1.0402848720550537\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.039115905761719 | KNN Loss: 5.0300679206848145 | BCE Loss: 1.0090482234954834\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.092504978179932 | KNN Loss: 5.052698612213135 | BCE Loss: 1.0398063659667969\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.077642440795898 | KNN Loss: 5.027489185333252 | BCE Loss: 1.050153374671936\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.070707321166992 | KNN Loss: 5.027765274047852 | BCE Loss: 1.0429422855377197\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.069238662719727 | KNN Loss: 5.024367809295654 | BCE Loss: 1.0448708534240723\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.0834527015686035 | KNN Loss: 5.036893367767334 | BCE Loss: 1.0465593338012695\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.05015230178833 | KNN Loss: 5.029906272888184 | BCE Loss: 1.0202460289001465\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.067097187042236 | KNN Loss: 5.038374423980713 | BCE Loss: 1.0287226438522339\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.094299793243408 | KNN Loss: 5.047231674194336 | BCE Loss: 1.0470681190490723\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.095954418182373 | KNN Loss: 5.040042400360107 | BCE Loss: 1.0559121370315552\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.0425872802734375 | KNN Loss: 5.023438453674316 | BCE Loss: 1.019148826599121\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.052340507507324 | KNN Loss: 5.029117584228516 | BCE Loss: 1.0232231616973877\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.08112907409668 | KNN Loss: 5.031081199645996 | BCE Loss: 1.0500481128692627\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.083111763000488 | KNN Loss: 5.036728382110596 | BCE Loss: 1.0463833808898926\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.082059383392334 | KNN Loss: 5.02774715423584 | BCE Loss: 1.0543122291564941\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.096828460693359 | KNN Loss: 5.050924301147461 | BCE Loss: 1.045904278755188\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.084511756896973 | KNN Loss: 5.031767845153809 | BCE Loss: 1.052743911743164\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.082899570465088 | KNN Loss: 5.031896591186523 | BCE Loss: 1.0510029792785645\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.095216751098633 | KNN Loss: 5.04748010635376 | BCE Loss: 1.0477368831634521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.041339874267578 | KNN Loss: 5.031439304351807 | BCE Loss: 1.0099003314971924\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.0415143966674805 | KNN Loss: 5.028441429138184 | BCE Loss: 1.0130727291107178\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.065425872802734 | KNN Loss: 5.032253265380859 | BCE Loss: 1.033172369003296\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.065649032592773 | KNN Loss: 5.0324835777282715 | BCE Loss: 1.0331655740737915\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.055556297302246 | KNN Loss: 5.026913166046143 | BCE Loss: 1.0286431312561035\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.106433868408203 | KNN Loss: 5.0446553230285645 | BCE Loss: 1.0617787837982178\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.083904266357422 | KNN Loss: 5.026700019836426 | BCE Loss: 1.057204246520996\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.079084873199463 | KNN Loss: 5.033573627471924 | BCE Loss: 1.045511245727539\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.085547924041748 | KNN Loss: 5.055298328399658 | BCE Loss: 1.0302497148513794\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.066614151000977 | KNN Loss: 5.031923294067383 | BCE Loss: 1.0346908569335938\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.074540138244629 | KNN Loss: 5.029274940490723 | BCE Loss: 1.0452654361724854\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.078472137451172 | KNN Loss: 5.025485515594482 | BCE Loss: 1.0529868602752686\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.073543548583984 | KNN Loss: 5.030567646026611 | BCE Loss: 1.042975902557373\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.071155548095703 | KNN Loss: 5.030044078826904 | BCE Loss: 1.0411115884780884\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.063508033752441 | KNN Loss: 5.037918567657471 | BCE Loss: 1.0255897045135498\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.046220779418945 | KNN Loss: 5.037545204162598 | BCE Loss: 1.0086756944656372\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.03505802154541 | KNN Loss: 5.030367851257324 | BCE Loss: 1.0046899318695068\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.112323760986328 | KNN Loss: 5.067841053009033 | BCE Loss: 1.044482946395874\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.076210021972656 | KNN Loss: 5.034030914306641 | BCE Loss: 1.0421792268753052\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.066202163696289 | KNN Loss: 5.033945560455322 | BCE Loss: 1.032256841659546\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.058041095733643 | KNN Loss: 5.027516841888428 | BCE Loss: 1.0305241346359253\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.0732269287109375 | KNN Loss: 5.052384853363037 | BCE Loss: 1.0208423137664795\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.112306118011475 | KNN Loss: 5.0488667488098145 | BCE Loss: 1.0634393692016602\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.095164775848389 | KNN Loss: 5.03934383392334 | BCE Loss: 1.0558209419250488\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.066909313201904 | KNN Loss: 5.033697128295898 | BCE Loss: 1.0332123041152954\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.093543529510498 | KNN Loss: 5.034489631652832 | BCE Loss: 1.059053897857666\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.046903133392334 | KNN Loss: 5.033326625823975 | BCE Loss: 1.0135763883590698\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.057652473449707 | KNN Loss: 5.030911445617676 | BCE Loss: 1.0267411470413208\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.081965446472168 | KNN Loss: 5.044079780578613 | BCE Loss: 1.0378856658935547\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.073497772216797 | KNN Loss: 5.030258655548096 | BCE Loss: 1.0432393550872803\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.080671310424805 | KNN Loss: 5.032704830169678 | BCE Loss: 1.047966480255127\n",
      "Epoch   230: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.076183319091797 | KNN Loss: 5.031147003173828 | BCE Loss: 1.0450363159179688\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.080841064453125 | KNN Loss: 5.037667274475098 | BCE Loss: 1.0431740283966064\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.081499099731445 | KNN Loss: 5.037713050842285 | BCE Loss: 1.043785810470581\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.071310997009277 | KNN Loss: 5.034527778625488 | BCE Loss: 1.036783218383789\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.095155715942383 | KNN Loss: 5.04683256149292 | BCE Loss: 1.048323392868042\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.0998029708862305 | KNN Loss: 5.026525974273682 | BCE Loss: 1.073277235031128\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.098197937011719 | KNN Loss: 5.03647518157959 | BCE Loss: 1.061722755432129\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.067878246307373 | KNN Loss: 5.034389972686768 | BCE Loss: 1.0334882736206055\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.1110029220581055 | KNN Loss: 5.035617351531982 | BCE Loss: 1.075385570526123\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.077983379364014 | KNN Loss: 5.04429292678833 | BCE Loss: 1.0336904525756836\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.073211669921875 | KNN Loss: 5.0337090492248535 | BCE Loss: 1.039502501487732\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.074370861053467 | KNN Loss: 5.043975830078125 | BCE Loss: 1.0303949117660522\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.046426296234131 | KNN Loss: 5.029938220977783 | BCE Loss: 1.0164880752563477\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.052114009857178 | KNN Loss: 5.031289577484131 | BCE Loss: 1.0208244323730469\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.073281288146973 | KNN Loss: 5.037759304046631 | BCE Loss: 1.0355221033096313\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.063726425170898 | KNN Loss: 5.031581401824951 | BCE Loss: 1.0321450233459473\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.040868759155273 | KNN Loss: 5.028244495391846 | BCE Loss: 1.0126240253448486\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.106995582580566 | KNN Loss: 5.03412389755249 | BCE Loss: 1.0728719234466553\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.094117164611816 | KNN Loss: 5.050424575805664 | BCE Loss: 1.0436925888061523\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.048437595367432 | KNN Loss: 5.031033039093018 | BCE Loss: 1.017404556274414\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.09340763092041 | KNN Loss: 5.0449957847595215 | BCE Loss: 1.0484118461608887\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.045072555541992 | KNN Loss: 5.03420352935791 | BCE Loss: 1.010869026184082\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.064567565917969 | KNN Loss: 5.034919261932373 | BCE Loss: 1.0296484231948853\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.137269973754883 | KNN Loss: 5.063763618469238 | BCE Loss: 1.0735065937042236\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.06827974319458 | KNN Loss: 5.037776947021484 | BCE Loss: 1.0305027961730957\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.078176975250244 | KNN Loss: 5.027317523956299 | BCE Loss: 1.0508594512939453\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.111125946044922 | KNN Loss: 5.035325527191162 | BCE Loss: 1.0758002996444702\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.079916000366211 | KNN Loss: 5.037487030029297 | BCE Loss: 1.042428731918335\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.073101997375488 | KNN Loss: 5.041383266448975 | BCE Loss: 1.0317184925079346\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.07435417175293 | KNN Loss: 5.034585475921631 | BCE Loss: 1.0397686958312988\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.13617467880249 | KNN Loss: 5.0811238288879395 | BCE Loss: 1.0550509691238403\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.0632781982421875 | KNN Loss: 5.025672435760498 | BCE Loss: 1.0376057624816895\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.067261219024658 | KNN Loss: 5.029231071472168 | BCE Loss: 1.0380301475524902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.0731401443481445 | KNN Loss: 5.026750087738037 | BCE Loss: 1.046390175819397\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.082367897033691 | KNN Loss: 5.027311325073242 | BCE Loss: 1.0550568103790283\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.061336517333984 | KNN Loss: 5.034867286682129 | BCE Loss: 1.0264692306518555\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.073512554168701 | KNN Loss: 5.031449794769287 | BCE Loss: 1.0420626401901245\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.075008392333984 | KNN Loss: 5.038653373718262 | BCE Loss: 1.0363550186157227\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.067986488342285 | KNN Loss: 5.029291152954102 | BCE Loss: 1.0386953353881836\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.0874152183532715 | KNN Loss: 5.039962291717529 | BCE Loss: 1.0474530458450317\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.040919303894043 | KNN Loss: 5.037524700164795 | BCE Loss: 1.003394603729248\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.054251194000244 | KNN Loss: 5.030920028686523 | BCE Loss: 1.0233311653137207\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.052528381347656 | KNN Loss: 5.0298991203308105 | BCE Loss: 1.0226293802261353\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.121167182922363 | KNN Loss: 5.05350923538208 | BCE Loss: 1.0676581859588623\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.083424091339111 | KNN Loss: 5.031316757202148 | BCE Loss: 1.052107334136963\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.041270732879639 | KNN Loss: 5.0304741859436035 | BCE Loss: 1.0107966661453247\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.064007759094238 | KNN Loss: 5.027596950531006 | BCE Loss: 1.0364108085632324\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.062552452087402 | KNN Loss: 5.033769130706787 | BCE Loss: 1.0287833213806152\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.052822113037109 | KNN Loss: 5.027744770050049 | BCE Loss: 1.0250775814056396\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.081726551055908 | KNN Loss: 5.0408549308776855 | BCE Loss: 1.0408716201782227\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.037735462188721 | KNN Loss: 5.036460876464844 | BCE Loss: 1.001274585723877\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.108823776245117 | KNN Loss: 5.0415239334106445 | BCE Loss: 1.0672996044158936\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.066306114196777 | KNN Loss: 5.03150749206543 | BCE Loss: 1.0347983837127686\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.0857253074646 | KNN Loss: 5.027065277099609 | BCE Loss: 1.0586599111557007\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.0508880615234375 | KNN Loss: 5.023153305053711 | BCE Loss: 1.0277345180511475\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.080311298370361 | KNN Loss: 5.038033485412598 | BCE Loss: 1.0422778129577637\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.056319236755371 | KNN Loss: 5.0263237953186035 | BCE Loss: 1.0299954414367676\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.110805511474609 | KNN Loss: 5.067196846008301 | BCE Loss: 1.0436089038848877\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.072333812713623 | KNN Loss: 5.038890838623047 | BCE Loss: 1.0334429740905762\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.0909881591796875 | KNN Loss: 5.025771141052246 | BCE Loss: 1.0652170181274414\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.076478958129883 | KNN Loss: 5.027255058288574 | BCE Loss: 1.0492236614227295\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.040154457092285 | KNN Loss: 5.029479503631592 | BCE Loss: 1.0106751918792725\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.081550121307373 | KNN Loss: 5.030223369598389 | BCE Loss: 1.0513267517089844\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.075840473175049 | KNN Loss: 5.025414943695068 | BCE Loss: 1.050425410270691\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.0502142906188965 | KNN Loss: 5.035531520843506 | BCE Loss: 1.0146827697753906\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.047885894775391 | KNN Loss: 5.035297393798828 | BCE Loss: 1.0125882625579834\n",
      "Epoch   241: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.059401988983154 | KNN Loss: 5.0450944900512695 | BCE Loss: 1.0143074989318848\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.068758010864258 | KNN Loss: 5.043200492858887 | BCE Loss: 1.025557279586792\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.084305286407471 | KNN Loss: 5.037737846374512 | BCE Loss: 1.0465675592422485\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.074337005615234 | KNN Loss: 5.029916286468506 | BCE Loss: 1.0444207191467285\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.076846122741699 | KNN Loss: 5.033046722412109 | BCE Loss: 1.0437992811203003\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.080028533935547 | KNN Loss: 5.035163402557373 | BCE Loss: 1.0448652505874634\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.05345344543457 | KNN Loss: 5.026602745056152 | BCE Loss: 1.026850700378418\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.058382987976074 | KNN Loss: 5.027343273162842 | BCE Loss: 1.0310397148132324\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.073227405548096 | KNN Loss: 5.030796527862549 | BCE Loss: 1.0424307584762573\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.090788841247559 | KNN Loss: 5.025099754333496 | BCE Loss: 1.0656890869140625\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.078742980957031 | KNN Loss: 5.063517093658447 | BCE Loss: 1.015225887298584\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.063277721405029 | KNN Loss: 5.027149200439453 | BCE Loss: 1.0361285209655762\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.100778102874756 | KNN Loss: 5.042239665985107 | BCE Loss: 1.058538556098938\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.082167148590088 | KNN Loss: 5.045164108276367 | BCE Loss: 1.0370029211044312\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.059021949768066 | KNN Loss: 5.035736560821533 | BCE Loss: 1.023285150527954\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.049515724182129 | KNN Loss: 5.034473896026611 | BCE Loss: 1.0150415897369385\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.080437660217285 | KNN Loss: 5.041275978088379 | BCE Loss: 1.0391616821289062\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.057929515838623 | KNN Loss: 5.030409336090088 | BCE Loss: 1.0275201797485352\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.086601734161377 | KNN Loss: 5.032450199127197 | BCE Loss: 1.0541515350341797\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.079227447509766 | KNN Loss: 5.032595634460449 | BCE Loss: 1.0466320514678955\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.055017471313477 | KNN Loss: 5.037688732147217 | BCE Loss: 1.0173285007476807\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.067668914794922 | KNN Loss: 5.044797897338867 | BCE Loss: 1.0228712558746338\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.0418381690979 | KNN Loss: 5.032256126403809 | BCE Loss: 1.0095820426940918\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.080538272857666 | KNN Loss: 5.025904178619385 | BCE Loss: 1.0546340942382812\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.053937911987305 | KNN Loss: 5.033039093017578 | BCE Loss: 1.0208990573883057\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.084619998931885 | KNN Loss: 5.048043727874756 | BCE Loss: 1.0365761518478394\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.032252788543701 | KNN Loss: 5.030839920043945 | BCE Loss: 1.0014128684997559\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.117822170257568 | KNN Loss: 5.0654826164245605 | BCE Loss: 1.0523394346237183\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.086383819580078 | KNN Loss: 5.03264045715332 | BCE Loss: 1.0537433624267578\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.105950355529785 | KNN Loss: 5.061631679534912 | BCE Loss: 1.044318437576294\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.079754829406738 | KNN Loss: 5.036100387573242 | BCE Loss: 1.043654441833496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.055229187011719 | KNN Loss: 5.036261558532715 | BCE Loss: 1.0189673900604248\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.089101791381836 | KNN Loss: 5.045258522033691 | BCE Loss: 1.043843388557434\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.055931091308594 | KNN Loss: 5.025661945343018 | BCE Loss: 1.0302691459655762\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.070645809173584 | KNN Loss: 5.028929233551025 | BCE Loss: 1.0417165756225586\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.062268257141113 | KNN Loss: 5.0315093994140625 | BCE Loss: 1.0307588577270508\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.105381965637207 | KNN Loss: 5.043605327606201 | BCE Loss: 1.0617766380310059\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.079785346984863 | KNN Loss: 5.0329155921936035 | BCE Loss: 1.0468697547912598\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.078884124755859 | KNN Loss: 5.036197662353516 | BCE Loss: 1.0426862239837646\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.045764446258545 | KNN Loss: 5.035048961639404 | BCE Loss: 1.010715365409851\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.050574779510498 | KNN Loss: 5.037790298461914 | BCE Loss: 1.012784481048584\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.057614326477051 | KNN Loss: 5.028403282165527 | BCE Loss: 1.0292109251022339\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.087677955627441 | KNN Loss: 5.0280842781066895 | BCE Loss: 1.059593915939331\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.066140174865723 | KNN Loss: 5.031403541564941 | BCE Loss: 1.0347363948822021\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.109805583953857 | KNN Loss: 5.060971736907959 | BCE Loss: 1.0488337278366089\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.101578712463379 | KNN Loss: 5.029482364654541 | BCE Loss: 1.072096347808838\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.020220756530762 | KNN Loss: 5.0329813957214355 | BCE Loss: 0.9872391223907471\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.067584037780762 | KNN Loss: 5.030932426452637 | BCE Loss: 1.0366514921188354\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.0779242515563965 | KNN Loss: 5.0299601554870605 | BCE Loss: 1.0479642152786255\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.052410125732422 | KNN Loss: 5.048993110656738 | BCE Loss: 1.0034172534942627\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.072076797485352 | KNN Loss: 5.030782222747803 | BCE Loss: 1.0412944555282593\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.05080509185791 | KNN Loss: 5.037063121795654 | BCE Loss: 1.0137418508529663\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.126944541931152 | KNN Loss: 5.078650951385498 | BCE Loss: 1.0482938289642334\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.100897312164307 | KNN Loss: 5.051449775695801 | BCE Loss: 1.0494474172592163\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.062791347503662 | KNN Loss: 5.0212483406066895 | BCE Loss: 1.0415430068969727\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.033717155456543 | KNN Loss: 5.024019718170166 | BCE Loss: 1.0096971988677979\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.079862117767334 | KNN Loss: 5.0419230461120605 | BCE Loss: 1.0379390716552734\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.051066875457764 | KNN Loss: 5.026078701019287 | BCE Loss: 1.0249882936477661\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.06203556060791 | KNN Loss: 5.025226593017578 | BCE Loss: 1.036808967590332\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.063370227813721 | KNN Loss: 5.028542518615723 | BCE Loss: 1.034827709197998\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.066187381744385 | KNN Loss: 5.025025844573975 | BCE Loss: 1.0411614179611206\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.077333927154541 | KNN Loss: 5.030819892883301 | BCE Loss: 1.0465141534805298\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.094130992889404 | KNN Loss: 5.039278507232666 | BCE Loss: 1.0548526048660278\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.086556434631348 | KNN Loss: 5.03955078125 | BCE Loss: 1.047005534172058\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.058576583862305 | KNN Loss: 5.040349006652832 | BCE Loss: 1.0182278156280518\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.070309162139893 | KNN Loss: 5.033133506774902 | BCE Loss: 1.0371756553649902\n",
      "Epoch   252: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.081607818603516 | KNN Loss: 5.033624649047852 | BCE Loss: 1.047982931137085\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.088858604431152 | KNN Loss: 5.029191493988037 | BCE Loss: 1.0596673488616943\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.0394511222839355 | KNN Loss: 5.030542850494385 | BCE Loss: 1.0089082717895508\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.069069862365723 | KNN Loss: 5.0264177322387695 | BCE Loss: 1.0426523685455322\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.082921028137207 | KNN Loss: 5.039404392242432 | BCE Loss: 1.0435166358947754\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.030557632446289 | KNN Loss: 5.025187969207764 | BCE Loss: 1.0053695440292358\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.074335098266602 | KNN Loss: 5.0494890213012695 | BCE Loss: 1.0248459577560425\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.061615943908691 | KNN Loss: 5.0395050048828125 | BCE Loss: 1.022110939025879\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.074819564819336 | KNN Loss: 5.03264856338501 | BCE Loss: 1.0421711206436157\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.06685209274292 | KNN Loss: 5.02476692199707 | BCE Loss: 1.0420851707458496\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.103852272033691 | KNN Loss: 5.043578147888184 | BCE Loss: 1.060274362564087\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.0858259201049805 | KNN Loss: 5.036750316619873 | BCE Loss: 1.049075722694397\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.068579196929932 | KNN Loss: 5.030505180358887 | BCE Loss: 1.038074016571045\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.090163707733154 | KNN Loss: 5.038048267364502 | BCE Loss: 1.0521154403686523\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.062380313873291 | KNN Loss: 5.038241863250732 | BCE Loss: 1.024138331413269\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.055886745452881 | KNN Loss: 5.034572601318359 | BCE Loss: 1.021314263343811\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.093432903289795 | KNN Loss: 5.04038667678833 | BCE Loss: 1.0530461072921753\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.10939884185791 | KNN Loss: 5.0385284423828125 | BCE Loss: 1.0708703994750977\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.096789360046387 | KNN Loss: 5.044848442077637 | BCE Loss: 1.051941156387329\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.090912818908691 | KNN Loss: 5.0341997146606445 | BCE Loss: 1.0567129850387573\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.087062835693359 | KNN Loss: 5.033022403717041 | BCE Loss: 1.0540404319763184\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.086801528930664 | KNN Loss: 5.053353309631348 | BCE Loss: 1.0334482192993164\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.063355445861816 | KNN Loss: 5.0344014167785645 | BCE Loss: 1.0289539098739624\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.042401313781738 | KNN Loss: 5.023845672607422 | BCE Loss: 1.0185556411743164\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.096768379211426 | KNN Loss: 5.0429863929748535 | BCE Loss: 1.0537822246551514\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.096981048583984 | KNN Loss: 5.044120788574219 | BCE Loss: 1.052860140800476\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.041694641113281 | KNN Loss: 5.031893253326416 | BCE Loss: 1.0098016262054443\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.083413600921631 | KNN Loss: 5.043910503387451 | BCE Loss: 1.0395029783248901\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.084871292114258 | KNN Loss: 5.03894567489624 | BCE Loss: 1.0459256172180176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.041276931762695 | KNN Loss: 5.02662992477417 | BCE Loss: 1.0146468877792358\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.083417892456055 | KNN Loss: 5.036128997802734 | BCE Loss: 1.0472886562347412\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.05924129486084 | KNN Loss: 5.030652046203613 | BCE Loss: 1.0285892486572266\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.083641052246094 | KNN Loss: 5.033901214599609 | BCE Loss: 1.0497397184371948\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.095284461975098 | KNN Loss: 5.032439708709717 | BCE Loss: 1.0628447532653809\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.07939338684082 | KNN Loss: 5.0299177169799805 | BCE Loss: 1.0494756698608398\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.054522514343262 | KNN Loss: 5.0301947593688965 | BCE Loss: 1.0243279933929443\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.066711902618408 | KNN Loss: 5.036285877227783 | BCE Loss: 1.030426025390625\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.059174060821533 | KNN Loss: 5.0302734375 | BCE Loss: 1.0289007425308228\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.070811748504639 | KNN Loss: 5.038707256317139 | BCE Loss: 1.0321044921875\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.074652671813965 | KNN Loss: 5.026714324951172 | BCE Loss: 1.047938346862793\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.096194267272949 | KNN Loss: 5.03302001953125 | BCE Loss: 1.0631744861602783\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.081576347351074 | KNN Loss: 5.041117191314697 | BCE Loss: 1.040459156036377\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.0713911056518555 | KNN Loss: 5.033079624176025 | BCE Loss: 1.03831148147583\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.0313873291015625 | KNN Loss: 5.024975299835205 | BCE Loss: 1.0064117908477783\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.083722114562988 | KNN Loss: 5.049641132354736 | BCE Loss: 1.034080982208252\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.066645622253418 | KNN Loss: 5.029293537139893 | BCE Loss: 1.0373523235321045\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.075095176696777 | KNN Loss: 5.039666175842285 | BCE Loss: 1.035428762435913\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.063202381134033 | KNN Loss: 5.029016971588135 | BCE Loss: 1.034185528755188\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.073253154754639 | KNN Loss: 5.034147262573242 | BCE Loss: 1.0391058921813965\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.084894180297852 | KNN Loss: 5.030320644378662 | BCE Loss: 1.0545737743377686\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.081697463989258 | KNN Loss: 5.034553050994873 | BCE Loss: 1.0471441745758057\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.070869445800781 | KNN Loss: 5.045296669006348 | BCE Loss: 1.0255730152130127\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.0691328048706055 | KNN Loss: 5.04097843170166 | BCE Loss: 1.0281542539596558\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.09046745300293 | KNN Loss: 5.049652576446533 | BCE Loss: 1.0408146381378174\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.0526814460754395 | KNN Loss: 5.038491725921631 | BCE Loss: 1.0141897201538086\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.072903633117676 | KNN Loss: 5.031777381896973 | BCE Loss: 1.0411263704299927\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.08698034286499 | KNN Loss: 5.029755592346191 | BCE Loss: 1.0572246313095093\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.067515850067139 | KNN Loss: 5.041327476501465 | BCE Loss: 1.0261883735656738\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.033412933349609 | KNN Loss: 5.024704456329346 | BCE Loss: 1.0087084770202637\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.075741291046143 | KNN Loss: 5.034975051879883 | BCE Loss: 1.0407662391662598\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.071462154388428 | KNN Loss: 5.035305500030518 | BCE Loss: 1.0361565351486206\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.1147990226745605 | KNN Loss: 5.049393177032471 | BCE Loss: 1.0654059648513794\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.10929536819458 | KNN Loss: 5.036038875579834 | BCE Loss: 1.073256492614746\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.065786361694336 | KNN Loss: 5.0363240242004395 | BCE Loss: 1.0294623374938965\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.068836212158203 | KNN Loss: 5.03847599029541 | BCE Loss: 1.0303599834442139\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.060633659362793 | KNN Loss: 5.029583930969238 | BCE Loss: 1.0310497283935547\n",
      "Epoch   263: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.038816452026367 | KNN Loss: 5.028376579284668 | BCE Loss: 1.0104398727416992\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.055107593536377 | KNN Loss: 5.0395684242248535 | BCE Loss: 1.015539288520813\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.0704498291015625 | KNN Loss: 5.042061805725098 | BCE Loss: 1.0283877849578857\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.053534984588623 | KNN Loss: 5.025081157684326 | BCE Loss: 1.0284538269042969\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.094889163970947 | KNN Loss: 5.040191650390625 | BCE Loss: 1.0546975135803223\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.110503673553467 | KNN Loss: 5.062972545623779 | BCE Loss: 1.047531247138977\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.1144585609436035 | KNN Loss: 5.03439474105835 | BCE Loss: 1.080063819885254\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.098306179046631 | KNN Loss: 5.051990985870361 | BCE Loss: 1.0463151931762695\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.107876300811768 | KNN Loss: 5.057644367218018 | BCE Loss: 1.05023193359375\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.088446140289307 | KNN Loss: 5.045687675476074 | BCE Loss: 1.0427583456039429\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.057634353637695 | KNN Loss: 5.045109748840332 | BCE Loss: 1.0125243663787842\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.053494453430176 | KNN Loss: 5.033736228942871 | BCE Loss: 1.0197584629058838\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.086931228637695 | KNN Loss: 5.028778076171875 | BCE Loss: 1.0581531524658203\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.070863723754883 | KNN Loss: 5.038341999053955 | BCE Loss: 1.0325216054916382\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.093439102172852 | KNN Loss: 5.066464424133301 | BCE Loss: 1.0269747972488403\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.061617851257324 | KNN Loss: 5.037076473236084 | BCE Loss: 1.0245416164398193\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.098694324493408 | KNN Loss: 5.031215667724609 | BCE Loss: 1.0674786567687988\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.072623252868652 | KNN Loss: 5.0288405418396 | BCE Loss: 1.0437824726104736\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.090703964233398 | KNN Loss: 5.046096324920654 | BCE Loss: 1.0446078777313232\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.060690879821777 | KNN Loss: 5.0315327644348145 | BCE Loss: 1.029158115386963\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.089851379394531 | KNN Loss: 5.0481181144714355 | BCE Loss: 1.0417333841323853\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.1003522872924805 | KNN Loss: 5.043537616729736 | BCE Loss: 1.0568147897720337\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.0540452003479 | KNN Loss: 5.025640487670898 | BCE Loss: 1.028404712677002\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.0564494132995605 | KNN Loss: 5.043772220611572 | BCE Loss: 1.0126770734786987\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.108356475830078 | KNN Loss: 5.063673973083496 | BCE Loss: 1.044682264328003\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.086071014404297 | KNN Loss: 5.038881301879883 | BCE Loss: 1.0471899509429932\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.1157684326171875 | KNN Loss: 5.030078887939453 | BCE Loss: 1.085689663887024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.086468696594238 | KNN Loss: 5.0313401222229 | BCE Loss: 1.055128574371338\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.087555885314941 | KNN Loss: 5.046238422393799 | BCE Loss: 1.0413177013397217\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.056197643280029 | KNN Loss: 5.035429000854492 | BCE Loss: 1.020768642425537\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.072237014770508 | KNN Loss: 5.034692287445068 | BCE Loss: 1.0375444889068604\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.060156345367432 | KNN Loss: 5.029608249664307 | BCE Loss: 1.030548095703125\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.06443977355957 | KNN Loss: 5.037171840667725 | BCE Loss: 1.0272676944732666\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.098935127258301 | KNN Loss: 5.03472375869751 | BCE Loss: 1.064211368560791\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.097809791564941 | KNN Loss: 5.047125816345215 | BCE Loss: 1.0506837368011475\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.066783428192139 | KNN Loss: 5.031368255615234 | BCE Loss: 1.0354152917861938\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.086122035980225 | KNN Loss: 5.040450572967529 | BCE Loss: 1.0456715822219849\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.071683883666992 | KNN Loss: 5.030007362365723 | BCE Loss: 1.0416765213012695\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.054081916809082 | KNN Loss: 5.035595417022705 | BCE Loss: 1.018486738204956\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.051641941070557 | KNN Loss: 5.027641296386719 | BCE Loss: 1.0240005254745483\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.038556098937988 | KNN Loss: 5.028954982757568 | BCE Loss: 1.009601354598999\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.048358917236328 | KNN Loss: 5.037350177764893 | BCE Loss: 1.011008620262146\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.039492607116699 | KNN Loss: 5.036078929901123 | BCE Loss: 1.0034139156341553\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.04558801651001 | KNN Loss: 5.037911891937256 | BCE Loss: 1.0076762437820435\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.089654922485352 | KNN Loss: 5.035268783569336 | BCE Loss: 1.0543861389160156\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.073794364929199 | KNN Loss: 5.031732082366943 | BCE Loss: 1.0420622825622559\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.078786373138428 | KNN Loss: 5.036916732788086 | BCE Loss: 1.0418695211410522\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.032311916351318 | KNN Loss: 5.030468463897705 | BCE Loss: 1.0018434524536133\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.078583240509033 | KNN Loss: 5.038825035095215 | BCE Loss: 1.0397582054138184\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.0603437423706055 | KNN Loss: 5.0294318199157715 | BCE Loss: 1.030911922454834\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.064394950866699 | KNN Loss: 5.040781497955322 | BCE Loss: 1.023613691329956\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.062297821044922 | KNN Loss: 5.048699855804443 | BCE Loss: 1.013597846031189\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.0612874031066895 | KNN Loss: 5.030423641204834 | BCE Loss: 1.030863881111145\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.049781799316406 | KNN Loss: 5.036613464355469 | BCE Loss: 1.0131680965423584\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.06712532043457 | KNN Loss: 5.03861665725708 | BCE Loss: 1.0285084247589111\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.057231903076172 | KNN Loss: 5.030335426330566 | BCE Loss: 1.026896357536316\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.061062812805176 | KNN Loss: 5.027574062347412 | BCE Loss: 1.0334885120391846\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.073975563049316 | KNN Loss: 5.031640529632568 | BCE Loss: 1.0423352718353271\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.072441577911377 | KNN Loss: 5.027275085449219 | BCE Loss: 1.0451664924621582\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.072376251220703 | KNN Loss: 5.034048557281494 | BCE Loss: 1.038327932357788\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.080349445343018 | KNN Loss: 5.049114227294922 | BCE Loss: 1.0312352180480957\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.065930366516113 | KNN Loss: 5.04275369644165 | BCE Loss: 1.0231764316558838\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.071014881134033 | KNN Loss: 5.035250186920166 | BCE Loss: 1.0357645750045776\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.0612311363220215 | KNN Loss: 5.031055927276611 | BCE Loss: 1.0301752090454102\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.055529594421387 | KNN Loss: 5.027390956878662 | BCE Loss: 1.0281383991241455\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.077845573425293 | KNN Loss: 5.042776107788086 | BCE Loss: 1.035069465637207\n",
      "Epoch   274: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.049324035644531 | KNN Loss: 5.030478000640869 | BCE Loss: 1.018845796585083\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.045225620269775 | KNN Loss: 5.034722328186035 | BCE Loss: 1.0105032920837402\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.044379234313965 | KNN Loss: 5.032352447509766 | BCE Loss: 1.0120265483856201\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.074306488037109 | KNN Loss: 5.04043436050415 | BCE Loss: 1.0338718891143799\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.064398765563965 | KNN Loss: 5.026523590087891 | BCE Loss: 1.0378754138946533\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.065822601318359 | KNN Loss: 5.034119129180908 | BCE Loss: 1.031703233718872\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.0500898361206055 | KNN Loss: 5.030136585235596 | BCE Loss: 1.0199533700942993\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.082996368408203 | KNN Loss: 5.036404132843018 | BCE Loss: 1.046592116355896\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.096711158752441 | KNN Loss: 5.048245429992676 | BCE Loss: 1.0484654903411865\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.1045355796813965 | KNN Loss: 5.053554534912109 | BCE Loss: 1.0509811639785767\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.081807613372803 | KNN Loss: 5.043957710266113 | BCE Loss: 1.0378499031066895\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.077278137207031 | KNN Loss: 5.047535419464111 | BCE Loss: 1.02974271774292\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.058018684387207 | KNN Loss: 5.040652275085449 | BCE Loss: 1.017366647720337\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.062921524047852 | KNN Loss: 5.033517360687256 | BCE Loss: 1.0294044017791748\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.078836441040039 | KNN Loss: 5.035600662231445 | BCE Loss: 1.0432357788085938\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.0790934562683105 | KNN Loss: 5.0421271324157715 | BCE Loss: 1.036966323852539\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.062117099761963 | KNN Loss: 5.026393890380859 | BCE Loss: 1.0357232093811035\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.0587029457092285 | KNN Loss: 5.038665771484375 | BCE Loss: 1.0200371742248535\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.088157653808594 | KNN Loss: 5.04080057144165 | BCE Loss: 1.0473570823669434\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.087337493896484 | KNN Loss: 5.03766393661499 | BCE Loss: 1.0496737957000732\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.101759433746338 | KNN Loss: 5.053082466125488 | BCE Loss: 1.0486770868301392\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.062327861785889 | KNN Loss: 5.039396286010742 | BCE Loss: 1.0229315757751465\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.061573505401611 | KNN Loss: 5.037104606628418 | BCE Loss: 1.024469017982483\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.071603775024414 | KNN Loss: 5.027649879455566 | BCE Loss: 1.0439541339874268\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.073398113250732 | KNN Loss: 5.040297508239746 | BCE Loss: 1.0331006050109863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.0572052001953125 | KNN Loss: 5.023245334625244 | BCE Loss: 1.0339596271514893\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.079711437225342 | KNN Loss: 5.042956829071045 | BCE Loss: 1.0367544889450073\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.069212913513184 | KNN Loss: 5.040118217468262 | BCE Loss: 1.029094934463501\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.061029434204102 | KNN Loss: 5.0335307121276855 | BCE Loss: 1.0274986028671265\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.098328113555908 | KNN Loss: 5.034042835235596 | BCE Loss: 1.0642852783203125\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.089414119720459 | KNN Loss: 5.03481388092041 | BCE Loss: 1.0546001195907593\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.0807952880859375 | KNN Loss: 5.032888412475586 | BCE Loss: 1.0479066371917725\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.0657148361206055 | KNN Loss: 5.03255033493042 | BCE Loss: 1.0331645011901855\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.1168317794799805 | KNN Loss: 5.044537544250488 | BCE Loss: 1.072293996810913\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.058550834655762 | KNN Loss: 5.032464981079102 | BCE Loss: 1.0260860919952393\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.082639217376709 | KNN Loss: 5.035521984100342 | BCE Loss: 1.0471172332763672\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.052614212036133 | KNN Loss: 5.029638290405273 | BCE Loss: 1.0229756832122803\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.094679832458496 | KNN Loss: 5.031376361846924 | BCE Loss: 1.0633032321929932\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.062928199768066 | KNN Loss: 5.037374496459961 | BCE Loss: 1.0255537033081055\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.052394866943359 | KNN Loss: 5.040961265563965 | BCE Loss: 1.0114333629608154\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.078154563903809 | KNN Loss: 5.040128707885742 | BCE Loss: 1.0380256175994873\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.084089279174805 | KNN Loss: 5.030933380126953 | BCE Loss: 1.0531556606292725\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.058648586273193 | KNN Loss: 5.031558990478516 | BCE Loss: 1.0270897150039673\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.076364994049072 | KNN Loss: 5.057206630706787 | BCE Loss: 1.0191583633422852\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.124145984649658 | KNN Loss: 5.0480146408081055 | BCE Loss: 1.0761314630508423\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.071225166320801 | KNN Loss: 5.024433612823486 | BCE Loss: 1.0467917919158936\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.04862642288208 | KNN Loss: 5.02975606918335 | BCE Loss: 1.018870234489441\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.0498247146606445 | KNN Loss: 5.034576416015625 | BCE Loss: 1.0152485370635986\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.036972522735596 | KNN Loss: 5.0308146476745605 | BCE Loss: 1.0061579942703247\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.06171989440918 | KNN Loss: 5.031126499176025 | BCE Loss: 1.0305931568145752\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.062722206115723 | KNN Loss: 5.038935661315918 | BCE Loss: 1.0237863063812256\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.072355270385742 | KNN Loss: 5.046518325805664 | BCE Loss: 1.0258371829986572\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.0747151374816895 | KNN Loss: 5.03099250793457 | BCE Loss: 1.0437225103378296\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.074840545654297 | KNN Loss: 5.036673545837402 | BCE Loss: 1.038167119026184\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.049349784851074 | KNN Loss: 5.0362749099731445 | BCE Loss: 1.0130751132965088\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.053774833679199 | KNN Loss: 5.033458709716797 | BCE Loss: 1.0203158855438232\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.116999626159668 | KNN Loss: 5.043976306915283 | BCE Loss: 1.0730230808258057\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.072052955627441 | KNN Loss: 5.02668571472168 | BCE Loss: 1.0453673601150513\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.078902721405029 | KNN Loss: 5.031980991363525 | BCE Loss: 1.0469216108322144\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.053323745727539 | KNN Loss: 5.028055191040039 | BCE Loss: 1.0252685546875\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.070364952087402 | KNN Loss: 5.033511161804199 | BCE Loss: 1.0368537902832031\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.061843395233154 | KNN Loss: 5.028777599334717 | BCE Loss: 1.0330657958984375\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.080577850341797 | KNN Loss: 5.029543876647949 | BCE Loss: 1.0510337352752686\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.085028648376465 | KNN Loss: 5.035503387451172 | BCE Loss: 1.0495251417160034\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.092037200927734 | KNN Loss: 5.041650772094727 | BCE Loss: 1.0503863096237183\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.0590949058532715 | KNN Loss: 5.028217792510986 | BCE Loss: 1.0308771133422852\n",
      "Epoch   285: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.111274719238281 | KNN Loss: 5.042797088623047 | BCE Loss: 1.0684776306152344\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.083040237426758 | KNN Loss: 5.029387950897217 | BCE Loss: 1.0536524057388306\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.062649726867676 | KNN Loss: 5.036610126495361 | BCE Loss: 1.026039481163025\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.06072998046875 | KNN Loss: 5.027566432952881 | BCE Loss: 1.0331637859344482\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.091672897338867 | KNN Loss: 5.03570032119751 | BCE Loss: 1.0559725761413574\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.081907272338867 | KNN Loss: 5.026650428771973 | BCE Loss: 1.055256962776184\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.062776565551758 | KNN Loss: 5.029036521911621 | BCE Loss: 1.0337401628494263\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.058213710784912 | KNN Loss: 5.03407096862793 | BCE Loss: 1.0241426229476929\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.066801071166992 | KNN Loss: 5.040314674377441 | BCE Loss: 1.0264863967895508\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.062714576721191 | KNN Loss: 5.037240982055664 | BCE Loss: 1.0254738330841064\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.077232360839844 | KNN Loss: 5.039180278778076 | BCE Loss: 1.0380520820617676\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.11068058013916 | KNN Loss: 5.041895866394043 | BCE Loss: 1.0687847137451172\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.093865871429443 | KNN Loss: 5.035852432250977 | BCE Loss: 1.0580134391784668\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.059009552001953 | KNN Loss: 5.037380218505859 | BCE Loss: 1.0216293334960938\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.099925994873047 | KNN Loss: 5.039593696594238 | BCE Loss: 1.0603324174880981\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.064245223999023 | KNN Loss: 5.035059452056885 | BCE Loss: 1.0291860103607178\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.051453590393066 | KNN Loss: 5.028942584991455 | BCE Loss: 1.0225110054016113\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.087728977203369 | KNN Loss: 5.04985237121582 | BCE Loss: 1.0378767251968384\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.094045639038086 | KNN Loss: 5.035068035125732 | BCE Loss: 1.0589773654937744\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.0931196212768555 | KNN Loss: 5.0311198234558105 | BCE Loss: 1.061999797821045\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.0756940841674805 | KNN Loss: 5.046716213226318 | BCE Loss: 1.0289781093597412\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.068553447723389 | KNN Loss: 5.033858299255371 | BCE Loss: 1.0346951484680176\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.074782848358154 | KNN Loss: 5.044685363769531 | BCE Loss: 1.0300976037979126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.0515594482421875 | KNN Loss: 5.032769680023193 | BCE Loss: 1.0187900066375732\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.083308219909668 | KNN Loss: 5.037139415740967 | BCE Loss: 1.0461688041687012\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.092341423034668 | KNN Loss: 5.0307297706604 | BCE Loss: 1.0616118907928467\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.058474540710449 | KNN Loss: 5.038815498352051 | BCE Loss: 1.0196588039398193\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.053526401519775 | KNN Loss: 5.034684658050537 | BCE Loss: 1.0188418626785278\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.061361312866211 | KNN Loss: 5.044064998626709 | BCE Loss: 1.017296552658081\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.065059185028076 | KNN Loss: 5.024938583374023 | BCE Loss: 1.0401207208633423\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.1092424392700195 | KNN Loss: 5.036752700805664 | BCE Loss: 1.072489857673645\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.098995685577393 | KNN Loss: 5.0426764488220215 | BCE Loss: 1.056319236755371\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.0835113525390625 | KNN Loss: 5.050604343414307 | BCE Loss: 1.0329070091247559\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.071530342102051 | KNN Loss: 5.040646076202393 | BCE Loss: 1.0308842658996582\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.0815324783325195 | KNN Loss: 5.040885925292969 | BCE Loss: 1.0406463146209717\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.076651573181152 | KNN Loss: 5.033440113067627 | BCE Loss: 1.0432114601135254\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.062979698181152 | KNN Loss: 5.029120445251465 | BCE Loss: 1.0338590145111084\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.099877834320068 | KNN Loss: 5.039229869842529 | BCE Loss: 1.060647964477539\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.077192306518555 | KNN Loss: 5.029390811920166 | BCE Loss: 1.0478014945983887\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.083487510681152 | KNN Loss: 5.03171968460083 | BCE Loss: 1.0517675876617432\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.0424957275390625 | KNN Loss: 5.030555248260498 | BCE Loss: 1.0119402408599854\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.068855285644531 | KNN Loss: 5.033599376678467 | BCE Loss: 1.0352561473846436\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.084230422973633 | KNN Loss: 5.030173301696777 | BCE Loss: 1.054057002067566\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.0895843505859375 | KNN Loss: 5.0472846031188965 | BCE Loss: 1.042299509048462\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.078811168670654 | KNN Loss: 5.038787841796875 | BCE Loss: 1.0400234460830688\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.074313640594482 | KNN Loss: 5.0300798416137695 | BCE Loss: 1.0442339181900024\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.084455490112305 | KNN Loss: 5.032509803771973 | BCE Loss: 1.051945686340332\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.076630592346191 | KNN Loss: 5.027362823486328 | BCE Loss: 1.0492676496505737\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.0464277267456055 | KNN Loss: 5.036379337310791 | BCE Loss: 1.0100486278533936\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.079700469970703 | KNN Loss: 5.029958248138428 | BCE Loss: 1.0497419834136963\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.080686569213867 | KNN Loss: 5.02982234954834 | BCE Loss: 1.0508641004562378\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.052372932434082 | KNN Loss: 5.032259941101074 | BCE Loss: 1.020113229751587\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.098894119262695 | KNN Loss: 5.033228874206543 | BCE Loss: 1.0656651258468628\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.10372257232666 | KNN Loss: 5.068205833435059 | BCE Loss: 1.0355167388916016\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.068502426147461 | KNN Loss: 5.0332207679748535 | BCE Loss: 1.0352814197540283\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.066269397735596 | KNN Loss: 5.029027462005615 | BCE Loss: 1.0372419357299805\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.056308269500732 | KNN Loss: 5.029088020324707 | BCE Loss: 1.027220368385315\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.081865310668945 | KNN Loss: 5.043978691101074 | BCE Loss: 1.0378868579864502\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.034491539001465 | KNN Loss: 5.026442050933838 | BCE Loss: 1.008049726486206\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.070764541625977 | KNN Loss: 5.0375142097473145 | BCE Loss: 1.0332505702972412\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.055212020874023 | KNN Loss: 5.031397819519043 | BCE Loss: 1.023814082145691\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.08140230178833 | KNN Loss: 5.0316596031188965 | BCE Loss: 1.049742579460144\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.063244819641113 | KNN Loss: 5.028778553009033 | BCE Loss: 1.03446626663208\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.073497772216797 | KNN Loss: 5.0401225090026855 | BCE Loss: 1.0333755016326904\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.057112693786621 | KNN Loss: 5.0368428230285645 | BCE Loss: 1.0202698707580566\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.0833539962768555 | KNN Loss: 5.0587053298950195 | BCE Loss: 1.0246485471725464\n",
      "Epoch   296: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.0935258865356445 | KNN Loss: 5.03458833694458 | BCE Loss: 1.0589375495910645\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.071198463439941 | KNN Loss: 5.041242599487305 | BCE Loss: 1.0299561023712158\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.055816173553467 | KNN Loss: 5.032312870025635 | BCE Loss: 1.023503303527832\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.043795585632324 | KNN Loss: 5.027822971343994 | BCE Loss: 1.0159728527069092\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.059418678283691 | KNN Loss: 5.040310859680176 | BCE Loss: 1.0191078186035156\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.095695972442627 | KNN Loss: 5.0384202003479 | BCE Loss: 1.057275652885437\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.073146820068359 | KNN Loss: 5.0299601554870605 | BCE Loss: 1.043186902999878\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.065952777862549 | KNN Loss: 5.051986217498779 | BCE Loss: 1.01396644115448\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.0724616050720215 | KNN Loss: 5.0385422706604 | BCE Loss: 1.0339192152023315\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.072686195373535 | KNN Loss: 5.034478664398193 | BCE Loss: 1.038207769393921\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.066520690917969 | KNN Loss: 5.028541088104248 | BCE Loss: 1.0379798412322998\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.069170951843262 | KNN Loss: 5.034732818603516 | BCE Loss: 1.0344383716583252\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.076303482055664 | KNN Loss: 5.048184394836426 | BCE Loss: 1.0281188488006592\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.034708023071289 | KNN Loss: 5.032268047332764 | BCE Loss: 1.0024398565292358\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.087531566619873 | KNN Loss: 5.045112609863281 | BCE Loss: 1.0424190759658813\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.059291839599609 | KNN Loss: 5.031116485595703 | BCE Loss: 1.0281754732131958\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.0856122970581055 | KNN Loss: 5.031185626983643 | BCE Loss: 1.054426670074463\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.067323684692383 | KNN Loss: 5.0298895835876465 | BCE Loss: 1.0374338626861572\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.084902286529541 | KNN Loss: 5.030989170074463 | BCE Loss: 1.0539131164550781\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.0779619216918945 | KNN Loss: 5.029325485229492 | BCE Loss: 1.0486366748809814\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.061844825744629 | KNN Loss: 5.025367736816406 | BCE Loss: 1.0364770889282227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.07545280456543 | KNN Loss: 5.040984153747559 | BCE Loss: 1.0344688892364502\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.058280944824219 | KNN Loss: 5.037469387054443 | BCE Loss: 1.0208115577697754\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.069382190704346 | KNN Loss: 5.045237064361572 | BCE Loss: 1.024145245552063\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.079411029815674 | KNN Loss: 5.037721157073975 | BCE Loss: 1.0416897535324097\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.0621209144592285 | KNN Loss: 5.037947654724121 | BCE Loss: 1.0241732597351074\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.0689239501953125 | KNN Loss: 5.033682823181152 | BCE Loss: 1.035240888595581\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.074100017547607 | KNN Loss: 5.029005527496338 | BCE Loss: 1.045094609260559\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.099788665771484 | KNN Loss: 5.033951282501221 | BCE Loss: 1.0658376216888428\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.069263458251953 | KNN Loss: 5.026185989379883 | BCE Loss: 1.0430774688720703\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.091479301452637 | KNN Loss: 5.048987865447998 | BCE Loss: 1.0424916744232178\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.057101249694824 | KNN Loss: 5.028011798858643 | BCE Loss: 1.0290892124176025\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.034670829772949 | KNN Loss: 5.02827787399292 | BCE Loss: 1.0063931941986084\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.106700897216797 | KNN Loss: 5.044798851013184 | BCE Loss: 1.0619022846221924\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.081245422363281 | KNN Loss: 5.047702789306641 | BCE Loss: 1.0335427522659302\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.0847883224487305 | KNN Loss: 5.033736705780029 | BCE Loss: 1.0510516166687012\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.092864513397217 | KNN Loss: 5.0374579429626465 | BCE Loss: 1.0554064512252808\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.103324890136719 | KNN Loss: 5.051903247833252 | BCE Loss: 1.0514217615127563\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.07334041595459 | KNN Loss: 5.028298377990723 | BCE Loss: 1.0450422763824463\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.086040496826172 | KNN Loss: 5.0378217697143555 | BCE Loss: 1.0482189655303955\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.070298194885254 | KNN Loss: 5.030239105224609 | BCE Loss: 1.040058970451355\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.037501335144043 | KNN Loss: 5.038234233856201 | BCE Loss: 0.9992669820785522\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.0908403396606445 | KNN Loss: 5.051325798034668 | BCE Loss: 1.0395147800445557\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.049274444580078 | KNN Loss: 5.03201961517334 | BCE Loss: 1.0172545909881592\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.049859046936035 | KNN Loss: 5.025798320770264 | BCE Loss: 1.024060606956482\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.117977142333984 | KNN Loss: 5.0340986251831055 | BCE Loss: 1.083878517150879\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.047430038452148 | KNN Loss: 5.027985095977783 | BCE Loss: 1.0194451808929443\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.0857648849487305 | KNN Loss: 5.040919303894043 | BCE Loss: 1.0448453426361084\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.096348762512207 | KNN Loss: 5.048575401306152 | BCE Loss: 1.0477735996246338\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.050498962402344 | KNN Loss: 5.0343146324157715 | BCE Loss: 1.0161843299865723\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.083242416381836 | KNN Loss: 5.032812118530273 | BCE Loss: 1.0504302978515625\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.0789103507995605 | KNN Loss: 5.046961307525635 | BCE Loss: 1.0319490432739258\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.074100971221924 | KNN Loss: 5.02767276763916 | BCE Loss: 1.0464280843734741\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.097316741943359 | KNN Loss: 5.0504937171936035 | BCE Loss: 1.0468230247497559\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.074000835418701 | KNN Loss: 5.0433573722839355 | BCE Loss: 1.030643343925476\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.054181098937988 | KNN Loss: 5.0306806564331055 | BCE Loss: 1.0235002040863037\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.085207939147949 | KNN Loss: 5.027590274810791 | BCE Loss: 1.0576179027557373\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.0997443199157715 | KNN Loss: 5.045560836791992 | BCE Loss: 1.0541836023330688\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.065310478210449 | KNN Loss: 5.039902210235596 | BCE Loss: 1.0254080295562744\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.099464416503906 | KNN Loss: 5.026437759399414 | BCE Loss: 1.0730268955230713\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.067284107208252 | KNN Loss: 5.030153751373291 | BCE Loss: 1.037130355834961\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.126481056213379 | KNN Loss: 5.040065765380859 | BCE Loss: 1.0864155292510986\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.06926155090332 | KNN Loss: 5.053152561187744 | BCE Loss: 1.0161088705062866\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.039060592651367 | KNN Loss: 5.0290913581848145 | BCE Loss: 1.0099694728851318\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.060485363006592 | KNN Loss: 5.045831680297852 | BCE Loss: 1.0146538019180298\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.072028160095215 | KNN Loss: 5.036698818206787 | BCE Loss: 1.0353295803070068\n",
      "Epoch   307: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.073037147521973 | KNN Loss: 5.042987823486328 | BCE Loss: 1.0300495624542236\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.090085983276367 | KNN Loss: 5.051699161529541 | BCE Loss: 1.038386583328247\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.103672504425049 | KNN Loss: 5.042873382568359 | BCE Loss: 1.0607990026474\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.068681240081787 | KNN Loss: 5.03627347946167 | BCE Loss: 1.0324076414108276\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.078954696655273 | KNN Loss: 5.042261123657227 | BCE Loss: 1.0366935729980469\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.106790542602539 | KNN Loss: 5.042902946472168 | BCE Loss: 1.0638878345489502\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.113391876220703 | KNN Loss: 5.043637752532959 | BCE Loss: 1.069753885269165\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.069446086883545 | KNN Loss: 5.036557197570801 | BCE Loss: 1.0328888893127441\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.0451765060424805 | KNN Loss: 5.026646614074707 | BCE Loss: 1.0185298919677734\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.069478988647461 | KNN Loss: 5.037326335906982 | BCE Loss: 1.0321524143218994\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.079905033111572 | KNN Loss: 5.037793159484863 | BCE Loss: 1.042111873626709\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.074769973754883 | KNN Loss: 5.033883094787598 | BCE Loss: 1.0408867597579956\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.075037002563477 | KNN Loss: 5.0438642501831055 | BCE Loss: 1.0311728715896606\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.074358940124512 | KNN Loss: 5.033106327056885 | BCE Loss: 1.041252851486206\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.066825866699219 | KNN Loss: 5.040863037109375 | BCE Loss: 1.0259625911712646\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.0787248611450195 | KNN Loss: 5.038234710693359 | BCE Loss: 1.040489912033081\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.079365253448486 | KNN Loss: 5.029195308685303 | BCE Loss: 1.050169825553894\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.0445451736450195 | KNN Loss: 5.032070159912109 | BCE Loss: 1.0124752521514893\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.063840866088867 | KNN Loss: 5.031253814697266 | BCE Loss: 1.0325872898101807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.081766128540039 | KNN Loss: 5.027576446533203 | BCE Loss: 1.054189682006836\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.089794158935547 | KNN Loss: 5.039479732513428 | BCE Loss: 1.0503146648406982\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.1127166748046875 | KNN Loss: 5.041300296783447 | BCE Loss: 1.0714163780212402\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.088671684265137 | KNN Loss: 5.05385160446167 | BCE Loss: 1.0348198413848877\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.057802200317383 | KNN Loss: 5.023895740509033 | BCE Loss: 1.0339064598083496\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.06911039352417 | KNN Loss: 5.025988578796387 | BCE Loss: 1.0431219339370728\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.065236568450928 | KNN Loss: 5.025812149047852 | BCE Loss: 1.0394243001937866\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.076449394226074 | KNN Loss: 5.031102180480957 | BCE Loss: 1.0453474521636963\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.059798240661621 | KNN Loss: 5.0427680015563965 | BCE Loss: 1.0170300006866455\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.107501983642578 | KNN Loss: 5.046403408050537 | BCE Loss: 1.061098575592041\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.070525169372559 | KNN Loss: 5.031228065490723 | BCE Loss: 1.039297342300415\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.0657877922058105 | KNN Loss: 5.040937423706055 | BCE Loss: 1.0248504877090454\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.085153579711914 | KNN Loss: 5.034598350524902 | BCE Loss: 1.0505549907684326\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.077012062072754 | KNN Loss: 5.033892631530762 | BCE Loss: 1.0431195497512817\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.040434837341309 | KNN Loss: 5.0265936851501465 | BCE Loss: 1.013840913772583\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.074230194091797 | KNN Loss: 5.035580158233643 | BCE Loss: 1.0386497974395752\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.120787620544434 | KNN Loss: 5.044483661651611 | BCE Loss: 1.0763038396835327\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.054562568664551 | KNN Loss: 5.0268330574035645 | BCE Loss: 1.0277296304702759\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.047046661376953 | KNN Loss: 5.028573989868164 | BCE Loss: 1.018472671508789\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.08537483215332 | KNN Loss: 5.026370048522949 | BCE Loss: 1.059004545211792\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.06539249420166 | KNN Loss: 5.026848793029785 | BCE Loss: 1.038543939590454\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.07451868057251 | KNN Loss: 5.039679050445557 | BCE Loss: 1.0348396301269531\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.083201885223389 | KNN Loss: 5.029983043670654 | BCE Loss: 1.0532188415527344\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.072198390960693 | KNN Loss: 5.026762962341309 | BCE Loss: 1.0454354286193848\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.087708473205566 | KNN Loss: 5.041839599609375 | BCE Loss: 1.0458688735961914\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.055255889892578 | KNN Loss: 5.0318121910095215 | BCE Loss: 1.0234438180923462\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.070134162902832 | KNN Loss: 5.030130863189697 | BCE Loss: 1.0400030612945557\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.084461212158203 | KNN Loss: 5.031649112701416 | BCE Loss: 1.052812099456787\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.075037956237793 | KNN Loss: 5.02947998046875 | BCE Loss: 1.045558214187622\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.050116539001465 | KNN Loss: 5.043394088745117 | BCE Loss: 1.0067222118377686\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.086292266845703 | KNN Loss: 5.040668487548828 | BCE Loss: 1.045623540878296\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.051619529724121 | KNN Loss: 5.037018299102783 | BCE Loss: 1.0146009922027588\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.06937313079834 | KNN Loss: 5.030274868011475 | BCE Loss: 1.0390981435775757\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.04837703704834 | KNN Loss: 5.029295921325684 | BCE Loss: 1.0190812349319458\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.032127380371094 | KNN Loss: 5.028349876403809 | BCE Loss: 1.0037776231765747\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.074095726013184 | KNN Loss: 5.035332679748535 | BCE Loss: 1.038763165473938\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.05692195892334 | KNN Loss: 5.032891750335693 | BCE Loss: 1.024030089378357\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.052023887634277 | KNN Loss: 5.025008201599121 | BCE Loss: 1.0270156860351562\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.083898067474365 | KNN Loss: 5.040287971496582 | BCE Loss: 1.0436100959777832\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.092905521392822 | KNN Loss: 5.031949996948242 | BCE Loss: 1.0609556436538696\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.09272575378418 | KNN Loss: 5.055274963378906 | BCE Loss: 1.0374507904052734\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.054401397705078 | KNN Loss: 5.026834011077881 | BCE Loss: 1.0275676250457764\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.097458839416504 | KNN Loss: 5.035762310028076 | BCE Loss: 1.0616964101791382\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.079232215881348 | KNN Loss: 5.031408309936523 | BCE Loss: 1.0478239059448242\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.0907416343688965 | KNN Loss: 5.038973331451416 | BCE Loss: 1.0517683029174805\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.051435470581055 | KNN Loss: 5.037840843200684 | BCE Loss: 1.0135945081710815\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.077905178070068 | KNN Loss: 5.030007362365723 | BCE Loss: 1.0478978157043457\n",
      "Epoch   318: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.061986923217773 | KNN Loss: 5.035688400268555 | BCE Loss: 1.0262982845306396\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.062410354614258 | KNN Loss: 5.0300188064575195 | BCE Loss: 1.0323915481567383\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.056717395782471 | KNN Loss: 5.025365352630615 | BCE Loss: 1.031352162361145\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.046351909637451 | KNN Loss: 5.0325398445129395 | BCE Loss: 1.0138119459152222\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.090848922729492 | KNN Loss: 5.03802490234375 | BCE Loss: 1.052823781967163\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.063355445861816 | KNN Loss: 5.031694412231445 | BCE Loss: 1.031661033630371\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.057791233062744 | KNN Loss: 5.044314384460449 | BCE Loss: 1.0134767293930054\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.057966709136963 | KNN Loss: 5.0277099609375 | BCE Loss: 1.030256748199463\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.085025787353516 | KNN Loss: 5.036980628967285 | BCE Loss: 1.0480453968048096\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.082013130187988 | KNN Loss: 5.043402194976807 | BCE Loss: 1.0386111736297607\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.074527740478516 | KNN Loss: 5.0440473556518555 | BCE Loss: 1.030480146408081\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.0448150634765625 | KNN Loss: 5.038457870483398 | BCE Loss: 1.0063574314117432\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.070399761199951 | KNN Loss: 5.034661769866943 | BCE Loss: 1.0357379913330078\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.095595359802246 | KNN Loss: 5.03980016708374 | BCE Loss: 1.0557950735092163\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.029456615447998 | KNN Loss: 5.0353593826293945 | BCE Loss: 0.9940970540046692\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.076070785522461 | KNN Loss: 5.02914571762085 | BCE Loss: 1.0469249486923218\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.045127868652344 | KNN Loss: 5.026229381561279 | BCE Loss: 1.0188984870910645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.07716178894043 | KNN Loss: 5.039038181304932 | BCE Loss: 1.0381238460540771\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.0949788093566895 | KNN Loss: 5.047385215759277 | BCE Loss: 1.047593593597412\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.08770227432251 | KNN Loss: 5.035257816314697 | BCE Loss: 1.0524444580078125\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.0838823318481445 | KNN Loss: 5.033627033233643 | BCE Loss: 1.050255537033081\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.031824111938477 | KNN Loss: 5.031899929046631 | BCE Loss: 0.99992436170578\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.099966049194336 | KNN Loss: 5.032649993896484 | BCE Loss: 1.0673160552978516\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.053305149078369 | KNN Loss: 5.02946138381958 | BCE Loss: 1.0238438844680786\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.07880973815918 | KNN Loss: 5.035634994506836 | BCE Loss: 1.0431749820709229\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.079893112182617 | KNN Loss: 5.030874252319336 | BCE Loss: 1.0490188598632812\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.064126014709473 | KNN Loss: 5.028907299041748 | BCE Loss: 1.0352187156677246\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.082890510559082 | KNN Loss: 5.031778812408447 | BCE Loss: 1.0511116981506348\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.108865261077881 | KNN Loss: 5.031224727630615 | BCE Loss: 1.0776405334472656\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.069226264953613 | KNN Loss: 5.032447814941406 | BCE Loss: 1.0367783308029175\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.070479393005371 | KNN Loss: 5.039827823638916 | BCE Loss: 1.030651330947876\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.044890403747559 | KNN Loss: 5.026695251464844 | BCE Loss: 1.0181949138641357\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.092286109924316 | KNN Loss: 5.033890724182129 | BCE Loss: 1.0583953857421875\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.0670976638793945 | KNN Loss: 5.033445835113525 | BCE Loss: 1.0336518287658691\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.07401704788208 | KNN Loss: 5.03184700012207 | BCE Loss: 1.0421701669692993\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.057515621185303 | KNN Loss: 5.025596618652344 | BCE Loss: 1.0319188833236694\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.07741641998291 | KNN Loss: 5.042391777038574 | BCE Loss: 1.035024642944336\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.083984375 | KNN Loss: 5.030342102050781 | BCE Loss: 1.0536423921585083\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.068315505981445 | KNN Loss: 5.035252571105957 | BCE Loss: 1.0330629348754883\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.048732757568359 | KNN Loss: 5.028565406799316 | BCE Loss: 1.0201674699783325\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.068820953369141 | KNN Loss: 5.029928207397461 | BCE Loss: 1.0388925075531006\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.074765682220459 | KNN Loss: 5.037816524505615 | BCE Loss: 1.0369490385055542\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.050401210784912 | KNN Loss: 5.0320515632629395 | BCE Loss: 1.0183496475219727\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.075990200042725 | KNN Loss: 5.031924247741699 | BCE Loss: 1.0440658330917358\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.0837860107421875 | KNN Loss: 5.041858196258545 | BCE Loss: 1.0419278144836426\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.068539619445801 | KNN Loss: 5.0317511558532715 | BCE Loss: 1.0367887020111084\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.072427749633789 | KNN Loss: 5.026166915893555 | BCE Loss: 1.0462605953216553\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.083785057067871 | KNN Loss: 5.0335001945495605 | BCE Loss: 1.0502848625183105\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.061617374420166 | KNN Loss: 5.053033828735352 | BCE Loss: 1.0085835456848145\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.0553741455078125 | KNN Loss: 5.034972667694092 | BCE Loss: 1.0204012393951416\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.085787773132324 | KNN Loss: 5.030501842498779 | BCE Loss: 1.055286169052124\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.0782599449157715 | KNN Loss: 5.0322675704956055 | BCE Loss: 1.045992374420166\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.1373491287231445 | KNN Loss: 5.064759731292725 | BCE Loss: 1.072589635848999\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.079941749572754 | KNN Loss: 5.039147853851318 | BCE Loss: 1.0407938957214355\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.119448661804199 | KNN Loss: 5.039091110229492 | BCE Loss: 1.0803574323654175\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.047125816345215 | KNN Loss: 5.02871036529541 | BCE Loss: 1.0184155702590942\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.111389636993408 | KNN Loss: 5.0344061851501465 | BCE Loss: 1.0769833326339722\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.064001560211182 | KNN Loss: 5.0428996086120605 | BCE Loss: 1.021101951599121\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.106999397277832 | KNN Loss: 5.039133548736572 | BCE Loss: 1.0678658485412598\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.052351951599121 | KNN Loss: 5.031521797180176 | BCE Loss: 1.0208301544189453\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.064972400665283 | KNN Loss: 5.027785301208496 | BCE Loss: 1.037187099456787\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.099648475646973 | KNN Loss: 5.043484210968018 | BCE Loss: 1.0561645030975342\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.079106330871582 | KNN Loss: 5.039818286895752 | BCE Loss: 1.039287805557251\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.07179069519043 | KNN Loss: 5.034710884094238 | BCE Loss: 1.0370795726776123\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.070676326751709 | KNN Loss: 5.032537460327148 | BCE Loss: 1.0381388664245605\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.078763008117676 | KNN Loss: 5.032253265380859 | BCE Loss: 1.0465099811553955\n",
      "Epoch   329: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.128105640411377 | KNN Loss: 5.046976089477539 | BCE Loss: 1.0811294317245483\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.050060272216797 | KNN Loss: 5.031040191650391 | BCE Loss: 1.0190203189849854\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.092167854309082 | KNN Loss: 5.043821811676025 | BCE Loss: 1.0483462810516357\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.055547714233398 | KNN Loss: 5.029146194458008 | BCE Loss: 1.0264012813568115\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.064170837402344 | KNN Loss: 5.024226188659668 | BCE Loss: 1.0399448871612549\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.0848307609558105 | KNN Loss: 5.0323309898376465 | BCE Loss: 1.0524996519088745\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.073784351348877 | KNN Loss: 5.031935691833496 | BCE Loss: 1.0418486595153809\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.054409027099609 | KNN Loss: 5.026786804199219 | BCE Loss: 1.0276222229003906\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.031769752502441 | KNN Loss: 5.032278537750244 | BCE Loss: 0.9994912147521973\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.058714389801025 | KNN Loss: 5.02798318862915 | BCE Loss: 1.0307313203811646\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.076898574829102 | KNN Loss: 5.03448486328125 | BCE Loss: 1.0424137115478516\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.09699010848999 | KNN Loss: 5.027438163757324 | BCE Loss: 1.069551944732666\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.061246871948242 | KNN Loss: 5.034983158111572 | BCE Loss: 1.026263952255249\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.086181640625 | KNN Loss: 5.028380870819092 | BCE Loss: 1.0578006505966187\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.051208972930908 | KNN Loss: 5.035547256469727 | BCE Loss: 1.015661597251892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.097304344177246 | KNN Loss: 5.057595252990723 | BCE Loss: 1.0397093296051025\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.050758361816406 | KNN Loss: 5.027897834777832 | BCE Loss: 1.0228602886199951\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.097956657409668 | KNN Loss: 5.041385173797607 | BCE Loss: 1.0565712451934814\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.098381042480469 | KNN Loss: 5.044937610626221 | BCE Loss: 1.053443431854248\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.057406425476074 | KNN Loss: 5.037785053253174 | BCE Loss: 1.01962149143219\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.073476314544678 | KNN Loss: 5.027244567871094 | BCE Loss: 1.0462318658828735\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.069616317749023 | KNN Loss: 5.037257671356201 | BCE Loss: 1.0323587656021118\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.0621466636657715 | KNN Loss: 5.027753829956055 | BCE Loss: 1.0343928337097168\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.068112373352051 | KNN Loss: 5.029212951660156 | BCE Loss: 1.0388996601104736\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.087130069732666 | KNN Loss: 5.04514741897583 | BCE Loss: 1.0419825315475464\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.067822456359863 | KNN Loss: 5.038730144500732 | BCE Loss: 1.02909255027771\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.051752090454102 | KNN Loss: 5.026716709136963 | BCE Loss: 1.0250351428985596\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.112715721130371 | KNN Loss: 5.060779094696045 | BCE Loss: 1.051936388015747\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.067723274230957 | KNN Loss: 5.030376434326172 | BCE Loss: 1.0373468399047852\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.086516380310059 | KNN Loss: 5.047354698181152 | BCE Loss: 1.0391616821289062\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.035006523132324 | KNN Loss: 5.040809631347656 | BCE Loss: 0.994196891784668\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.066301345825195 | KNN Loss: 5.025347709655762 | BCE Loss: 1.0409533977508545\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.081997871398926 | KNN Loss: 5.037721633911133 | BCE Loss: 1.044276475906372\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.057745456695557 | KNN Loss: 5.038192272186279 | BCE Loss: 1.0195531845092773\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.099606513977051 | KNN Loss: 5.032334327697754 | BCE Loss: 1.0672721862792969\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.071876525878906 | KNN Loss: 5.030811309814453 | BCE Loss: 1.0410650968551636\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.0566935539245605 | KNN Loss: 5.028529167175293 | BCE Loss: 1.0281643867492676\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.076703071594238 | KNN Loss: 5.034297466278076 | BCE Loss: 1.042405366897583\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.101772308349609 | KNN Loss: 5.035483360290527 | BCE Loss: 1.066288709640503\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.063177108764648 | KNN Loss: 5.029148578643799 | BCE Loss: 1.0340285301208496\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.097131729125977 | KNN Loss: 5.067648410797119 | BCE Loss: 1.0294831991195679\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.066282749176025 | KNN Loss: 5.034393310546875 | BCE Loss: 1.0318894386291504\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.094058513641357 | KNN Loss: 5.061563968658447 | BCE Loss: 1.0324945449829102\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.093235015869141 | KNN Loss: 5.049651145935059 | BCE Loss: 1.043583869934082\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.085589408874512 | KNN Loss: 5.038633346557617 | BCE Loss: 1.0469558238983154\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.122690200805664 | KNN Loss: 5.045721530914307 | BCE Loss: 1.076968789100647\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.079371929168701 | KNN Loss: 5.033254146575928 | BCE Loss: 1.0461177825927734\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.0449323654174805 | KNN Loss: 5.036109447479248 | BCE Loss: 1.0088226795196533\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.055357933044434 | KNN Loss: 5.027810573577881 | BCE Loss: 1.0275473594665527\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.0534162521362305 | KNN Loss: 5.031454086303711 | BCE Loss: 1.021962285041809\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.085061550140381 | KNN Loss: 5.0285725593566895 | BCE Loss: 1.0564889907836914\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.115734100341797 | KNN Loss: 5.045583724975586 | BCE Loss: 1.0701502561569214\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.040701866149902 | KNN Loss: 5.034431457519531 | BCE Loss: 1.0062706470489502\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.065115451812744 | KNN Loss: 5.033228874206543 | BCE Loss: 1.0318864583969116\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.053036689758301 | KNN Loss: 5.027927875518799 | BCE Loss: 1.025108814239502\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.101596832275391 | KNN Loss: 5.05665922164917 | BCE Loss: 1.0449373722076416\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.081418991088867 | KNN Loss: 5.031026363372803 | BCE Loss: 1.0503928661346436\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.042789459228516 | KNN Loss: 5.031161785125732 | BCE Loss: 1.0116275548934937\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.076387405395508 | KNN Loss: 5.033605098724365 | BCE Loss: 1.0427823066711426\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.076599597930908 | KNN Loss: 5.045947074890137 | BCE Loss: 1.0306525230407715\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.068572044372559 | KNN Loss: 5.0292067527771 | BCE Loss: 1.039365291595459\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.071627616882324 | KNN Loss: 5.029918670654297 | BCE Loss: 1.0417087078094482\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.071718692779541 | KNN Loss: 5.038989543914795 | BCE Loss: 1.032729148864746\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.0812482833862305 | KNN Loss: 5.033642292022705 | BCE Loss: 1.0476062297821045\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.106912612915039 | KNN Loss: 5.049017906188965 | BCE Loss: 1.0578947067260742\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.080278396606445 | KNN Loss: 5.046609878540039 | BCE Loss: 1.0336685180664062\n",
      "Epoch   340: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.088804244995117 | KNN Loss: 5.044548034667969 | BCE Loss: 1.0442564487457275\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.059657096862793 | KNN Loss: 5.0313401222229 | BCE Loss: 1.028316855430603\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.080386161804199 | KNN Loss: 5.031642913818359 | BCE Loss: 1.0487432479858398\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.052570343017578 | KNN Loss: 5.03230619430542 | BCE Loss: 1.0202642679214478\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.0767130851745605 | KNN Loss: 5.044731140136719 | BCE Loss: 1.0319819450378418\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.046106338500977 | KNN Loss: 5.03239107131958 | BCE Loss: 1.0137152671813965\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.040383338928223 | KNN Loss: 5.028473377227783 | BCE Loss: 1.0119099617004395\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.046674728393555 | KNN Loss: 5.027863025665283 | BCE Loss: 1.0188117027282715\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.122452259063721 | KNN Loss: 5.032025337219238 | BCE Loss: 1.090427041053772\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.098862648010254 | KNN Loss: 5.042163848876953 | BCE Loss: 1.0566985607147217\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.101808547973633 | KNN Loss: 5.041248798370361 | BCE Loss: 1.0605599880218506\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.038304805755615 | KNN Loss: 5.032090663909912 | BCE Loss: 1.0062142610549927\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.06034517288208 | KNN Loss: 5.030508041381836 | BCE Loss: 1.0298371315002441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.0912041664123535 | KNN Loss: 5.035335540771484 | BCE Loss: 1.0558687448501587\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.085033416748047 | KNN Loss: 5.047191619873047 | BCE Loss: 1.037842035293579\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.060906410217285 | KNN Loss: 5.029366970062256 | BCE Loss: 1.0315396785736084\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.061972618103027 | KNN Loss: 5.030698299407959 | BCE Loss: 1.031274437904358\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.038148403167725 | KNN Loss: 5.029440879821777 | BCE Loss: 1.0087075233459473\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.070773124694824 | KNN Loss: 5.030824661254883 | BCE Loss: 1.0399484634399414\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.042928695678711 | KNN Loss: 5.034734725952148 | BCE Loss: 1.0081937313079834\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.05419921875 | KNN Loss: 5.033200740814209 | BCE Loss: 1.0209985971450806\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.059534072875977 | KNN Loss: 5.0246663093566895 | BCE Loss: 1.0348676443099976\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.050401210784912 | KNN Loss: 5.032966613769531 | BCE Loss: 1.0174345970153809\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.061915397644043 | KNN Loss: 5.032439708709717 | BCE Loss: 1.0294759273529053\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.066068649291992 | KNN Loss: 5.043618202209473 | BCE Loss: 1.022450566291809\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.069342136383057 | KNN Loss: 5.04107141494751 | BCE Loss: 1.0282707214355469\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.061373710632324 | KNN Loss: 5.039775371551514 | BCE Loss: 1.0215983390808105\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.06717586517334 | KNN Loss: 5.030941963195801 | BCE Loss: 1.03623366355896\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.082378387451172 | KNN Loss: 5.049773693084717 | BCE Loss: 1.032604694366455\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.061761856079102 | KNN Loss: 5.031663417816162 | BCE Loss: 1.0300986766815186\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.038643836975098 | KNN Loss: 5.031254291534424 | BCE Loss: 1.0073895454406738\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.0467939376831055 | KNN Loss: 5.030564785003662 | BCE Loss: 1.0162291526794434\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.078320026397705 | KNN Loss: 5.04168176651001 | BCE Loss: 1.0366382598876953\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.067235946655273 | KNN Loss: 5.030701160430908 | BCE Loss: 1.0365346670150757\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.080447196960449 | KNN Loss: 5.034598350524902 | BCE Loss: 1.0458486080169678\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.081723213195801 | KNN Loss: 5.041287422180176 | BCE Loss: 1.040435791015625\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.053049087524414 | KNN Loss: 5.041936874389648 | BCE Loss: 1.0111122131347656\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.083251476287842 | KNN Loss: 5.036046504974365 | BCE Loss: 1.0472049713134766\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.084613800048828 | KNN Loss: 5.0298333168029785 | BCE Loss: 1.0547807216644287\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.100337028503418 | KNN Loss: 5.024888515472412 | BCE Loss: 1.075448751449585\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.051405906677246 | KNN Loss: 5.032938480377197 | BCE Loss: 1.0184674263000488\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.073572158813477 | KNN Loss: 5.033019542694092 | BCE Loss: 1.0405524969100952\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.088042259216309 | KNN Loss: 5.028675079345703 | BCE Loss: 1.059367299079895\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.075641632080078 | KNN Loss: 5.027708530426025 | BCE Loss: 1.0479333400726318\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.075137138366699 | KNN Loss: 5.032614707946777 | BCE Loss: 1.0425225496292114\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.060827732086182 | KNN Loss: 5.033412933349609 | BCE Loss: 1.0274147987365723\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.08056640625 | KNN Loss: 5.030614376068115 | BCE Loss: 1.0499522686004639\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.061213493347168 | KNN Loss: 5.031192302703857 | BCE Loss: 1.0300209522247314\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.068477630615234 | KNN Loss: 5.032991409301758 | BCE Loss: 1.035486102104187\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.08159875869751 | KNN Loss: 5.037308216094971 | BCE Loss: 1.0442904233932495\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.034083843231201 | KNN Loss: 5.037633419036865 | BCE Loss: 0.9964505434036255\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.077603340148926 | KNN Loss: 5.0337724685668945 | BCE Loss: 1.0438306331634521\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.093423366546631 | KNN Loss: 5.031113147735596 | BCE Loss: 1.0623100996017456\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.109402179718018 | KNN Loss: 5.035396575927734 | BCE Loss: 1.0740054845809937\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.079955101013184 | KNN Loss: 5.030783653259277 | BCE Loss: 1.0491713285446167\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.061760902404785 | KNN Loss: 5.034733772277832 | BCE Loss: 1.0270273685455322\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.052343368530273 | KNN Loss: 5.0298027992248535 | BCE Loss: 1.022540807723999\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.082375526428223 | KNN Loss: 5.029959201812744 | BCE Loss: 1.0524163246154785\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.071342468261719 | KNN Loss: 5.032398223876953 | BCE Loss: 1.0389442443847656\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.095871448516846 | KNN Loss: 5.035552024841309 | BCE Loss: 1.060319423675537\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.046813011169434 | KNN Loss: 5.036183834075928 | BCE Loss: 1.0106289386749268\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.090032577514648 | KNN Loss: 5.038480281829834 | BCE Loss: 1.0515520572662354\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.084817409515381 | KNN Loss: 5.036640644073486 | BCE Loss: 1.0481767654418945\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.105473518371582 | KNN Loss: 5.028213977813721 | BCE Loss: 1.0772593021392822\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.080899238586426 | KNN Loss: 5.039345741271973 | BCE Loss: 1.0415534973144531\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.063426971435547 | KNN Loss: 5.046916961669922 | BCE Loss: 1.016510248184204\n",
      "Epoch   351: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.055588245391846 | KNN Loss: 5.031239032745361 | BCE Loss: 1.024349331855774\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.0717267990112305 | KNN Loss: 5.031929016113281 | BCE Loss: 1.0397977828979492\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.055513858795166 | KNN Loss: 5.036241054534912 | BCE Loss: 1.0192726850509644\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.044930934906006 | KNN Loss: 5.02719259262085 | BCE Loss: 1.0177382230758667\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.113974571228027 | KNN Loss: 5.0470194816589355 | BCE Loss: 1.0669552087783813\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.07381010055542 | KNN Loss: 5.043585777282715 | BCE Loss: 1.0302244424819946\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.092623710632324 | KNN Loss: 5.033761978149414 | BCE Loss: 1.0588618516921997\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.071952819824219 | KNN Loss: 5.041338920593262 | BCE Loss: 1.0306137800216675\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.116124629974365 | KNN Loss: 5.053154945373535 | BCE Loss: 1.0629698038101196\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.071089744567871 | KNN Loss: 5.041271209716797 | BCE Loss: 1.0298182964324951\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.078423023223877 | KNN Loss: 5.036281108856201 | BCE Loss: 1.0421420335769653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.080989837646484 | KNN Loss: 5.040487766265869 | BCE Loss: 1.0405018329620361\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.086658000946045 | KNN Loss: 5.028843879699707 | BCE Loss: 1.057814121246338\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.049939155578613 | KNN Loss: 5.0324625968933105 | BCE Loss: 1.0174767971038818\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.059459686279297 | KNN Loss: 5.030266761779785 | BCE Loss: 1.0291929244995117\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.061821937561035 | KNN Loss: 5.0346903800964355 | BCE Loss: 1.0271315574645996\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.041083812713623 | KNN Loss: 5.032082557678223 | BCE Loss: 1.0090012550354004\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.100796699523926 | KNN Loss: 5.039881229400635 | BCE Loss: 1.0609153509140015\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.119894504547119 | KNN Loss: 5.032805442810059 | BCE Loss: 1.0870890617370605\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.078519821166992 | KNN Loss: 5.042973518371582 | BCE Loss: 1.0355463027954102\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.061323165893555 | KNN Loss: 5.029226303100586 | BCE Loss: 1.0320971012115479\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.105090141296387 | KNN Loss: 5.034761428833008 | BCE Loss: 1.070328712463379\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.041252136230469 | KNN Loss: 5.034947872161865 | BCE Loss: 1.0063042640686035\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.081907272338867 | KNN Loss: 5.038588523864746 | BCE Loss: 1.0433186292648315\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.06616735458374 | KNN Loss: 5.034224033355713 | BCE Loss: 1.031943440437317\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.0720930099487305 | KNN Loss: 5.0264892578125 | BCE Loss: 1.0456039905548096\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.094605445861816 | KNN Loss: 5.030678749084473 | BCE Loss: 1.0639264583587646\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.066761016845703 | KNN Loss: 5.032471179962158 | BCE Loss: 1.034289836883545\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.054387092590332 | KNN Loss: 5.022805213928223 | BCE Loss: 1.0315816402435303\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.081552028656006 | KNN Loss: 5.035702228546143 | BCE Loss: 1.0458499193191528\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.056789875030518 | KNN Loss: 5.033401012420654 | BCE Loss: 1.0233888626098633\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.094740390777588 | KNN Loss: 5.058236598968506 | BCE Loss: 1.036503791809082\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.063102722167969 | KNN Loss: 5.029472351074219 | BCE Loss: 1.033630132675171\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.042994499206543 | KNN Loss: 5.026155471801758 | BCE Loss: 1.0168392658233643\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.069780349731445 | KNN Loss: 5.032419204711914 | BCE Loss: 1.0373611450195312\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.0789031982421875 | KNN Loss: 5.052640438079834 | BCE Loss: 1.0262627601623535\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.057672500610352 | KNN Loss: 5.033515930175781 | BCE Loss: 1.0241568088531494\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.075954437255859 | KNN Loss: 5.0482497215271 | BCE Loss: 1.0277044773101807\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.096837997436523 | KNN Loss: 5.031072616577148 | BCE Loss: 1.065765142440796\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.078022003173828 | KNN Loss: 5.031670093536377 | BCE Loss: 1.0463521480560303\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.096865177154541 | KNN Loss: 5.035854816436768 | BCE Loss: 1.061010479927063\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.06904935836792 | KNN Loss: 5.031222820281982 | BCE Loss: 1.0378265380859375\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.114295482635498 | KNN Loss: 5.048050880432129 | BCE Loss: 1.0662447214126587\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.103542804718018 | KNN Loss: 5.029628753662109 | BCE Loss: 1.0739140510559082\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.112473011016846 | KNN Loss: 5.04182767868042 | BCE Loss: 1.0706454515457153\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.098946571350098 | KNN Loss: 5.048305511474609 | BCE Loss: 1.0506408214569092\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.064844608306885 | KNN Loss: 5.052960395812988 | BCE Loss: 1.011884093284607\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.0606184005737305 | KNN Loss: 5.034954071044922 | BCE Loss: 1.0256645679473877\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.046060562133789 | KNN Loss: 5.029727935791016 | BCE Loss: 1.0163328647613525\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.071747303009033 | KNN Loss: 5.030604839324951 | BCE Loss: 1.0411425828933716\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.071681022644043 | KNN Loss: 5.03235387802124 | BCE Loss: 1.0393269062042236\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.069645881652832 | KNN Loss: 5.032140254974365 | BCE Loss: 1.0375053882598877\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.1157941818237305 | KNN Loss: 5.058148384094238 | BCE Loss: 1.0576456785202026\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.078126907348633 | KNN Loss: 5.043074131011963 | BCE Loss: 1.035053014755249\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.089735984802246 | KNN Loss: 5.032273769378662 | BCE Loss: 1.0574619770050049\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.080693244934082 | KNN Loss: 5.044712066650391 | BCE Loss: 1.0359809398651123\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.07138204574585 | KNN Loss: 5.033224105834961 | BCE Loss: 1.0381579399108887\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.083916664123535 | KNN Loss: 5.041799545288086 | BCE Loss: 1.0421168804168701\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.1097822189331055 | KNN Loss: 5.042463302612305 | BCE Loss: 1.0673186779022217\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.081106185913086 | KNN Loss: 5.030073165893555 | BCE Loss: 1.0510330200195312\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.0552778244018555 | KNN Loss: 5.035006999969482 | BCE Loss: 1.020270586013794\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.067910194396973 | KNN Loss: 5.035141944885254 | BCE Loss: 1.0327680110931396\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.081789016723633 | KNN Loss: 5.029719829559326 | BCE Loss: 1.0520693063735962\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.093012809753418 | KNN Loss: 5.0427703857421875 | BCE Loss: 1.05024254322052\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.06551456451416 | KNN Loss: 5.028726577758789 | BCE Loss: 1.0367882251739502\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.034808158874512 | KNN Loss: 5.02499532699585 | BCE Loss: 1.009812593460083\n",
      "Epoch   362: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.089631080627441 | KNN Loss: 5.031485557556152 | BCE Loss: 1.05814528465271\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.05549430847168 | KNN Loss: 5.0322723388671875 | BCE Loss: 1.0232222080230713\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.064449787139893 | KNN Loss: 5.029391765594482 | BCE Loss: 1.0350580215454102\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.066286087036133 | KNN Loss: 5.028836250305176 | BCE Loss: 1.0374500751495361\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.055070877075195 | KNN Loss: 5.031491279602051 | BCE Loss: 1.023579478263855\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.082376480102539 | KNN Loss: 5.036953449249268 | BCE Loss: 1.0454230308532715\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.052201747894287 | KNN Loss: 5.031196117401123 | BCE Loss: 1.021005630493164\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.061903953552246 | KNN Loss: 5.030388832092285 | BCE Loss: 1.031515121459961\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.124854564666748 | KNN Loss: 5.038259029388428 | BCE Loss: 1.0865955352783203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.063923358917236 | KNN Loss: 5.037242889404297 | BCE Loss: 1.026680588722229\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.074005126953125 | KNN Loss: 5.035107612609863 | BCE Loss: 1.0388977527618408\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.076988220214844 | KNN Loss: 5.043315887451172 | BCE Loss: 1.033672571182251\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.070315361022949 | KNN Loss: 5.030518054962158 | BCE Loss: 1.039797067642212\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.090076446533203 | KNN Loss: 5.050982475280762 | BCE Loss: 1.0390942096710205\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.121639251708984 | KNN Loss: 5.052523136138916 | BCE Loss: 1.0691161155700684\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.106818675994873 | KNN Loss: 5.05309534072876 | BCE Loss: 1.0537232160568237\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.059082508087158 | KNN Loss: 5.030580043792725 | BCE Loss: 1.0285025835037231\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.09753942489624 | KNN Loss: 5.0426411628723145 | BCE Loss: 1.0548982620239258\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.072585105895996 | KNN Loss: 5.039813995361328 | BCE Loss: 1.0327709913253784\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.100865364074707 | KNN Loss: 5.046725749969482 | BCE Loss: 1.0541396141052246\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.110012054443359 | KNN Loss: 5.035735130310059 | BCE Loss: 1.0742766857147217\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.059731483459473 | KNN Loss: 5.037653923034668 | BCE Loss: 1.0220777988433838\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.083697319030762 | KNN Loss: 5.033185005187988 | BCE Loss: 1.0505125522613525\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.048016548156738 | KNN Loss: 5.034820079803467 | BCE Loss: 1.0131967067718506\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.059834003448486 | KNN Loss: 5.0328216552734375 | BCE Loss: 1.0270123481750488\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.069570541381836 | KNN Loss: 5.036156177520752 | BCE Loss: 1.0334144830703735\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.099308967590332 | KNN Loss: 5.049655914306641 | BCE Loss: 1.0496530532836914\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.097165107727051 | KNN Loss: 5.0426554679870605 | BCE Loss: 1.0545098781585693\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.072330951690674 | KNN Loss: 5.028430938720703 | BCE Loss: 1.0439000129699707\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.067794322967529 | KNN Loss: 5.0365986824035645 | BCE Loss: 1.0311957597732544\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.085137844085693 | KNN Loss: 5.036673545837402 | BCE Loss: 1.0484641790390015\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.077347278594971 | KNN Loss: 5.031124591827393 | BCE Loss: 1.0462226867675781\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.102649688720703 | KNN Loss: 5.0435471534729 | BCE Loss: 1.0591022968292236\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.067795753479004 | KNN Loss: 5.030386924743652 | BCE Loss: 1.0374088287353516\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.060274600982666 | KNN Loss: 5.0302557945251465 | BCE Loss: 1.030018925666809\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.054474353790283 | KNN Loss: 5.033059597015381 | BCE Loss: 1.0214147567749023\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.090034484863281 | KNN Loss: 5.03727388381958 | BCE Loss: 1.052760362625122\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.04343843460083 | KNN Loss: 5.023530960083008 | BCE Loss: 1.0199074745178223\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.0653157234191895 | KNN Loss: 5.040604114532471 | BCE Loss: 1.0247116088867188\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.067088603973389 | KNN Loss: 5.030686855316162 | BCE Loss: 1.0364017486572266\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.066475868225098 | KNN Loss: 5.041891098022461 | BCE Loss: 1.0245847702026367\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.110988616943359 | KNN Loss: 5.07558536529541 | BCE Loss: 1.0354031324386597\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.08947229385376 | KNN Loss: 5.032046318054199 | BCE Loss: 1.0574259757995605\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.053318023681641 | KNN Loss: 5.0271735191345215 | BCE Loss: 1.02614426612854\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.066560745239258 | KNN Loss: 5.032325267791748 | BCE Loss: 1.0342353582382202\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.078268051147461 | KNN Loss: 5.03004789352417 | BCE Loss: 1.0482203960418701\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.063420295715332 | KNN Loss: 5.0280351638793945 | BCE Loss: 1.0353853702545166\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.039492607116699 | KNN Loss: 5.0381269454956055 | BCE Loss: 1.0013655424118042\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.074106693267822 | KNN Loss: 5.037639617919922 | BCE Loss: 1.03646719455719\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.075572967529297 | KNN Loss: 5.031535625457764 | BCE Loss: 1.044037103652954\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.057044982910156 | KNN Loss: 5.029630184173584 | BCE Loss: 1.0274145603179932\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.062771797180176 | KNN Loss: 5.030928134918213 | BCE Loss: 1.0318435430526733\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.064332485198975 | KNN Loss: 5.029212474822998 | BCE Loss: 1.0351200103759766\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.097107887268066 | KNN Loss: 5.046322345733643 | BCE Loss: 1.0507853031158447\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.076211452484131 | KNN Loss: 5.029051303863525 | BCE Loss: 1.0471601486206055\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.064267158508301 | KNN Loss: 5.041424751281738 | BCE Loss: 1.0228426456451416\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.045694351196289 | KNN Loss: 5.032673358917236 | BCE Loss: 1.0130209922790527\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.062938690185547 | KNN Loss: 5.0411763191223145 | BCE Loss: 1.0217626094818115\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.103526592254639 | KNN Loss: 5.050727844238281 | BCE Loss: 1.0527986288070679\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.043311595916748 | KNN Loss: 5.027654647827148 | BCE Loss: 1.0156570672988892\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.054859161376953 | KNN Loss: 5.025964736938477 | BCE Loss: 1.0288944244384766\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.08483362197876 | KNN Loss: 5.044589519500732 | BCE Loss: 1.0402441024780273\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.046517372131348 | KNN Loss: 5.028307914733887 | BCE Loss: 1.018209457397461\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.057229042053223 | KNN Loss: 5.037893772125244 | BCE Loss: 1.0193350315093994\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.066695213317871 | KNN Loss: 5.03799295425415 | BCE Loss: 1.0287022590637207\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.056210041046143 | KNN Loss: 5.046642780303955 | BCE Loss: 1.009567379951477\n",
      "Epoch   373: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.0949296951293945 | KNN Loss: 5.058491230010986 | BCE Loss: 1.0364385843276978\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.050173282623291 | KNN Loss: 5.030480861663818 | BCE Loss: 1.0196924209594727\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.087848663330078 | KNN Loss: 5.027623653411865 | BCE Loss: 1.0602247714996338\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.061183929443359 | KNN Loss: 5.0277886390686035 | BCE Loss: 1.033395528793335\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.055168628692627 | KNN Loss: 5.031072616577148 | BCE Loss: 1.024095892906189\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.035317897796631 | KNN Loss: 5.03643274307251 | BCE Loss: 0.9988850355148315\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.073888778686523 | KNN Loss: 5.041868209838867 | BCE Loss: 1.0320205688476562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.083366870880127 | KNN Loss: 5.036783218383789 | BCE Loss: 1.0465835332870483\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.058257102966309 | KNN Loss: 5.030787944793701 | BCE Loss: 1.0274691581726074\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.058038234710693 | KNN Loss: 5.031131267547607 | BCE Loss: 1.026906967163086\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.079001426696777 | KNN Loss: 5.033856391906738 | BCE Loss: 1.04514479637146\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.08532190322876 | KNN Loss: 5.046485424041748 | BCE Loss: 1.0388364791870117\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.063356399536133 | KNN Loss: 5.039042949676514 | BCE Loss: 1.0243136882781982\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.044239044189453 | KNN Loss: 5.027237892150879 | BCE Loss: 1.0170009136199951\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.0674028396606445 | KNN Loss: 5.0397162437438965 | BCE Loss: 1.0276868343353271\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.057137489318848 | KNN Loss: 5.032113552093506 | BCE Loss: 1.0250239372253418\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.132705211639404 | KNN Loss: 5.033212184906006 | BCE Loss: 1.0994930267333984\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.053725719451904 | KNN Loss: 5.033555030822754 | BCE Loss: 1.0201705694198608\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.0470380783081055 | KNN Loss: 5.0307183265686035 | BCE Loss: 1.0163195133209229\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.092656135559082 | KNN Loss: 5.047101974487305 | BCE Loss: 1.045554280281067\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.092950344085693 | KNN Loss: 5.047238826751709 | BCE Loss: 1.0457115173339844\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.086481094360352 | KNN Loss: 5.041496276855469 | BCE Loss: 1.0449845790863037\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.072316646575928 | KNN Loss: 5.025596618652344 | BCE Loss: 1.0467201471328735\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.0747222900390625 | KNN Loss: 5.023169994354248 | BCE Loss: 1.051552414894104\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.0904130935668945 | KNN Loss: 5.053232192993164 | BCE Loss: 1.0371806621551514\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.0662078857421875 | KNN Loss: 5.033022880554199 | BCE Loss: 1.0331851243972778\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.082118034362793 | KNN Loss: 5.032173156738281 | BCE Loss: 1.0499446392059326\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.049324035644531 | KNN Loss: 5.031015872955322 | BCE Loss: 1.0183080434799194\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.086271286010742 | KNN Loss: 5.041533946990967 | BCE Loss: 1.0447375774383545\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.1329731941223145 | KNN Loss: 5.068785190582275 | BCE Loss: 1.064188003540039\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.107752799987793 | KNN Loss: 5.041760444641113 | BCE Loss: 1.0659925937652588\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.124237060546875 | KNN Loss: 5.033627986907959 | BCE Loss: 1.090608835220337\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.085880279541016 | KNN Loss: 5.033257484436035 | BCE Loss: 1.0526227951049805\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.05458927154541 | KNN Loss: 5.025660037994385 | BCE Loss: 1.028929352760315\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.064496994018555 | KNN Loss: 5.030265808105469 | BCE Loss: 1.034231185913086\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.067280292510986 | KNN Loss: 5.042697429656982 | BCE Loss: 1.0245827436447144\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.051355361938477 | KNN Loss: 5.034061908721924 | BCE Loss: 1.0172935724258423\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.04968786239624 | KNN Loss: 5.030323505401611 | BCE Loss: 1.019364356994629\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.067899703979492 | KNN Loss: 5.032205581665039 | BCE Loss: 1.0356943607330322\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.054886817932129 | KNN Loss: 5.034885883331299 | BCE Loss: 1.02000093460083\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.080240726470947 | KNN Loss: 5.031194686889648 | BCE Loss: 1.0490460395812988\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.088759422302246 | KNN Loss: 5.036552906036377 | BCE Loss: 1.0522065162658691\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.096472263336182 | KNN Loss: 5.035628318786621 | BCE Loss: 1.06084406375885\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.1003594398498535 | KNN Loss: 5.036735534667969 | BCE Loss: 1.0636237859725952\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.036211013793945 | KNN Loss: 5.02669095993042 | BCE Loss: 1.0095198154449463\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.046151161193848 | KNN Loss: 5.026698589324951 | BCE Loss: 1.0194528102874756\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.133448600769043 | KNN Loss: 5.068502426147461 | BCE Loss: 1.0649464130401611\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.066771030426025 | KNN Loss: 5.029904842376709 | BCE Loss: 1.0368660688400269\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.053571701049805 | KNN Loss: 5.028697967529297 | BCE Loss: 1.0248737335205078\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.068737030029297 | KNN Loss: 5.028686046600342 | BCE Loss: 1.0400512218475342\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.094280242919922 | KNN Loss: 5.030940532684326 | BCE Loss: 1.0633399486541748\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.03481388092041 | KNN Loss: 5.026857376098633 | BCE Loss: 1.0079562664031982\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.076193809509277 | KNN Loss: 5.035268306732178 | BCE Loss: 1.0409257411956787\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.087796211242676 | KNN Loss: 5.0568437576293945 | BCE Loss: 1.0309525728225708\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.1133503913879395 | KNN Loss: 5.058649063110352 | BCE Loss: 1.0547014474868774\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.09571647644043 | KNN Loss: 5.046788215637207 | BCE Loss: 1.0489284992218018\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.098380088806152 | KNN Loss: 5.054157733917236 | BCE Loss: 1.044222354888916\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.098311424255371 | KNN Loss: 5.043241024017334 | BCE Loss: 1.055070400238037\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.093293190002441 | KNN Loss: 5.0416693687438965 | BCE Loss: 1.0516239404678345\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.079015254974365 | KNN Loss: 5.043045997619629 | BCE Loss: 1.0359691381454468\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.069365978240967 | KNN Loss: 5.030283451080322 | BCE Loss: 1.0390825271606445\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.07623291015625 | KNN Loss: 5.029757022857666 | BCE Loss: 1.0464756488800049\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.065386772155762 | KNN Loss: 5.029496669769287 | BCE Loss: 1.0358902215957642\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.101169586181641 | KNN Loss: 5.049283504486084 | BCE Loss: 1.0518860816955566\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.045947074890137 | KNN Loss: 5.0295844078063965 | BCE Loss: 1.0163625478744507\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.094334125518799 | KNN Loss: 5.027562618255615 | BCE Loss: 1.0667715072631836\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.055181503295898 | KNN Loss: 5.030362129211426 | BCE Loss: 1.0248193740844727\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.066660404205322 | KNN Loss: 5.030584812164307 | BCE Loss: 1.036075472831726\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.064552307128906 | KNN Loss: 5.040406703948975 | BCE Loss: 1.0241456031799316\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.0769734382629395 | KNN Loss: 5.0311360359191895 | BCE Loss: 1.04583740234375\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.057500839233398 | KNN Loss: 5.031260967254639 | BCE Loss: 1.0262396335601807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.043972492218018 | KNN Loss: 5.026390552520752 | BCE Loss: 1.0175819396972656\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.071282386779785 | KNN Loss: 5.037160396575928 | BCE Loss: 1.0341217517852783\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.046072006225586 | KNN Loss: 5.0238776206970215 | BCE Loss: 1.0221941471099854\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.080229759216309 | KNN Loss: 5.039120197296143 | BCE Loss: 1.0411098003387451\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.098684310913086 | KNN Loss: 5.046571731567383 | BCE Loss: 1.052112340927124\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.057171821594238 | KNN Loss: 5.028180122375488 | BCE Loss: 1.028991460800171\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.069547653198242 | KNN Loss: 5.033443927764893 | BCE Loss: 1.0361034870147705\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.059021949768066 | KNN Loss: 5.029446125030518 | BCE Loss: 1.0295757055282593\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.048927307128906 | KNN Loss: 5.0224080085754395 | BCE Loss: 1.0265190601348877\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.098724842071533 | KNN Loss: 5.033605575561523 | BCE Loss: 1.0651192665100098\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.0549211502075195 | KNN Loss: 5.023452281951904 | BCE Loss: 1.0314688682556152\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.050715923309326 | KNN Loss: 5.036924839019775 | BCE Loss: 1.0137909650802612\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.05696964263916 | KNN Loss: 5.033509254455566 | BCE Loss: 1.0234606266021729\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.058985710144043 | KNN Loss: 5.031436443328857 | BCE Loss: 1.0275492668151855\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.030900001525879 | KNN Loss: 5.027769088745117 | BCE Loss: 1.0031307935714722\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.0629472732543945 | KNN Loss: 5.032861232757568 | BCE Loss: 1.0300859212875366\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.084624290466309 | KNN Loss: 5.033272743225098 | BCE Loss: 1.0513516664505005\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.051648139953613 | KNN Loss: 5.026298999786377 | BCE Loss: 1.0253489017486572\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.040722370147705 | KNN Loss: 5.023036479949951 | BCE Loss: 1.017685890197754\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.062082290649414 | KNN Loss: 5.026957988739014 | BCE Loss: 1.0351240634918213\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.08023738861084 | KNN Loss: 5.027176856994629 | BCE Loss: 1.053060531616211\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.092637062072754 | KNN Loss: 5.036396026611328 | BCE Loss: 1.0562412738800049\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.069709777832031 | KNN Loss: 5.039693355560303 | BCE Loss: 1.0300161838531494\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.052727699279785 | KNN Loss: 5.032047748565674 | BCE Loss: 1.0206799507141113\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.083105087280273 | KNN Loss: 5.037817478179932 | BCE Loss: 1.045287847518921\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.062712669372559 | KNN Loss: 5.027246475219727 | BCE Loss: 1.035465955734253\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.069645881652832 | KNN Loss: 5.033876419067383 | BCE Loss: 1.0357693433761597\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.092555999755859 | KNN Loss: 5.036977291107178 | BCE Loss: 1.055578589439392\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.062195777893066 | KNN Loss: 5.046041965484619 | BCE Loss: 1.0161535739898682\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.0801239013671875 | KNN Loss: 5.042636871337891 | BCE Loss: 1.0374867916107178\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.10089111328125 | KNN Loss: 5.059607028961182 | BCE Loss: 1.0412838459014893\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.07632303237915 | KNN Loss: 5.027798652648926 | BCE Loss: 1.0485243797302246\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.089302062988281 | KNN Loss: 5.028749465942383 | BCE Loss: 1.0605528354644775\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.081334114074707 | KNN Loss: 5.032143592834473 | BCE Loss: 1.0491907596588135\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.0905656814575195 | KNN Loss: 5.0410356521606445 | BCE Loss: 1.049530267715454\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.061769485473633 | KNN Loss: 5.038360118865967 | BCE Loss: 1.023409366607666\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.0822601318359375 | KNN Loss: 5.033380031585693 | BCE Loss: 1.048879861831665\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.062029838562012 | KNN Loss: 5.038631439208984 | BCE Loss: 1.0233983993530273\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.110735893249512 | KNN Loss: 5.043633937835693 | BCE Loss: 1.0671019554138184\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.087484359741211 | KNN Loss: 5.041075706481934 | BCE Loss: 1.0464085340499878\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.077680587768555 | KNN Loss: 5.054453372955322 | BCE Loss: 1.0232272148132324\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.041297912597656 | KNN Loss: 5.03451681137085 | BCE Loss: 1.0067812204360962\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.029150009155273 | KNN Loss: 5.026112079620361 | BCE Loss: 1.0030381679534912\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.064767360687256 | KNN Loss: 5.03783655166626 | BCE Loss: 1.026930809020996\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.087008953094482 | KNN Loss: 5.030889987945557 | BCE Loss: 1.0561189651489258\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.0967888832092285 | KNN Loss: 5.055261135101318 | BCE Loss: 1.0415276288986206\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.073115825653076 | KNN Loss: 5.033851146697998 | BCE Loss: 1.0392645597457886\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.068274974822998 | KNN Loss: 5.041711807250977 | BCE Loss: 1.026563048362732\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.095776557922363 | KNN Loss: 5.042918682098389 | BCE Loss: 1.0528578758239746\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.0720977783203125 | KNN Loss: 5.044656276702881 | BCE Loss: 1.0274416208267212\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.060353755950928 | KNN Loss: 5.029254913330078 | BCE Loss: 1.0310988426208496\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.06720495223999 | KNN Loss: 5.031716346740723 | BCE Loss: 1.0354887247085571\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.094399452209473 | KNN Loss: 5.030977725982666 | BCE Loss: 1.0634214878082275\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.096458911895752 | KNN Loss: 5.026883125305176 | BCE Loss: 1.0695756673812866\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.074963569641113 | KNN Loss: 5.047842502593994 | BCE Loss: 1.02712082862854\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.069557189941406 | KNN Loss: 5.0282745361328125 | BCE Loss: 1.0412828922271729\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.070944786071777 | KNN Loss: 5.036639213562012 | BCE Loss: 1.0343058109283447\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.044914245605469 | KNN Loss: 5.032557487487793 | BCE Loss: 1.0123569965362549\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.033715724945068 | KNN Loss: 5.031671047210693 | BCE Loss: 1.0020445585250854\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.076959133148193 | KNN Loss: 5.033240795135498 | BCE Loss: 1.0437184572219849\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.0750732421875 | KNN Loss: 5.0326151847839355 | BCE Loss: 1.0424580574035645\n",
      "Epoch   395: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.082140922546387 | KNN Loss: 5.0399169921875 | BCE Loss: 1.0422241687774658\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.089170455932617 | KNN Loss: 5.028815746307373 | BCE Loss: 1.060354471206665\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.055013656616211 | KNN Loss: 5.032528877258301 | BCE Loss: 1.0224847793579102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.125980854034424 | KNN Loss: 5.054740905761719 | BCE Loss: 1.0712398290634155\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.045041561126709 | KNN Loss: 5.038346290588379 | BCE Loss: 1.0066953897476196\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.05496883392334 | KNN Loss: 5.027224063873291 | BCE Loss: 1.0277445316314697\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.0308027267456055 | KNN Loss: 5.042102336883545 | BCE Loss: 0.9887003302574158\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.054042339324951 | KNN Loss: 5.035949230194092 | BCE Loss: 1.0180931091308594\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.104779243469238 | KNN Loss: 5.047010898590088 | BCE Loss: 1.0577685832977295\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.071751594543457 | KNN Loss: 5.036447525024414 | BCE Loss: 1.0353041887283325\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.050614356994629 | KNN Loss: 5.037130355834961 | BCE Loss: 1.013484001159668\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.07289981842041 | KNN Loss: 5.030308246612549 | BCE Loss: 1.0425918102264404\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.070681571960449 | KNN Loss: 5.0292558670043945 | BCE Loss: 1.0414257049560547\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.091306686401367 | KNN Loss: 5.030966281890869 | BCE Loss: 1.0603405237197876\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.094440937042236 | KNN Loss: 5.03380823135376 | BCE Loss: 1.0606327056884766\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.051204204559326 | KNN Loss: 5.034459590911865 | BCE Loss: 1.016744613647461\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.068243026733398 | KNN Loss: 5.03197717666626 | BCE Loss: 1.0362658500671387\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.04798698425293 | KNN Loss: 5.030206203460693 | BCE Loss: 1.0177806615829468\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.045658111572266 | KNN Loss: 5.028674125671387 | BCE Loss: 1.0169837474822998\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.030950546264648 | KNN Loss: 5.025890827178955 | BCE Loss: 1.0050594806671143\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.053985595703125 | KNN Loss: 5.030764579772949 | BCE Loss: 1.0232210159301758\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.087132930755615 | KNN Loss: 5.035101890563965 | BCE Loss: 1.0520310401916504\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.079928398132324 | KNN Loss: 5.029682636260986 | BCE Loss: 1.050246000289917\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.024331092834473 | KNN Loss: 5.030301570892334 | BCE Loss: 0.9940294623374939\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.072078704833984 | KNN Loss: 5.028548240661621 | BCE Loss: 1.0435304641723633\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.077850341796875 | KNN Loss: 5.030982971191406 | BCE Loss: 1.0468673706054688\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.051298141479492 | KNN Loss: 5.033556938171387 | BCE Loss: 1.0177409648895264\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.064826965332031 | KNN Loss: 5.0284295082092285 | BCE Loss: 1.0363972187042236\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.083218097686768 | KNN Loss: 5.049504280090332 | BCE Loss: 1.0337138175964355\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.067418098449707 | KNN Loss: 5.033198356628418 | BCE Loss: 1.034219741821289\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.054566383361816 | KNN Loss: 5.032425880432129 | BCE Loss: 1.022140622138977\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.063933372497559 | KNN Loss: 5.032196521759033 | BCE Loss: 1.0317366123199463\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.085504055023193 | KNN Loss: 5.038055896759033 | BCE Loss: 1.0474481582641602\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.087764739990234 | KNN Loss: 5.039374828338623 | BCE Loss: 1.0483901500701904\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.11275577545166 | KNN Loss: 5.053900241851807 | BCE Loss: 1.058855414390564\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.102560043334961 | KNN Loss: 5.031399250030518 | BCE Loss: 1.0711607933044434\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.044122695922852 | KNN Loss: 5.032341957092285 | BCE Loss: 1.0117807388305664\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.065034866333008 | KNN Loss: 5.034754753112793 | BCE Loss: 1.0302799940109253\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.081873893737793 | KNN Loss: 5.027529239654541 | BCE Loss: 1.054344654083252\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.100643157958984 | KNN Loss: 5.034583568572998 | BCE Loss: 1.0660595893859863\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.038496017456055 | KNN Loss: 5.036447048187256 | BCE Loss: 1.0020490884780884\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.078523635864258 | KNN Loss: 5.037637710571289 | BCE Loss: 1.0408860445022583\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.071591377258301 | KNN Loss: 5.033167362213135 | BCE Loss: 1.0384241342544556\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.116796970367432 | KNN Loss: 5.052080154418945 | BCE Loss: 1.0647168159484863\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.080440521240234 | KNN Loss: 5.034635543823242 | BCE Loss: 1.045804738998413\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.069267749786377 | KNN Loss: 5.031351089477539 | BCE Loss: 1.037916660308838\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.090031147003174 | KNN Loss: 5.041163444519043 | BCE Loss: 1.0488675832748413\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.092386245727539 | KNN Loss: 5.053345203399658 | BCE Loss: 1.0390410423278809\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.103431701660156 | KNN Loss: 5.0292558670043945 | BCE Loss: 1.0741755962371826\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.1190595626831055 | KNN Loss: 5.046175003051758 | BCE Loss: 1.0728847980499268\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.084890842437744 | KNN Loss: 5.035507678985596 | BCE Loss: 1.0493831634521484\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.066302299499512 | KNN Loss: 5.029487609863281 | BCE Loss: 1.0368146896362305\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.055130958557129 | KNN Loss: 5.033506870269775 | BCE Loss: 1.0216238498687744\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.096630573272705 | KNN Loss: 5.029465675354004 | BCE Loss: 1.0671648979187012\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.06036376953125 | KNN Loss: 5.027566432952881 | BCE Loss: 1.0327975749969482\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.063039779663086 | KNN Loss: 5.029209136962891 | BCE Loss: 1.0338306427001953\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.052401542663574 | KNN Loss: 5.029244899749756 | BCE Loss: 1.0231568813323975\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.0866875648498535 | KNN Loss: 5.031964302062988 | BCE Loss: 1.0547232627868652\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.092678070068359 | KNN Loss: 5.0510430335998535 | BCE Loss: 1.0416347980499268\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.100895881652832 | KNN Loss: 5.063819408416748 | BCE Loss: 1.0370762348175049\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.095714569091797 | KNN Loss: 5.045426845550537 | BCE Loss: 1.0502876043319702\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.162801742553711 | KNN Loss: 5.087215423583984 | BCE Loss: 1.075586199760437\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.0684814453125 | KNN Loss: 5.031373500823975 | BCE Loss: 1.037108063697815\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.065412521362305 | KNN Loss: 5.0300140380859375 | BCE Loss: 1.0353984832763672\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.083454132080078 | KNN Loss: 5.038120269775391 | BCE Loss: 1.0453338623046875\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.056280136108398 | KNN Loss: 5.033455848693848 | BCE Loss: 1.0228242874145508\n",
      "Epoch   406: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.089199542999268 | KNN Loss: 5.04793643951416 | BCE Loss: 1.0412629842758179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.095922946929932 | KNN Loss: 5.046838283538818 | BCE Loss: 1.0490846633911133\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.056415557861328 | KNN Loss: 5.02637243270874 | BCE Loss: 1.030043125152588\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.069551467895508 | KNN Loss: 5.032211780548096 | BCE Loss: 1.037339448928833\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.066575050354004 | KNN Loss: 5.027983665466309 | BCE Loss: 1.0385916233062744\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.072520732879639 | KNN Loss: 5.033507347106934 | BCE Loss: 1.039013385772705\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.067142486572266 | KNN Loss: 5.030191421508789 | BCE Loss: 1.0369511842727661\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.062765598297119 | KNN Loss: 5.027805805206299 | BCE Loss: 1.0349596738815308\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.085871696472168 | KNN Loss: 5.03318452835083 | BCE Loss: 1.052687406539917\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.067841053009033 | KNN Loss: 5.035651206970215 | BCE Loss: 1.0321898460388184\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.088454246520996 | KNN Loss: 5.047908782958984 | BCE Loss: 1.0405454635620117\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.09266471862793 | KNN Loss: 5.031904697418213 | BCE Loss: 1.0607601404190063\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.076752185821533 | KNN Loss: 5.057309150695801 | BCE Loss: 1.0194429159164429\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.082395076751709 | KNN Loss: 5.039623260498047 | BCE Loss: 1.042771816253662\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.099148750305176 | KNN Loss: 5.045650482177734 | BCE Loss: 1.0534985065460205\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.0993194580078125 | KNN Loss: 5.0331292152404785 | BCE Loss: 1.0661900043487549\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.094608306884766 | KNN Loss: 5.033637523651123 | BCE Loss: 1.060970664024353\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.060068607330322 | KNN Loss: 5.031388759613037 | BCE Loss: 1.0286797285079956\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.046350479125977 | KNN Loss: 5.032820701599121 | BCE Loss: 1.013529896736145\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.0766496658325195 | KNN Loss: 5.034437656402588 | BCE Loss: 1.0422122478485107\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.065956115722656 | KNN Loss: 5.039243221282959 | BCE Loss: 1.0267131328582764\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.047075271606445 | KNN Loss: 5.027003765106201 | BCE Loss: 1.020071268081665\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.0584869384765625 | KNN Loss: 5.026097774505615 | BCE Loss: 1.0323889255523682\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.102383136749268 | KNN Loss: 5.043257713317871 | BCE Loss: 1.059125304222107\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.067512512207031 | KNN Loss: 5.030765056610107 | BCE Loss: 1.036747694015503\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.093497276306152 | KNN Loss: 5.040263652801514 | BCE Loss: 1.0532337427139282\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.042562484741211 | KNN Loss: 5.0294189453125 | BCE Loss: 1.0131433010101318\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.101358413696289 | KNN Loss: 5.035819053649902 | BCE Loss: 1.0655393600463867\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.059019088745117 | KNN Loss: 5.031564712524414 | BCE Loss: 1.0274543762207031\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.040223121643066 | KNN Loss: 5.028323650360107 | BCE Loss: 1.011899471282959\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.0800089836120605 | KNN Loss: 5.032858848571777 | BCE Loss: 1.0471502542495728\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.084730625152588 | KNN Loss: 5.039149761199951 | BCE Loss: 1.0455808639526367\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.044775009155273 | KNN Loss: 5.037124156951904 | BCE Loss: 1.0076510906219482\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.084249019622803 | KNN Loss: 5.046780586242676 | BCE Loss: 1.0374685525894165\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.105504035949707 | KNN Loss: 5.036947250366211 | BCE Loss: 1.0685569047927856\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.093961715698242 | KNN Loss: 5.035202503204346 | BCE Loss: 1.058759093284607\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.064598083496094 | KNN Loss: 5.034427642822266 | BCE Loss: 1.0301706790924072\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.072545051574707 | KNN Loss: 5.032684326171875 | BCE Loss: 1.039860486984253\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.0865559577941895 | KNN Loss: 5.0389580726623535 | BCE Loss: 1.047597885131836\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.098529815673828 | KNN Loss: 5.0278167724609375 | BCE Loss: 1.0707130432128906\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.04970645904541 | KNN Loss: 5.026327133178711 | BCE Loss: 1.0233790874481201\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.085826873779297 | KNN Loss: 5.041439533233643 | BCE Loss: 1.0443875789642334\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.081295967102051 | KNN Loss: 5.0282697677612305 | BCE Loss: 1.0530263185501099\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.108734130859375 | KNN Loss: 5.042264461517334 | BCE Loss: 1.066469430923462\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.0540452003479 | KNN Loss: 5.032163143157959 | BCE Loss: 1.0218820571899414\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.056321144104004 | KNN Loss: 5.055386066436768 | BCE Loss: 1.0009350776672363\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.059995651245117 | KNN Loss: 5.03277587890625 | BCE Loss: 1.0272197723388672\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.063429832458496 | KNN Loss: 5.026485443115234 | BCE Loss: 1.0369446277618408\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.047760009765625 | KNN Loss: 5.030704498291016 | BCE Loss: 1.0170555114746094\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.105976104736328 | KNN Loss: 5.0519561767578125 | BCE Loss: 1.054019808769226\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.090140342712402 | KNN Loss: 5.0344014167785645 | BCE Loss: 1.055739164352417\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.0963568687438965 | KNN Loss: 5.032595634460449 | BCE Loss: 1.0637612342834473\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.07597541809082 | KNN Loss: 5.029799938201904 | BCE Loss: 1.046175241470337\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.081387519836426 | KNN Loss: 5.034891605377197 | BCE Loss: 1.046496033668518\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.073915958404541 | KNN Loss: 5.02556848526001 | BCE Loss: 1.0483474731445312\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.097175598144531 | KNN Loss: 5.045753479003906 | BCE Loss: 1.051422357559204\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.045283317565918 | KNN Loss: 5.034640312194824 | BCE Loss: 1.0106427669525146\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.085725784301758 | KNN Loss: 5.057612419128418 | BCE Loss: 1.0281133651733398\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.065832614898682 | KNN Loss: 5.031327247619629 | BCE Loss: 1.0345053672790527\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.0576982498168945 | KNN Loss: 5.036985874176025 | BCE Loss: 1.0207123756408691\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.060766696929932 | KNN Loss: 5.0291290283203125 | BCE Loss: 1.0316376686096191\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.058688163757324 | KNN Loss: 5.03832483291626 | BCE Loss: 1.0203633308410645\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.054449081420898 | KNN Loss: 5.028348445892334 | BCE Loss: 1.026100516319275\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.069920063018799 | KNN Loss: 5.034902095794678 | BCE Loss: 1.035017967224121\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.080889701843262 | KNN Loss: 5.034631252288818 | BCE Loss: 1.0462584495544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.071086883544922 | KNN Loss: 5.036660671234131 | BCE Loss: 1.034425973892212\n",
      "Epoch   417: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.077520370483398 | KNN Loss: 5.03652811050415 | BCE Loss: 1.0409921407699585\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.122861862182617 | KNN Loss: 5.073422908782959 | BCE Loss: 1.0494391918182373\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.067098617553711 | KNN Loss: 5.0358405113220215 | BCE Loss: 1.0312583446502686\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.039282321929932 | KNN Loss: 5.034909248352051 | BCE Loss: 1.0043730735778809\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.063196182250977 | KNN Loss: 5.035141468048096 | BCE Loss: 1.0280547142028809\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.072604179382324 | KNN Loss: 5.029934883117676 | BCE Loss: 1.0426692962646484\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.093597888946533 | KNN Loss: 5.0385308265686035 | BCE Loss: 1.0550670623779297\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.060312271118164 | KNN Loss: 5.045425891876221 | BCE Loss: 1.0148863792419434\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.085261821746826 | KNN Loss: 5.033365249633789 | BCE Loss: 1.0518966913223267\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.080033779144287 | KNN Loss: 5.033419132232666 | BCE Loss: 1.0466145277023315\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.065354347229004 | KNN Loss: 5.043653964996338 | BCE Loss: 1.021700143814087\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.124892234802246 | KNN Loss: 5.030683994293213 | BCE Loss: 1.0942081212997437\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.064354419708252 | KNN Loss: 5.036055564880371 | BCE Loss: 1.0282988548278809\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.055397987365723 | KNN Loss: 5.030461311340332 | BCE Loss: 1.0249369144439697\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.034545421600342 | KNN Loss: 5.0259108543396 | BCE Loss: 1.0086344480514526\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.098031520843506 | KNN Loss: 5.033588409423828 | BCE Loss: 1.0644431114196777\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.068890571594238 | KNN Loss: 5.028008937835693 | BCE Loss: 1.0408815145492554\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.080867290496826 | KNN Loss: 5.0304646492004395 | BCE Loss: 1.0504026412963867\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.061900615692139 | KNN Loss: 5.039144992828369 | BCE Loss: 1.02275550365448\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.028905868530273 | KNN Loss: 5.027161121368408 | BCE Loss: 1.0017449855804443\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.052448272705078 | KNN Loss: 5.027917861938477 | BCE Loss: 1.0245304107666016\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.086617469787598 | KNN Loss: 5.0420708656311035 | BCE Loss: 1.044546365737915\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.0482378005981445 | KNN Loss: 5.033614635467529 | BCE Loss: 1.0146231651306152\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.047967910766602 | KNN Loss: 5.031103134155273 | BCE Loss: 1.016864538192749\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.061785697937012 | KNN Loss: 5.032277584075928 | BCE Loss: 1.0295078754425049\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.0732526779174805 | KNN Loss: 5.033879280090332 | BCE Loss: 1.0393731594085693\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.088629722595215 | KNN Loss: 5.030736446380615 | BCE Loss: 1.0578930377960205\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.063808917999268 | KNN Loss: 5.029685974121094 | BCE Loss: 1.0341229438781738\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.075265407562256 | KNN Loss: 5.038553714752197 | BCE Loss: 1.0367118120193481\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.052788734436035 | KNN Loss: 5.0364227294921875 | BCE Loss: 1.0163662433624268\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.064111232757568 | KNN Loss: 5.022673606872559 | BCE Loss: 1.0414376258850098\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.074108600616455 | KNN Loss: 5.035707950592041 | BCE Loss: 1.0384005308151245\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.049170970916748 | KNN Loss: 5.036346435546875 | BCE Loss: 1.012824535369873\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.126550674438477 | KNN Loss: 5.050639629364014 | BCE Loss: 1.0759108066558838\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.058389663696289 | KNN Loss: 5.030269622802734 | BCE Loss: 1.0281198024749756\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.070189952850342 | KNN Loss: 5.039736747741699 | BCE Loss: 1.030453085899353\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.074810028076172 | KNN Loss: 5.039623260498047 | BCE Loss: 1.0351868867874146\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.078697204589844 | KNN Loss: 5.04939079284668 | BCE Loss: 1.0293062925338745\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.072023391723633 | KNN Loss: 5.032862663269043 | BCE Loss: 1.0391607284545898\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.080098628997803 | KNN Loss: 5.04003381729126 | BCE Loss: 1.0400649309158325\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.0831193923950195 | KNN Loss: 5.027347087860107 | BCE Loss: 1.0557721853256226\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.080092430114746 | KNN Loss: 5.037571907043457 | BCE Loss: 1.0425207614898682\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.088871002197266 | KNN Loss: 5.061837673187256 | BCE Loss: 1.0270332098007202\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.060307502746582 | KNN Loss: 5.035323143005371 | BCE Loss: 1.024984359741211\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.087231636047363 | KNN Loss: 5.027310848236084 | BCE Loss: 1.0599205493927002\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.089410781860352 | KNN Loss: 5.042825698852539 | BCE Loss: 1.0465853214263916\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.067685127258301 | KNN Loss: 5.030534267425537 | BCE Loss: 1.0371508598327637\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.051366806030273 | KNN Loss: 5.02647066116333 | BCE Loss: 1.0248963832855225\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.069828987121582 | KNN Loss: 5.029937267303467 | BCE Loss: 1.0398914813995361\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.087347984313965 | KNN Loss: 5.033251762390137 | BCE Loss: 1.0540963411331177\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.077463626861572 | KNN Loss: 5.030876636505127 | BCE Loss: 1.0465869903564453\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.038431167602539 | KNN Loss: 5.026793003082275 | BCE Loss: 1.0116382837295532\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.094350814819336 | KNN Loss: 5.038081169128418 | BCE Loss: 1.0562695264816284\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.069730758666992 | KNN Loss: 5.046555519104004 | BCE Loss: 1.0231754779815674\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.075492858886719 | KNN Loss: 5.0300469398498535 | BCE Loss: 1.0454459190368652\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.051054000854492 | KNN Loss: 5.032958984375 | BCE Loss: 1.0180950164794922\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.085124969482422 | KNN Loss: 5.038747310638428 | BCE Loss: 1.0463778972625732\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.098331451416016 | KNN Loss: 5.038543701171875 | BCE Loss: 1.0597879886627197\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.130158424377441 | KNN Loss: 5.040332794189453 | BCE Loss: 1.0898258686065674\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.0692009925842285 | KNN Loss: 5.0370049476623535 | BCE Loss: 1.0321961641311646\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.068737030029297 | KNN Loss: 5.041979789733887 | BCE Loss: 1.0267571210861206\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.044087886810303 | KNN Loss: 5.027415752410889 | BCE Loss: 1.0166720151901245\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.050168037414551 | KNN Loss: 5.031549453735352 | BCE Loss: 1.0186185836791992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.062180042266846 | KNN Loss: 5.026371002197266 | BCE Loss: 1.0358091592788696\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.050676345825195 | KNN Loss: 5.028989315032959 | BCE Loss: 1.0216871500015259\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.114026069641113 | KNN Loss: 5.033078670501709 | BCE Loss: 1.0809476375579834\n",
      "Epoch   428: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.051994323730469 | KNN Loss: 5.031574249267578 | BCE Loss: 1.0204203128814697\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.065311908721924 | KNN Loss: 5.033810615539551 | BCE Loss: 1.031501293182373\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.040180683135986 | KNN Loss: 5.02991247177124 | BCE Loss: 1.010268211364746\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.069477558135986 | KNN Loss: 5.028721332550049 | BCE Loss: 1.0407562255859375\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.101532936096191 | KNN Loss: 5.034773349761963 | BCE Loss: 1.0667598247528076\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.07908821105957 | KNN Loss: 5.036500930786133 | BCE Loss: 1.0425875186920166\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.088870048522949 | KNN Loss: 5.04299783706665 | BCE Loss: 1.0458722114562988\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.056177139282227 | KNN Loss: 5.038907527923584 | BCE Loss: 1.0172693729400635\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.070089340209961 | KNN Loss: 5.030343055725098 | BCE Loss: 1.0397461652755737\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.038779258728027 | KNN Loss: 5.0281500816345215 | BCE Loss: 1.0106289386749268\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.099030494689941 | KNN Loss: 5.037569999694824 | BCE Loss: 1.0614604949951172\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.072348594665527 | KNN Loss: 5.031311511993408 | BCE Loss: 1.0410373210906982\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.073483467102051 | KNN Loss: 5.02647590637207 | BCE Loss: 1.0470075607299805\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.095113754272461 | KNN Loss: 5.0330610275268555 | BCE Loss: 1.0620529651641846\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.061771392822266 | KNN Loss: 5.029385089874268 | BCE Loss: 1.032386302947998\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.119836807250977 | KNN Loss: 5.046322822570801 | BCE Loss: 1.0735141038894653\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.075272560119629 | KNN Loss: 5.039882183074951 | BCE Loss: 1.0353903770446777\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.082110404968262 | KNN Loss: 5.031777381896973 | BCE Loss: 1.0503329038619995\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.0603227615356445 | KNN Loss: 5.027591705322266 | BCE Loss: 1.0327308177947998\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.092555046081543 | KNN Loss: 5.0391435623168945 | BCE Loss: 1.0534117221832275\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.079079627990723 | KNN Loss: 5.042717456817627 | BCE Loss: 1.0363624095916748\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.099441051483154 | KNN Loss: 5.050848007202148 | BCE Loss: 1.0485930442810059\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.105938911437988 | KNN Loss: 5.037779808044434 | BCE Loss: 1.0681591033935547\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.089712142944336 | KNN Loss: 5.056017875671387 | BCE Loss: 1.0336943864822388\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.0658369064331055 | KNN Loss: 5.0288543701171875 | BCE Loss: 1.0369822978973389\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.077157020568848 | KNN Loss: 5.043306350708008 | BCE Loss: 1.0338506698608398\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.052283763885498 | KNN Loss: 5.028980731964111 | BCE Loss: 1.0233030319213867\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.073215961456299 | KNN Loss: 5.0263166427612305 | BCE Loss: 1.046899437904358\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.107379913330078 | KNN Loss: 5.050222873687744 | BCE Loss: 1.057157039642334\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.066993236541748 | KNN Loss: 5.039967060089111 | BCE Loss: 1.0270261764526367\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.075038433074951 | KNN Loss: 5.028982639312744 | BCE Loss: 1.046055793762207\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.0690178871154785 | KNN Loss: 5.035284042358398 | BCE Loss: 1.0337339639663696\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.1130781173706055 | KNN Loss: 5.052685260772705 | BCE Loss: 1.0603928565979004\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.11251974105835 | KNN Loss: 5.049853801727295 | BCE Loss: 1.0626660585403442\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.0762128829956055 | KNN Loss: 5.038286209106445 | BCE Loss: 1.037926435470581\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.067387580871582 | KNN Loss: 5.027048587799072 | BCE Loss: 1.0403387546539307\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.063109397888184 | KNN Loss: 5.048485279083252 | BCE Loss: 1.0146241188049316\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.062686443328857 | KNN Loss: 5.028446197509766 | BCE Loss: 1.0342402458190918\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.097985744476318 | KNN Loss: 5.034679412841797 | BCE Loss: 1.063306450843811\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.085231781005859 | KNN Loss: 5.037125110626221 | BCE Loss: 1.0481064319610596\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.041750431060791 | KNN Loss: 5.028442859649658 | BCE Loss: 1.0133075714111328\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.0933074951171875 | KNN Loss: 5.030904769897461 | BCE Loss: 1.062402606010437\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.051457405090332 | KNN Loss: 5.029783248901367 | BCE Loss: 1.0216741561889648\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.03397274017334 | KNN Loss: 5.033153533935547 | BCE Loss: 1.000819444656372\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.095996856689453 | KNN Loss: 5.037397384643555 | BCE Loss: 1.058599591255188\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.063930034637451 | KNN Loss: 5.02858304977417 | BCE Loss: 1.0353469848632812\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.111850261688232 | KNN Loss: 5.03729772567749 | BCE Loss: 1.0745526552200317\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.068626403808594 | KNN Loss: 5.029445171356201 | BCE Loss: 1.0391812324523926\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.069165229797363 | KNN Loss: 5.031327724456787 | BCE Loss: 1.0378373861312866\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.0547895431518555 | KNN Loss: 5.0314717292785645 | BCE Loss: 1.023317575454712\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.0833210945129395 | KNN Loss: 5.044600963592529 | BCE Loss: 1.0387201309204102\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.096014976501465 | KNN Loss: 5.039870262145996 | BCE Loss: 1.0561447143554688\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.064630508422852 | KNN Loss: 5.039891242980957 | BCE Loss: 1.0247390270233154\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.066168308258057 | KNN Loss: 5.035772323608398 | BCE Loss: 1.0303958654403687\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.0653791427612305 | KNN Loss: 5.026465892791748 | BCE Loss: 1.038913369178772\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.092525482177734 | KNN Loss: 5.041767120361328 | BCE Loss: 1.0507583618164062\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.043100357055664 | KNN Loss: 5.033116340637207 | BCE Loss: 1.009984016418457\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.088990211486816 | KNN Loss: 5.0404181480407715 | BCE Loss: 1.0485718250274658\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.059226989746094 | KNN Loss: 5.028732776641846 | BCE Loss: 1.0304944515228271\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.08498477935791 | KNN Loss: 5.033302307128906 | BCE Loss: 1.051682710647583\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.057516098022461 | KNN Loss: 5.044357776641846 | BCE Loss: 1.0131585597991943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.085519790649414 | KNN Loss: 5.031088352203369 | BCE Loss: 1.0544315576553345\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.05369234085083 | KNN Loss: 5.031317710876465 | BCE Loss: 1.0223746299743652\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.089005470275879 | KNN Loss: 5.030367374420166 | BCE Loss: 1.058638334274292\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.090102672576904 | KNN Loss: 5.054186820983887 | BCE Loss: 1.035915732383728\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.048975944519043 | KNN Loss: 5.028647422790527 | BCE Loss: 1.020328402519226\n",
      "Epoch   439: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.054462432861328 | KNN Loss: 5.035567283630371 | BCE Loss: 1.0188950300216675\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.09264612197876 | KNN Loss: 5.032491207122803 | BCE Loss: 1.0601547956466675\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.048490047454834 | KNN Loss: 5.030425071716309 | BCE Loss: 1.0180649757385254\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.076236724853516 | KNN Loss: 5.02984619140625 | BCE Loss: 1.0463907718658447\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.019503116607666 | KNN Loss: 5.031452655792236 | BCE Loss: 0.9880505204200745\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.071533203125 | KNN Loss: 5.05214262008667 | BCE Loss: 1.01939058303833\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.07513427734375 | KNN Loss: 5.032276153564453 | BCE Loss: 1.0428578853607178\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.079165458679199 | KNN Loss: 5.046693325042725 | BCE Loss: 1.0324723720550537\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.071811199188232 | KNN Loss: 5.038766860961914 | BCE Loss: 1.0330443382263184\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.060438632965088 | KNN Loss: 5.031654357910156 | BCE Loss: 1.0287843942642212\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.058618545532227 | KNN Loss: 5.030510425567627 | BCE Loss: 1.0281083583831787\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.0910868644714355 | KNN Loss: 5.035469055175781 | BCE Loss: 1.0556178092956543\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.095124244689941 | KNN Loss: 5.0355000495910645 | BCE Loss: 1.059624195098877\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.085875511169434 | KNN Loss: 5.0337419509887695 | BCE Loss: 1.052133321762085\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.078646659851074 | KNN Loss: 5.036309719085693 | BCE Loss: 1.0423369407653809\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.114378452301025 | KNN Loss: 5.034116744995117 | BCE Loss: 1.0802617073059082\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.08782434463501 | KNN Loss: 5.03516960144043 | BCE Loss: 1.05265474319458\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.076152801513672 | KNN Loss: 5.036509990692139 | BCE Loss: 1.0396430492401123\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.0602569580078125 | KNN Loss: 5.031120300292969 | BCE Loss: 1.0291366577148438\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.080770969390869 | KNN Loss: 5.03486967086792 | BCE Loss: 1.0459012985229492\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.0484538078308105 | KNN Loss: 5.036857604980469 | BCE Loss: 1.0115962028503418\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.072246074676514 | KNN Loss: 5.036469459533691 | BCE Loss: 1.0357767343521118\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.063443183898926 | KNN Loss: 5.035165309906006 | BCE Loss: 1.02827787399292\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.043641090393066 | KNN Loss: 5.033735752105713 | BCE Loss: 1.009905219078064\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.082342147827148 | KNN Loss: 5.031822681427002 | BCE Loss: 1.0505197048187256\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.06147575378418 | KNN Loss: 5.032171249389648 | BCE Loss: 1.0293045043945312\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.045027732849121 | KNN Loss: 5.033376693725586 | BCE Loss: 1.011650800704956\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.0942840576171875 | KNN Loss: 5.037788391113281 | BCE Loss: 1.0564954280853271\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.069753646850586 | KNN Loss: 5.047036170959473 | BCE Loss: 1.0227175951004028\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.089654445648193 | KNN Loss: 5.0361199378967285 | BCE Loss: 1.0535343885421753\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.097635269165039 | KNN Loss: 5.032653331756592 | BCE Loss: 1.0649819374084473\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.096692085266113 | KNN Loss: 5.0345001220703125 | BCE Loss: 1.0621917247772217\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.061334609985352 | KNN Loss: 5.049124240875244 | BCE Loss: 1.012210488319397\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.067269325256348 | KNN Loss: 5.033369541168213 | BCE Loss: 1.0338997840881348\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.082021236419678 | KNN Loss: 5.042382717132568 | BCE Loss: 1.0396384000778198\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.0800628662109375 | KNN Loss: 5.030390739440918 | BCE Loss: 1.0496718883514404\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.087649822235107 | KNN Loss: 5.041397571563721 | BCE Loss: 1.0462522506713867\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.086761474609375 | KNN Loss: 5.033300876617432 | BCE Loss: 1.0534603595733643\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.079223155975342 | KNN Loss: 5.027832508087158 | BCE Loss: 1.0513906478881836\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.065413951873779 | KNN Loss: 5.026223659515381 | BCE Loss: 1.0391902923583984\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.07729434967041 | KNN Loss: 5.04396390914917 | BCE Loss: 1.0333304405212402\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.045788764953613 | KNN Loss: 5.032070159912109 | BCE Loss: 1.0137187242507935\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.043720722198486 | KNN Loss: 5.025840759277344 | BCE Loss: 1.0178799629211426\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.06931209564209 | KNN Loss: 5.03510856628418 | BCE Loss: 1.0342037677764893\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.063210487365723 | KNN Loss: 5.032104969024658 | BCE Loss: 1.0311052799224854\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.060218334197998 | KNN Loss: 5.035253047943115 | BCE Loss: 1.0249651670455933\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.11151123046875 | KNN Loss: 5.033008575439453 | BCE Loss: 1.078502893447876\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.147122383117676 | KNN Loss: 5.072629451751709 | BCE Loss: 1.074493169784546\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.06076192855835 | KNN Loss: 5.039970874786377 | BCE Loss: 1.0207911729812622\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.06878662109375 | KNN Loss: 5.044275760650635 | BCE Loss: 1.0245109796524048\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.0740227699279785 | KNN Loss: 5.0381975173950195 | BCE Loss: 1.035825252532959\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.071194648742676 | KNN Loss: 5.024796962738037 | BCE Loss: 1.0463979244232178\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.069945812225342 | KNN Loss: 5.0341291427612305 | BCE Loss: 1.0358167886734009\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.05432653427124 | KNN Loss: 5.024738788604736 | BCE Loss: 1.0295878648757935\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.061132431030273 | KNN Loss: 5.0260844230651855 | BCE Loss: 1.0350477695465088\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.069357872009277 | KNN Loss: 5.0387091636657715 | BCE Loss: 1.030648946762085\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.097272872924805 | KNN Loss: 5.053550720214844 | BCE Loss: 1.04372239112854\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.0519561767578125 | KNN Loss: 5.040731906890869 | BCE Loss: 1.0112242698669434\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.103283882141113 | KNN Loss: 5.033397674560547 | BCE Loss: 1.0698864459991455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.085725784301758 | KNN Loss: 5.032927989959717 | BCE Loss: 1.052797555923462\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.07209587097168 | KNN Loss: 5.041560173034668 | BCE Loss: 1.0305359363555908\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.062740802764893 | KNN Loss: 5.03169059753418 | BCE Loss: 1.031050205230713\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.059811592102051 | KNN Loss: 5.02908182144165 | BCE Loss: 1.0307297706604004\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.049190521240234 | KNN Loss: 5.028652191162109 | BCE Loss: 1.020538091659546\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.059130668640137 | KNN Loss: 5.042571067810059 | BCE Loss: 1.016559362411499\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.058109283447266 | KNN Loss: 5.032116413116455 | BCE Loss: 1.0259931087493896\n",
      "Epoch   450: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.057590484619141 | KNN Loss: 5.032917499542236 | BCE Loss: 1.0246728658676147\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.063149452209473 | KNN Loss: 5.030277729034424 | BCE Loss: 1.0328717231750488\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.05826473236084 | KNN Loss: 5.032646179199219 | BCE Loss: 1.0256184339523315\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.047525405883789 | KNN Loss: 5.031087875366211 | BCE Loss: 1.016437292098999\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.109274864196777 | KNN Loss: 5.03097677230835 | BCE Loss: 1.0782982110977173\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.10198450088501 | KNN Loss: 5.0371880531311035 | BCE Loss: 1.0647964477539062\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.035830497741699 | KNN Loss: 5.026755332946777 | BCE Loss: 1.0090750455856323\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.096751689910889 | KNN Loss: 5.045587539672852 | BCE Loss: 1.0511642694473267\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.069661617279053 | KNN Loss: 5.035919189453125 | BCE Loss: 1.0337423086166382\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.085063457489014 | KNN Loss: 5.039038181304932 | BCE Loss: 1.046025276184082\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.092236042022705 | KNN Loss: 5.030021667480469 | BCE Loss: 1.0622143745422363\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.088040828704834 | KNN Loss: 5.050064563751221 | BCE Loss: 1.0379762649536133\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.043025970458984 | KNN Loss: 5.026236534118652 | BCE Loss: 1.016789197921753\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.089328765869141 | KNN Loss: 5.04027795791626 | BCE Loss: 1.0490509271621704\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.055341720581055 | KNN Loss: 5.035121917724609 | BCE Loss: 1.0202195644378662\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.072183132171631 | KNN Loss: 5.033517837524414 | BCE Loss: 1.0386652946472168\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.065356254577637 | KNN Loss: 5.028571605682373 | BCE Loss: 1.0367848873138428\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.12686824798584 | KNN Loss: 5.0594096183776855 | BCE Loss: 1.0674585103988647\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.042843818664551 | KNN Loss: 5.036588191986084 | BCE Loss: 1.006255865097046\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.098188400268555 | KNN Loss: 5.036155700683594 | BCE Loss: 1.06203293800354\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.04219913482666 | KNN Loss: 5.027807712554932 | BCE Loss: 1.0143914222717285\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.103280067443848 | KNN Loss: 5.043556213378906 | BCE Loss: 1.0597238540649414\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.08132266998291 | KNN Loss: 5.04482364654541 | BCE Loss: 1.0364990234375\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.051766872406006 | KNN Loss: 5.0336198806762695 | BCE Loss: 1.0181469917297363\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.071572303771973 | KNN Loss: 5.029980182647705 | BCE Loss: 1.041592001914978\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.069791793823242 | KNN Loss: 5.028464317321777 | BCE Loss: 1.0413272380828857\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.105268478393555 | KNN Loss: 5.0676350593566895 | BCE Loss: 1.0376336574554443\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.059621810913086 | KNN Loss: 5.030350685119629 | BCE Loss: 1.0292713642120361\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.059201240539551 | KNN Loss: 5.030683517456055 | BCE Loss: 1.0285176038742065\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.0733137130737305 | KNN Loss: 5.035918712615967 | BCE Loss: 1.0373947620391846\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.0943474769592285 | KNN Loss: 5.064295768737793 | BCE Loss: 1.0300517082214355\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.028733253479004 | KNN Loss: 5.0270466804504395 | BCE Loss: 1.0016868114471436\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.058687686920166 | KNN Loss: 5.027611255645752 | BCE Loss: 1.031076431274414\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.071633338928223 | KNN Loss: 5.027907848358154 | BCE Loss: 1.0437252521514893\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.065781593322754 | KNN Loss: 5.039331912994385 | BCE Loss: 1.0264495611190796\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.074422836303711 | KNN Loss: 5.030106067657471 | BCE Loss: 1.0443166494369507\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.079162120819092 | KNN Loss: 5.0294013023376465 | BCE Loss: 1.0497608184814453\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.080872535705566 | KNN Loss: 5.041898727416992 | BCE Loss: 1.0389736890792847\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.073363780975342 | KNN Loss: 5.033730506896973 | BCE Loss: 1.0396332740783691\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.090262413024902 | KNN Loss: 5.049461364746094 | BCE Loss: 1.0408012866973877\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.078516006469727 | KNN Loss: 5.030399322509766 | BCE Loss: 1.04811692237854\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.098114013671875 | KNN Loss: 5.034451961517334 | BCE Loss: 1.063661813735962\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.085833549499512 | KNN Loss: 5.0341267585754395 | BCE Loss: 1.0517067909240723\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.059802055358887 | KNN Loss: 5.030324935913086 | BCE Loss: 1.0294773578643799\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.061386585235596 | KNN Loss: 5.027952671051025 | BCE Loss: 1.0334337949752808\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.067507743835449 | KNN Loss: 5.042111396789551 | BCE Loss: 1.0253962278366089\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.052605152130127 | KNN Loss: 5.03531551361084 | BCE Loss: 1.0172897577285767\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.090163707733154 | KNN Loss: 5.034176826477051 | BCE Loss: 1.055987000465393\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.064343452453613 | KNN Loss: 5.031623840332031 | BCE Loss: 1.032719373703003\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.072105407714844 | KNN Loss: 5.032092094421387 | BCE Loss: 1.040013313293457\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.095757007598877 | KNN Loss: 5.032998085021973 | BCE Loss: 1.0627590417861938\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.06721305847168 | KNN Loss: 5.032815933227539 | BCE Loss: 1.0343968868255615\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.0929059982299805 | KNN Loss: 5.029739856719971 | BCE Loss: 1.0631662607192993\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.107858657836914 | KNN Loss: 5.056239604949951 | BCE Loss: 1.051619291305542\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.090816497802734 | KNN Loss: 5.034743785858154 | BCE Loss: 1.056072473526001\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.066039562225342 | KNN Loss: 5.029953956604004 | BCE Loss: 1.036085605621338\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.065032958984375 | KNN Loss: 5.035860061645508 | BCE Loss: 1.0291731357574463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.046849727630615 | KNN Loss: 5.028506278991699 | BCE Loss: 1.0183435678482056\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.051676273345947 | KNN Loss: 5.032701015472412 | BCE Loss: 1.0189752578735352\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.064789295196533 | KNN Loss: 5.031075954437256 | BCE Loss: 1.0337133407592773\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.062212944030762 | KNN Loss: 5.031175136566162 | BCE Loss: 1.0310375690460205\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.075439453125 | KNN Loss: 5.028006553649902 | BCE Loss: 1.0474330186843872\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.065629959106445 | KNN Loss: 5.045800685882568 | BCE Loss: 1.0198293924331665\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.087003707885742 | KNN Loss: 5.031047344207764 | BCE Loss: 1.0559563636779785\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.056163787841797 | KNN Loss: 5.03107213973999 | BCE Loss: 1.0250914096832275\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.068202018737793 | KNN Loss: 5.03293514251709 | BCE Loss: 1.0352671146392822\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.087919235229492 | KNN Loss: 5.050744533538818 | BCE Loss: 1.037174940109253\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.07681941986084 | KNN Loss: 5.032532215118408 | BCE Loss: 1.0442874431610107\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.077465057373047 | KNN Loss: 5.0314860343933105 | BCE Loss: 1.0459790229797363\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.07170295715332 | KNN Loss: 5.029791355133057 | BCE Loss: 1.0419113636016846\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.057622909545898 | KNN Loss: 5.0281662940979 | BCE Loss: 1.029456377029419\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.040040016174316 | KNN Loss: 5.0327019691467285 | BCE Loss: 1.007338285446167\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.058996200561523 | KNN Loss: 5.029520034790039 | BCE Loss: 1.0294761657714844\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.080796241760254 | KNN Loss: 5.034139156341553 | BCE Loss: 1.046656847000122\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.0863447189331055 | KNN Loss: 5.035888195037842 | BCE Loss: 1.0504567623138428\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.0613813400268555 | KNN Loss: 5.031797885894775 | BCE Loss: 1.02958345413208\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.037709712982178 | KNN Loss: 5.030139923095703 | BCE Loss: 1.0075697898864746\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.049041748046875 | KNN Loss: 5.041417598724365 | BCE Loss: 1.0076239109039307\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.073433876037598 | KNN Loss: 5.033142566680908 | BCE Loss: 1.0402913093566895\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.042679786682129 | KNN Loss: 5.026873588562012 | BCE Loss: 1.0158064365386963\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.0785136222839355 | KNN Loss: 5.033138275146484 | BCE Loss: 1.0453753471374512\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.110454082489014 | KNN Loss: 5.048624515533447 | BCE Loss: 1.0618294477462769\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.102572917938232 | KNN Loss: 5.037893772125244 | BCE Loss: 1.0646791458129883\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.055915355682373 | KNN Loss: 5.031838417053223 | BCE Loss: 1.0240768194198608\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.075087547302246 | KNN Loss: 5.031895637512207 | BCE Loss: 1.0431920289993286\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.047198295593262 | KNN Loss: 5.028329372406006 | BCE Loss: 1.0188690423965454\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.092947483062744 | KNN Loss: 5.035672664642334 | BCE Loss: 1.0572749376296997\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.093722820281982 | KNN Loss: 5.0494256019592285 | BCE Loss: 1.044297218322754\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.083973407745361 | KNN Loss: 5.052829265594482 | BCE Loss: 1.0311442613601685\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.035154342651367 | KNN Loss: 5.02805757522583 | BCE Loss: 1.007096767425537\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.080235958099365 | KNN Loss: 5.068662166595459 | BCE Loss: 1.0115739107131958\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.043718338012695 | KNN Loss: 5.037508487701416 | BCE Loss: 1.0062099695205688\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.110344886779785 | KNN Loss: 5.056272983551025 | BCE Loss: 1.0540717840194702\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.06793212890625 | KNN Loss: 5.040335655212402 | BCE Loss: 1.0275967121124268\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.07783317565918 | KNN Loss: 5.041066646575928 | BCE Loss: 1.036766767501831\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.106184005737305 | KNN Loss: 5.040693759918213 | BCE Loss: 1.065490484237671\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.0920515060424805 | KNN Loss: 5.030555248260498 | BCE Loss: 1.0614961385726929\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.068561553955078 | KNN Loss: 5.0435285568237305 | BCE Loss: 1.0250327587127686\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.0526556968688965 | KNN Loss: 5.026307106018066 | BCE Loss: 1.0263484716415405\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.0574049949646 | KNN Loss: 5.029887676239014 | BCE Loss: 1.027517318725586\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.094815731048584 | KNN Loss: 5.0363664627075195 | BCE Loss: 1.058449149131775\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.068061828613281 | KNN Loss: 5.037335395812988 | BCE Loss: 1.0307261943817139\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.0594072341918945 | KNN Loss: 5.025842189788818 | BCE Loss: 1.0335652828216553\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.0527496337890625 | KNN Loss: 5.026663780212402 | BCE Loss: 1.0260858535766602\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.127513885498047 | KNN Loss: 5.062049388885498 | BCE Loss: 1.0654644966125488\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.036035537719727 | KNN Loss: 5.031414031982422 | BCE Loss: 1.0046217441558838\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.108905792236328 | KNN Loss: 5.038175582885742 | BCE Loss: 1.070730209350586\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.067514896392822 | KNN Loss: 5.039567947387695 | BCE Loss: 1.027946949005127\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.059913158416748 | KNN Loss: 5.02799129486084 | BCE Loss: 1.0319218635559082\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.0681562423706055 | KNN Loss: 5.04390811920166 | BCE Loss: 1.0242478847503662\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.060184955596924 | KNN Loss: 5.045531272888184 | BCE Loss: 1.0146536827087402\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.045865058898926 | KNN Loss: 5.029147624969482 | BCE Loss: 1.0167176723480225\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.0785441398620605 | KNN Loss: 5.027536392211914 | BCE Loss: 1.0510077476501465\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.036704063415527 | KNN Loss: 5.036251068115234 | BCE Loss: 1.000452995300293\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.072615623474121 | KNN Loss: 5.039715766906738 | BCE Loss: 1.0328996181488037\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.067531585693359 | KNN Loss: 5.039736747741699 | BCE Loss: 1.0277950763702393\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.100302219390869 | KNN Loss: 5.0411553382873535 | BCE Loss: 1.059146761894226\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.0670599937438965 | KNN Loss: 5.0320210456848145 | BCE Loss: 1.0350390672683716\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.0910844802856445 | KNN Loss: 5.055720806121826 | BCE Loss: 1.0353634357452393\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.080104827880859 | KNN Loss: 5.048454284667969 | BCE Loss: 1.0316505432128906\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.038812160491943 | KNN Loss: 5.027898788452148 | BCE Loss: 1.010913372039795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.060054779052734 | KNN Loss: 5.034311771392822 | BCE Loss: 1.0257431268692017\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.076012134552002 | KNN Loss: 5.031058311462402 | BCE Loss: 1.0449538230895996\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.061872482299805 | KNN Loss: 5.039194107055664 | BCE Loss: 1.0226786136627197\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.129312992095947 | KNN Loss: 5.029121398925781 | BCE Loss: 1.1001917123794556\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.044210433959961 | KNN Loss: 5.0272393226623535 | BCE Loss: 1.0169711112976074\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.068950176239014 | KNN Loss: 5.03914213180542 | BCE Loss: 1.0298079252243042\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.090576171875 | KNN Loss: 5.035552501678467 | BCE Loss: 1.0550237894058228\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.102415084838867 | KNN Loss: 5.051633358001709 | BCE Loss: 1.0507817268371582\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.062337875366211 | KNN Loss: 5.02780818939209 | BCE Loss: 1.0345295667648315\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.097358703613281 | KNN Loss: 5.038665771484375 | BCE Loss: 1.0586928129196167\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.050105094909668 | KNN Loss: 5.040704250335693 | BCE Loss: 1.0094010829925537\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.062094211578369 | KNN Loss: 5.026404857635498 | BCE Loss: 1.0356894731521606\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.0679097175598145 | KNN Loss: 5.029908657073975 | BCE Loss: 1.0380011796951294\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.071641445159912 | KNN Loss: 5.0479631423950195 | BCE Loss: 1.0236783027648926\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.08505916595459 | KNN Loss: 5.026532173156738 | BCE Loss: 1.0585269927978516\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.087854385375977 | KNN Loss: 5.0404133796691895 | BCE Loss: 1.0474408864974976\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.0627288818359375 | KNN Loss: 5.040196895599365 | BCE Loss: 1.0225317478179932\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.027713775634766 | KNN Loss: 5.03217887878418 | BCE Loss: 0.995535135269165\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.079386234283447 | KNN Loss: 5.028006076812744 | BCE Loss: 1.0513800382614136\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.0497026443481445 | KNN Loss: 5.031050682067871 | BCE Loss: 1.0186522006988525\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.06483793258667 | KNN Loss: 5.031990051269531 | BCE Loss: 1.0328480005264282\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.079079627990723 | KNN Loss: 5.036398410797119 | BCE Loss: 1.042681097984314\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.081226825714111 | KNN Loss: 5.06184196472168 | BCE Loss: 1.019384741783142\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.051377296447754 | KNN Loss: 5.027273178100586 | BCE Loss: 1.024104118347168\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.077803611755371 | KNN Loss: 5.026222229003906 | BCE Loss: 1.0515815019607544\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.075021743774414 | KNN Loss: 5.039890289306641 | BCE Loss: 1.0351316928863525\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.049285411834717 | KNN Loss: 5.026243209838867 | BCE Loss: 1.02304208278656\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.067422389984131 | KNN Loss: 5.030444145202637 | BCE Loss: 1.0369782447814941\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.0920515060424805 | KNN Loss: 5.034152984619141 | BCE Loss: 1.0578985214233398\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.1009063720703125 | KNN Loss: 5.036063194274902 | BCE Loss: 1.0648431777954102\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.059688568115234 | KNN Loss: 5.024748802185059 | BCE Loss: 1.0349396467208862\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.078315258026123 | KNN Loss: 5.030457019805908 | BCE Loss: 1.0478582382202148\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.072338581085205 | KNN Loss: 5.03849458694458 | BCE Loss: 1.0338438749313354\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.0486555099487305 | KNN Loss: 5.034471035003662 | BCE Loss: 1.0141843557357788\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.034550666809082 | KNN Loss: 5.032909870147705 | BCE Loss: 1.0016406774520874\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.084873199462891 | KNN Loss: 5.028256416320801 | BCE Loss: 1.0566167831420898\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.074440002441406 | KNN Loss: 5.028903484344482 | BCE Loss: 1.0455365180969238\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.093217372894287 | KNN Loss: 5.039303779602051 | BCE Loss: 1.0539135932922363\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.0761213302612305 | KNN Loss: 5.041308403015137 | BCE Loss: 1.0348126888275146\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.064854145050049 | KNN Loss: 5.033960342407227 | BCE Loss: 1.0308938026428223\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.0778961181640625 | KNN Loss: 5.034965991973877 | BCE Loss: 1.0429303646087646\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.069438457489014 | KNN Loss: 5.027012825012207 | BCE Loss: 1.0424256324768066\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.102027893066406 | KNN Loss: 5.0387043952941895 | BCE Loss: 1.0633232593536377\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.059902667999268 | KNN Loss: 5.036779403686523 | BCE Loss: 1.0231232643127441\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.053618907928467 | KNN Loss: 5.027741432189941 | BCE Loss: 1.0258773565292358\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.101263046264648 | KNN Loss: 5.042135715484619 | BCE Loss: 1.0591274499893188\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.0999226570129395 | KNN Loss: 5.045051574707031 | BCE Loss: 1.0548712015151978\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.030276298522949 | KNN Loss: 5.0243401527404785 | BCE Loss: 1.0059363842010498\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.083981513977051 | KNN Loss: 5.04093599319458 | BCE Loss: 1.0430452823638916\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.085883617401123 | KNN Loss: 5.033483505249023 | BCE Loss: 1.0524002313613892\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.039891242980957 | KNN Loss: 5.0260910987854 | BCE Loss: 1.0138002634048462\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.083739280700684 | KNN Loss: 5.034362316131592 | BCE Loss: 1.049377202987671\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.065141201019287 | KNN Loss: 5.039083480834961 | BCE Loss: 1.0260577201843262\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.078880310058594 | KNN Loss: 5.033665657043457 | BCE Loss: 1.0452148914337158\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.048589706420898 | KNN Loss: 5.029611587524414 | BCE Loss: 1.0189781188964844\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.048612117767334 | KNN Loss: 5.028112888336182 | BCE Loss: 1.020499348640442\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.0813727378845215 | KNN Loss: 5.042373180389404 | BCE Loss: 1.0389996767044067\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.069849967956543 | KNN Loss: 5.030948638916016 | BCE Loss: 1.0389013290405273\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.055667877197266 | KNN Loss: 5.036036968231201 | BCE Loss: 1.0196311473846436\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.082270622253418 | KNN Loss: 5.041195869445801 | BCE Loss: 1.0410749912261963\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.096639633178711 | KNN Loss: 5.049232482910156 | BCE Loss: 1.0474073886871338\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.054078102111816 | KNN Loss: 5.026578903198242 | BCE Loss: 1.0274993181228638\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.083059310913086 | KNN Loss: 5.039853572845459 | BCE Loss: 1.043205976486206\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.0772504806518555 | KNN Loss: 5.030956745147705 | BCE Loss: 1.0462939739227295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.085815906524658 | KNN Loss: 5.036717414855957 | BCE Loss: 1.0490984916687012\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.078423500061035 | KNN Loss: 5.040165901184082 | BCE Loss: 1.0382577180862427\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.091352462768555 | KNN Loss: 5.043668746948242 | BCE Loss: 1.0476837158203125\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.054566860198975 | KNN Loss: 5.036487579345703 | BCE Loss: 1.0180792808532715\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.045516490936279 | KNN Loss: 5.042888641357422 | BCE Loss: 1.0026277303695679\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.068411827087402 | KNN Loss: 5.026695251464844 | BCE Loss: 1.041716456413269\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.059846878051758 | KNN Loss: 5.0303053855896 | BCE Loss: 1.029541254043579\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.07157564163208 | KNN Loss: 5.039799690246582 | BCE Loss: 1.031775951385498\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.047800064086914 | KNN Loss: 5.029890537261963 | BCE Loss: 1.0179094076156616\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.090646743774414 | KNN Loss: 5.03125 | BCE Loss: 1.059396743774414\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.092106819152832 | KNN Loss: 5.038747787475586 | BCE Loss: 1.053358793258667\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.067249774932861 | KNN Loss: 5.030989170074463 | BCE Loss: 1.0362606048583984\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.071197032928467 | KNN Loss: 5.026876449584961 | BCE Loss: 1.0443207025527954\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.056867599487305 | KNN Loss: 5.042632102966309 | BCE Loss: 1.014235258102417\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.07859992980957 | KNN Loss: 5.035361289978027 | BCE Loss: 1.043238639831543\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.079986095428467 | KNN Loss: 5.037721633911133 | BCE Loss: 1.0422645807266235\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.080596446990967 | KNN Loss: 5.048252582550049 | BCE Loss: 1.032343864440918\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.076240539550781 | KNN Loss: 5.041830062866211 | BCE Loss: 1.0344107151031494\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.0640411376953125 | KNN Loss: 5.0353593826293945 | BCE Loss: 1.0286815166473389\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.077546119689941 | KNN Loss: 5.029659748077393 | BCE Loss: 1.047886610031128\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.0605998039245605 | KNN Loss: 5.043203353881836 | BCE Loss: 1.0173965692520142\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.055342674255371 | KNN Loss: 5.029873847961426 | BCE Loss: 1.0254687070846558\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.08534574508667 | KNN Loss: 5.0349884033203125 | BCE Loss: 1.0503573417663574\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.050410270690918 | KNN Loss: 5.034930229187012 | BCE Loss: 1.0154798030853271\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.095678806304932 | KNN Loss: 5.026033878326416 | BCE Loss: 1.0696449279785156\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.085243225097656 | KNN Loss: 5.04376220703125 | BCE Loss: 1.0414810180664062\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.084702491760254 | KNN Loss: 5.033311367034912 | BCE Loss: 1.0513912439346313\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.07298469543457 | KNN Loss: 5.0320048332214355 | BCE Loss: 1.0409801006317139\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.071493625640869 | KNN Loss: 5.03806209564209 | BCE Loss: 1.0334316492080688\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.09260892868042 | KNN Loss: 5.037661075592041 | BCE Loss: 1.054947853088379\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.061829090118408 | KNN Loss: 5.027310371398926 | BCE Loss: 1.0345187187194824\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.062656402587891 | KNN Loss: 5.0336737632751465 | BCE Loss: 1.0289828777313232\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.059056282043457 | KNN Loss: 5.031293869018555 | BCE Loss: 1.0277624130249023\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.084382057189941 | KNN Loss: 5.040953159332275 | BCE Loss: 1.043428897857666\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.047067165374756 | KNN Loss: 5.030386924743652 | BCE Loss: 1.0166802406311035\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.070518493652344 | KNN Loss: 5.030231952667236 | BCE Loss: 1.0402865409851074\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.0823283195495605 | KNN Loss: 5.040207386016846 | BCE Loss: 1.0421209335327148\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.09060525894165 | KNN Loss: 5.039546966552734 | BCE Loss: 1.051058292388916\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.063684940338135 | KNN Loss: 5.0263671875 | BCE Loss: 1.0373177528381348\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.046561241149902 | KNN Loss: 5.0247650146484375 | BCE Loss: 1.0217959880828857\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.078281402587891 | KNN Loss: 5.030610084533691 | BCE Loss: 1.0476713180541992\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.063179969787598 | KNN Loss: 5.038793087005615 | BCE Loss: 1.0243866443634033\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.083900451660156 | KNN Loss: 5.031987190246582 | BCE Loss: 1.0519132614135742\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.062834739685059 | KNN Loss: 5.0289788246154785 | BCE Loss: 1.03385591506958\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.0814433097839355 | KNN Loss: 5.045257091522217 | BCE Loss: 1.0361862182617188\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.06628942489624 | KNN Loss: 5.041310787200928 | BCE Loss: 1.024978518486023\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.096044540405273 | KNN Loss: 5.0306396484375 | BCE Loss: 1.0654046535491943\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.073171615600586 | KNN Loss: 5.049525737762451 | BCE Loss: 1.0236456394195557\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.061623573303223 | KNN Loss: 5.030782699584961 | BCE Loss: 1.0308411121368408\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.084568500518799 | KNN Loss: 5.036363124847412 | BCE Loss: 1.0482054948806763\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.07765531539917 | KNN Loss: 5.028207302093506 | BCE Loss: 1.049448013305664\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.12230920791626 | KNN Loss: 5.066230773925781 | BCE Loss: 1.056078553199768\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.046847343444824 | KNN Loss: 5.029555320739746 | BCE Loss: 1.017291784286499\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.070857048034668 | KNN Loss: 5.041408061981201 | BCE Loss: 1.0294488668441772\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.057409286499023 | KNN Loss: 5.028561592102051 | BCE Loss: 1.0288474559783936\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.052180290222168 | KNN Loss: 5.034741401672363 | BCE Loss: 1.0174390077590942\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.067038536071777 | KNN Loss: 5.039429187774658 | BCE Loss: 1.0276095867156982\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.086751461029053 | KNN Loss: 5.042622089385986 | BCE Loss: 1.044129490852356\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.092602252960205 | KNN Loss: 5.032205581665039 | BCE Loss: 1.0603967905044556\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.06453800201416 | KNN Loss: 5.043039321899414 | BCE Loss: 1.021498441696167\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.09312629699707 | KNN Loss: 5.036325931549072 | BCE Loss: 1.056800365447998\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.064550876617432 | KNN Loss: 5.030797481536865 | BCE Loss: 1.0337533950805664\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.099920272827148 | KNN Loss: 5.027740478515625 | BCE Loss: 1.0721795558929443\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.096434593200684 | KNN Loss: 5.040188789367676 | BCE Loss: 1.0562456846237183\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.087594032287598 | KNN Loss: 5.041879653930664 | BCE Loss: 1.0457141399383545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.06508731842041 | KNN Loss: 5.025021076202393 | BCE Loss: 1.0400664806365967\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.080235004425049 | KNN Loss: 5.030549049377441 | BCE Loss: 1.0496859550476074\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.075401782989502 | KNN Loss: 5.027512550354004 | BCE Loss: 1.0478891134262085\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.110662460327148 | KNN Loss: 5.062050819396973 | BCE Loss: 1.0486117601394653\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.048522472381592 | KNN Loss: 5.0362067222595215 | BCE Loss: 1.0123156309127808\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.075016021728516 | KNN Loss: 5.038321495056152 | BCE Loss: 1.0366945266723633\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.092489719390869 | KNN Loss: 5.0410990715026855 | BCE Loss: 1.051390528678894\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.068001747131348 | KNN Loss: 5.043178558349609 | BCE Loss: 1.0248231887817383\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.09587287902832 | KNN Loss: 5.038086414337158 | BCE Loss: 1.057786464691162\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.09381103515625 | KNN Loss: 5.04371452331543 | BCE Loss: 1.0500967502593994\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.046186447143555 | KNN Loss: 5.028409481048584 | BCE Loss: 1.0177772045135498\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.115622043609619 | KNN Loss: 5.047183036804199 | BCE Loss: 1.0684388875961304\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.069849014282227 | KNN Loss: 5.035116195678711 | BCE Loss: 1.0347325801849365\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.044600009918213 | KNN Loss: 5.032848358154297 | BCE Loss: 1.0117515325546265\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.086220741271973 | KNN Loss: 5.040858268737793 | BCE Loss: 1.0453627109527588\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.088672161102295 | KNN Loss: 5.0404887199401855 | BCE Loss: 1.0481834411621094\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.0911865234375 | KNN Loss: 5.0355143547058105 | BCE Loss: 1.0556724071502686\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.056669235229492 | KNN Loss: 5.037694454193115 | BCE Loss: 1.018974781036377\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.081176280975342 | KNN Loss: 5.033265590667725 | BCE Loss: 1.0479108095169067\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.041866302490234 | KNN Loss: 5.032111644744873 | BCE Loss: 1.0097546577453613\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.078342437744141 | KNN Loss: 5.02982234954834 | BCE Loss: 1.0485198497772217\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.066396713256836 | KNN Loss: 5.04256534576416 | BCE Loss: 1.0238316059112549\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.06920862197876 | KNN Loss: 5.035192489624023 | BCE Loss: 1.0340161323547363\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.052631378173828 | KNN Loss: 5.030174255371094 | BCE Loss: 1.0224570035934448\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.100699424743652 | KNN Loss: 5.029446125030518 | BCE Loss: 1.0712530612945557\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.094593524932861 | KNN Loss: 5.040580749511719 | BCE Loss: 1.0540127754211426\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.075509071350098 | KNN Loss: 5.043505668640137 | BCE Loss: 1.0320031642913818\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.063824653625488 | KNN Loss: 5.027868270874023 | BCE Loss: 1.0359561443328857\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.031525135040283 | KNN Loss: 5.038113594055176 | BCE Loss: 0.993411660194397\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.067948818206787 | KNN Loss: 5.03770637512207 | BCE Loss: 1.0302425622940063\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.05979061126709 | KNN Loss: 5.03866720199585 | BCE Loss: 1.0211231708526611\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.133398056030273 | KNN Loss: 5.045085906982422 | BCE Loss: 1.0883123874664307\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.033206939697266 | KNN Loss: 5.027959823608398 | BCE Loss: 1.0052473545074463\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.08327054977417 | KNN Loss: 5.028059005737305 | BCE Loss: 1.0552115440368652\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.09147310256958 | KNN Loss: 5.029631614685059 | BCE Loss: 1.061841368675232\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.074036598205566 | KNN Loss: 5.043557643890381 | BCE Loss: 1.0304787158966064\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.048087120056152 | KNN Loss: 5.029077053070068 | BCE Loss: 1.019010066986084\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.074935436248779 | KNN Loss: 5.030810832977295 | BCE Loss: 1.0441246032714844\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.06431770324707 | KNN Loss: 5.027060031890869 | BCE Loss: 1.0372576713562012\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.0458784103393555 | KNN Loss: 5.029053211212158 | BCE Loss: 1.0168254375457764\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.070915222167969 | KNN Loss: 5.035588264465332 | BCE Loss: 1.0353271961212158\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.057528018951416 | KNN Loss: 5.0292582511901855 | BCE Loss: 1.028269648551941\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.065256118774414 | KNN Loss: 5.0305633544921875 | BCE Loss: 1.034692645072937\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.076254367828369 | KNN Loss: 5.031097888946533 | BCE Loss: 1.0451565980911255\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.09277868270874 | KNN Loss: 5.0296854972839355 | BCE Loss: 1.0630931854248047\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.090847969055176 | KNN Loss: 5.040764808654785 | BCE Loss: 1.0500829219818115\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.077775478363037 | KNN Loss: 5.040668487548828 | BCE Loss: 1.0371071100234985\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.038415908813477 | KNN Loss: 5.024197101593018 | BCE Loss: 1.014219045639038\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.067468643188477 | KNN Loss: 5.035168647766113 | BCE Loss: 1.0323002338409424\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.084620952606201 | KNN Loss: 5.038357257843018 | BCE Loss: 1.0462636947631836\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1004,  2.6817,  2.7539,  2.4920,  3.6779,  0.7150,  2.7538,  2.4548,\n",
      "          2.5670,  1.9031,  2.4237,  2.1422,  0.9989,  1.9468,  1.3392,  1.4412,\n",
      "          1.9644,  2.2851,  2.9922,  2.3174,  1.7563,  3.0557,  2.2364,  1.6399,\n",
      "          2.4427,  1.5676,  2.3108,  1.6814,  1.3940,  0.0944, -0.1598,  1.1897,\n",
      "          0.2207,  1.0250,  1.5466,  1.4039,  0.7653,  3.5941,  0.6789,  1.5340,\n",
      "          0.9580, -0.7711, -0.3380,  2.2785,  2.4960,  0.7685, -0.0435, -0.1210,\n",
      "          1.4144,  2.7551,  1.6707,  0.1121,  1.4718,  0.4221, -0.7428,  1.1006,\n",
      "          1.6403,  1.3968,  1.3351,  1.9359,  0.2931,  0.7008,  0.2174,  1.9189,\n",
      "          1.3878,  1.7041, -1.8440,  0.3478,  2.1132,  1.7647,  2.5614,  0.4807,\n",
      "          1.2871,  2.6378,  1.7086,  1.1656,  0.2173,  0.7676,  0.3412,  1.7553,\n",
      "          0.0473,  0.2614,  2.0176, -0.3224,  0.4090, -1.1499, -2.7509, -0.2275,\n",
      "          0.5427, -1.7010,  0.4813, -0.1728, -0.4958, -0.8848,  0.5925,  1.2779,\n",
      "         -0.5448, -0.9115,  0.4370,  1.1065,  0.7071, -1.3965,  0.7815,  1.2099,\n",
      "         -1.2937, -1.0868, -0.0144,  0.1098, -1.3180, -1.6196, -0.6504, -2.9363,\n",
      "         -0.4441,  1.8242,  1.5683, -0.3317, -0.5052, -0.1089,  1.4927, -2.4525,\n",
      "          0.3340, -0.3534,  0.5168, -0.5589,  0.0208, -0.8156, -1.1558,  1.0101,\n",
      "          0.4563, -0.4880,  0.4018, -0.6085, -1.1791, -0.4175, -0.5380,  0.8489,\n",
      "         -0.4792,  0.1747, -2.0392, -0.8789, -1.5905,  0.6933, -1.9223, -1.0321,\n",
      "         -1.1479, -0.5540, -1.5233, -1.1216, -2.4225, -1.0920, -1.3524, -0.5588,\n",
      "         -1.6607,  0.3044, -1.7330, -0.6460, -3.2812, -0.0227, -0.1973, -0.6715,\n",
      "         -2.1262, -1.6278, -1.1267, -1.6184, -2.7102, -2.4150, -2.9683]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.6779, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf44a447b784affb4c80152eff1394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 84.87it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3d16cf9b51481e86d88a7365be2cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561de6d9e0804741bf6f59eb5b05dab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1623b3f4aa648d785f3ed2f6d0d0fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.606 | Reg loss: 0.012 | Tree loss: 9.606 | Accuracy: 0.000000 | 1.466 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.597 | Reg loss: 0.011 | Tree loss: 9.597 | Accuracy: 0.000000 | 1.161 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.588 | Reg loss: 0.010 | Tree loss: 9.588 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.579 | Reg loss: 0.010 | Tree loss: 9.579 | Accuracy: 0.000000 | 1.04 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.572 | Reg loss: 0.009 | Tree loss: 9.572 | Accuracy: 0.000000 | 1.011 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.562 | Reg loss: 0.009 | Tree loss: 9.562 | Accuracy: 0.003906 | 0.989 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.552 | Reg loss: 0.009 | Tree loss: 9.552 | Accuracy: 0.005859 | 0.975 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.545 | Reg loss: 0.008 | Tree loss: 9.545 | Accuracy: 0.015625 | 0.966 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.538 | Reg loss: 0.008 | Tree loss: 9.538 | Accuracy: 0.054688 | 0.959 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.123047 | 0.954 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.523 | Reg loss: 0.008 | Tree loss: 9.523 | Accuracy: 0.222656 | 0.949 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.514 | Reg loss: 0.008 | Tree loss: 9.514 | Accuracy: 0.308594 | 0.946 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.503 | Reg loss: 0.008 | Tree loss: 9.503 | Accuracy: 0.322266 | 0.943 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.492 | Reg loss: 0.009 | Tree loss: 9.492 | Accuracy: 0.353516 | 0.94 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.492 | Reg loss: 0.009 | Tree loss: 9.492 | Accuracy: 0.318359 | 0.938 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.486 | Reg loss: 0.009 | Tree loss: 9.486 | Accuracy: 0.333984 | 0.936 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.474 | Reg loss: 0.009 | Tree loss: 9.474 | Accuracy: 0.333984 | 0.934 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.465 | Reg loss: 0.010 | Tree loss: 9.465 | Accuracy: 0.380859 | 0.933 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.460 | Reg loss: 0.010 | Tree loss: 9.460 | Accuracy: 0.343750 | 0.931 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.447 | Reg loss: 0.010 | Tree loss: 9.447 | Accuracy: 0.382812 | 0.93 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.449 | Reg loss: 0.011 | Tree loss: 9.449 | Accuracy: 0.375000 | 0.929 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.435 | Reg loss: 0.011 | Tree loss: 9.435 | Accuracy: 0.367188 | 0.928 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.428 | Reg loss: 0.011 | Tree loss: 9.428 | Accuracy: 0.384766 | 0.927 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.416 | Reg loss: 0.012 | Tree loss: 9.416 | Accuracy: 0.363281 | 0.926 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.415 | Reg loss: 0.012 | Tree loss: 9.415 | Accuracy: 0.339844 | 0.926 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.407 | Reg loss: 0.012 | Tree loss: 9.407 | Accuracy: 0.345703 | 0.925 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.399 | Reg loss: 0.013 | Tree loss: 9.399 | Accuracy: 0.369141 | 0.924 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.386 | Reg loss: 0.013 | Tree loss: 9.386 | Accuracy: 0.373047 | 0.924 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.397 | Reg loss: 0.013 | Tree loss: 9.397 | Accuracy: 0.350168 | 0.923 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.482 | Reg loss: 0.005 | Tree loss: 9.482 | Accuracy: 0.363281 | 0.932 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.474 | Reg loss: 0.005 | Tree loss: 9.474 | Accuracy: 0.357422 | 0.929 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.470 | Reg loss: 0.006 | Tree loss: 9.470 | Accuracy: 0.355469 | 0.927 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.460 | Reg loss: 0.006 | Tree loss: 9.460 | Accuracy: 0.357422 | 0.926 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.448 | Reg loss: 0.006 | Tree loss: 9.448 | Accuracy: 0.380859 | 0.924 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.447 | Reg loss: 0.006 | Tree loss: 9.447 | Accuracy: 0.326172 | 0.924 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.436 | Reg loss: 0.007 | Tree loss: 9.436 | Accuracy: 0.343750 | 0.923 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.428 | Reg loss: 0.007 | Tree loss: 9.428 | Accuracy: 0.373047 | 0.922 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.422 | Reg loss: 0.007 | Tree loss: 9.422 | Accuracy: 0.322266 | 0.922 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.413 | Reg loss: 0.008 | Tree loss: 9.413 | Accuracy: 0.367188 | 0.921 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.404 | Reg loss: 0.008 | Tree loss: 9.404 | Accuracy: 0.318359 | 0.921 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.395 | Reg loss: 0.009 | Tree loss: 9.395 | Accuracy: 0.347656 | 0.92 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.381 | Reg loss: 0.009 | Tree loss: 9.381 | Accuracy: 0.417969 | 0.92 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.378 | Reg loss: 0.010 | Tree loss: 9.378 | Accuracy: 0.386719 | 0.92 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.359 | Reg loss: 0.010 | Tree loss: 9.359 | Accuracy: 0.388672 | 0.919 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.355 | Reg loss: 0.011 | Tree loss: 9.355 | Accuracy: 0.378906 | 0.919 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.352 | Reg loss: 0.011 | Tree loss: 9.352 | Accuracy: 0.371094 | 0.919 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.346 | Reg loss: 0.011 | Tree loss: 9.346 | Accuracy: 0.330078 | 0.918 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.335 | Reg loss: 0.012 | Tree loss: 9.335 | Accuracy: 0.380859 | 0.918 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.322 | Reg loss: 0.012 | Tree loss: 9.322 | Accuracy: 0.394531 | 0.918 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.326 | Reg loss: 0.013 | Tree loss: 9.326 | Accuracy: 0.361328 | 0.918 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.319 | Reg loss: 0.013 | Tree loss: 9.319 | Accuracy: 0.359375 | 0.917 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.312 | Reg loss: 0.014 | Tree loss: 9.312 | Accuracy: 0.343750 | 0.917 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.305 | Reg loss: 0.014 | Tree loss: 9.305 | Accuracy: 0.361328 | 0.917 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.300 | Reg loss: 0.014 | Tree loss: 9.300 | Accuracy: 0.330078 | 0.917 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.285 | Reg loss: 0.015 | Tree loss: 9.285 | Accuracy: 0.324219 | 0.917 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.285 | Reg loss: 0.015 | Tree loss: 9.285 | Accuracy: 0.351562 | 0.916 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.281 | Reg loss: 0.016 | Tree loss: 9.281 | Accuracy: 0.298828 | 0.916 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.264 | Reg loss: 0.016 | Tree loss: 9.264 | Accuracy: 0.383838 | 0.916 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.379 | Reg loss: 0.008 | Tree loss: 9.379 | Accuracy: 0.353516 | 0.921 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.361 | Reg loss: 0.008 | Tree loss: 9.361 | Accuracy: 0.394531 | 0.92 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.358 | Reg loss: 0.009 | Tree loss: 9.358 | Accuracy: 0.361328 | 0.919 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.348 | Reg loss: 0.009 | Tree loss: 9.348 | Accuracy: 0.390625 | 0.918 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.345 | Reg loss: 0.009 | Tree loss: 9.345 | Accuracy: 0.332031 | 0.917 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.330 | Reg loss: 0.009 | Tree loss: 9.330 | Accuracy: 0.388672 | 0.917 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.338 | Reg loss: 0.010 | Tree loss: 9.338 | Accuracy: 0.285156 | 0.916 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.321 | Reg loss: 0.010 | Tree loss: 9.321 | Accuracy: 0.376953 | 0.916 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.313 | Reg loss: 0.010 | Tree loss: 9.313 | Accuracy: 0.363281 | 0.916 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.297 | Reg loss: 0.011 | Tree loss: 9.297 | Accuracy: 0.367188 | 0.916 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.291 | Reg loss: 0.011 | Tree loss: 9.291 | Accuracy: 0.312500 | 0.916 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.281 | Reg loss: 0.012 | Tree loss: 9.281 | Accuracy: 0.332031 | 0.915 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.273 | Reg loss: 0.012 | Tree loss: 9.273 | Accuracy: 0.378906 | 0.915 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.271 | Reg loss: 0.012 | Tree loss: 9.271 | Accuracy: 0.337891 | 0.915 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.253 | Reg loss: 0.013 | Tree loss: 9.253 | Accuracy: 0.365234 | 0.915 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.243 | Reg loss: 0.013 | Tree loss: 9.243 | Accuracy: 0.349609 | 0.915 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.233 | Reg loss: 0.014 | Tree loss: 9.233 | Accuracy: 0.347656 | 0.914 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.231 | Reg loss: 0.014 | Tree loss: 9.231 | Accuracy: 0.355469 | 0.914 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.222 | Reg loss: 0.015 | Tree loss: 9.222 | Accuracy: 0.382812 | 0.914 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.203 | Reg loss: 0.015 | Tree loss: 9.203 | Accuracy: 0.375000 | 0.914 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.195 | Reg loss: 0.016 | Tree loss: 9.195 | Accuracy: 0.365234 | 0.914 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 9.197 | Reg loss: 0.016 | Tree loss: 9.197 | Accuracy: 0.347656 | 0.913 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 9.193 | Reg loss: 0.017 | Tree loss: 9.193 | Accuracy: 0.363281 | 0.913 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 9.176 | Reg loss: 0.017 | Tree loss: 9.176 | Accuracy: 0.371094 | 0.913 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 9.156 | Reg loss: 0.018 | Tree loss: 9.156 | Accuracy: 0.337891 | 0.913 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 9.157 | Reg loss: 0.018 | Tree loss: 9.157 | Accuracy: 0.314453 | 0.913 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 9.146 | Reg loss: 0.018 | Tree loss: 9.146 | Accuracy: 0.382812 | 0.912 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 9.129 | Reg loss: 0.019 | Tree loss: 9.129 | Accuracy: 0.359375 | 0.912 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 9.116 | Reg loss: 0.019 | Tree loss: 9.116 | Accuracy: 0.383838 | 0.912 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.265 | Reg loss: 0.011 | Tree loss: 9.265 | Accuracy: 0.337891 | 0.915 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.253 | Reg loss: 0.012 | Tree loss: 9.253 | Accuracy: 0.363281 | 0.914 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.256 | Reg loss: 0.012 | Tree loss: 9.256 | Accuracy: 0.341797 | 0.913 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.228 | Reg loss: 0.012 | Tree loss: 9.228 | Accuracy: 0.341797 | 0.912 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.233 | Reg loss: 0.012 | Tree loss: 9.233 | Accuracy: 0.359375 | 0.911 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.213 | Reg loss: 0.012 | Tree loss: 9.213 | Accuracy: 0.378906 | 0.91 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 9.213 | Reg loss: 0.013 | Tree loss: 9.213 | Accuracy: 0.322266 | 0.909 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.195 | Reg loss: 0.013 | Tree loss: 9.195 | Accuracy: 0.349609 | 0.908 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 9.174 | Reg loss: 0.013 | Tree loss: 9.174 | Accuracy: 0.371094 | 0.908 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 9.177 | Reg loss: 0.014 | Tree loss: 9.177 | Accuracy: 0.349609 | 0.907 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 9.160 | Reg loss: 0.014 | Tree loss: 9.160 | Accuracy: 0.394531 | 0.907 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 9.158 | Reg loss: 0.015 | Tree loss: 9.158 | Accuracy: 0.361328 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 9.141 | Reg loss: 0.015 | Tree loss: 9.141 | Accuracy: 0.351562 | 0.906 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 9.128 | Reg loss: 0.016 | Tree loss: 9.128 | Accuracy: 0.359375 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 9.121 | Reg loss: 0.016 | Tree loss: 9.121 | Accuracy: 0.300781 | 0.905 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 9.120 | Reg loss: 0.017 | Tree loss: 9.120 | Accuracy: 0.332031 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 9.096 | Reg loss: 0.017 | Tree loss: 9.096 | Accuracy: 0.359375 | 0.904 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 9.071 | Reg loss: 0.018 | Tree loss: 9.071 | Accuracy: 0.390625 | 0.903 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 9.055 | Reg loss: 0.018 | Tree loss: 9.055 | Accuracy: 0.349609 | 0.903 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 9.058 | Reg loss: 0.019 | Tree loss: 9.058 | Accuracy: 0.361328 | 0.902 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 9.045 | Reg loss: 0.019 | Tree loss: 9.045 | Accuracy: 0.384766 | 0.902 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 9.037 | Reg loss: 0.020 | Tree loss: 9.037 | Accuracy: 0.384766 | 0.901 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 9.010 | Reg loss: 0.020 | Tree loss: 9.010 | Accuracy: 0.367188 | 0.901 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 9.024 | Reg loss: 0.021 | Tree loss: 9.024 | Accuracy: 0.308594 | 0.9 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 8.991 | Reg loss: 0.021 | Tree loss: 8.991 | Accuracy: 0.367188 | 0.9 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 8.993 | Reg loss: 0.022 | Tree loss: 8.993 | Accuracy: 0.349609 | 0.899 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 8.962 | Reg loss: 0.022 | Tree loss: 8.962 | Accuracy: 0.398438 | 0.899 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 8.969 | Reg loss: 0.022 | Tree loss: 8.969 | Accuracy: 0.359375 | 0.899 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 8.929 | Reg loss: 0.023 | Tree loss: 8.929 | Accuracy: 0.383838 | 0.899 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 9.135 | Reg loss: 0.015 | Tree loss: 9.135 | Accuracy: 0.371094 | 0.904 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 9.122 | Reg loss: 0.015 | Tree loss: 9.122 | Accuracy: 0.351562 | 0.903 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 9.117 | Reg loss: 0.015 | Tree loss: 9.117 | Accuracy: 0.363281 | 0.903 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 9.091 | Reg loss: 0.015 | Tree loss: 9.091 | Accuracy: 0.380859 | 0.902 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 9.076 | Reg loss: 0.015 | Tree loss: 9.076 | Accuracy: 0.373047 | 0.902 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 9.077 | Reg loss: 0.016 | Tree loss: 9.077 | Accuracy: 0.375000 | 0.902 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 9.047 | Reg loss: 0.016 | Tree loss: 9.047 | Accuracy: 0.388672 | 0.901 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 9.029 | Reg loss: 0.016 | Tree loss: 9.029 | Accuracy: 0.376953 | 0.901 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 9.031 | Reg loss: 0.017 | Tree loss: 9.031 | Accuracy: 0.363281 | 0.901 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 9.035 | Reg loss: 0.017 | Tree loss: 9.035 | Accuracy: 0.314453 | 0.901 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 9.004 | Reg loss: 0.017 | Tree loss: 9.004 | Accuracy: 0.330078 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 8.990 | Reg loss: 0.018 | Tree loss: 8.990 | Accuracy: 0.355469 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 8.961 | Reg loss: 0.018 | Tree loss: 8.961 | Accuracy: 0.355469 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 8.950 | Reg loss: 0.019 | Tree loss: 8.950 | Accuracy: 0.355469 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 8.946 | Reg loss: 0.019 | Tree loss: 8.946 | Accuracy: 0.367188 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 8.921 | Reg loss: 0.020 | Tree loss: 8.921 | Accuracy: 0.333984 | 0.9 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 8.922 | Reg loss: 0.020 | Tree loss: 8.922 | Accuracy: 0.361328 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 8.900 | Reg loss: 0.021 | Tree loss: 8.900 | Accuracy: 0.363281 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 8.884 | Reg loss: 0.021 | Tree loss: 8.884 | Accuracy: 0.347656 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 8.859 | Reg loss: 0.022 | Tree loss: 8.859 | Accuracy: 0.378906 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 8.847 | Reg loss: 0.022 | Tree loss: 8.847 | Accuracy: 0.339844 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 8.826 | Reg loss: 0.023 | Tree loss: 8.826 | Accuracy: 0.339844 | 0.899 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 8.813 | Reg loss: 0.023 | Tree loss: 8.813 | Accuracy: 0.365234 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 8.805 | Reg loss: 0.024 | Tree loss: 8.805 | Accuracy: 0.365234 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 8.791 | Reg loss: 0.024 | Tree loss: 8.791 | Accuracy: 0.363281 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.807 | Reg loss: 0.025 | Tree loss: 8.807 | Accuracy: 0.298828 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.786 | Reg loss: 0.025 | Tree loss: 8.786 | Accuracy: 0.330078 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.729 | Reg loss: 0.025 | Tree loss: 8.729 | Accuracy: 0.386719 | 0.898 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.752 | Reg loss: 0.026 | Tree loss: 8.752 | Accuracy: 0.370370 | 0.898 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 8.974 | Reg loss: 0.018 | Tree loss: 8.974 | Accuracy: 0.359375 | 0.901 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 8.961 | Reg loss: 0.018 | Tree loss: 8.961 | Accuracy: 0.353516 | 0.901 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 8.941 | Reg loss: 0.018 | Tree loss: 8.941 | Accuracy: 0.337891 | 0.901 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 8.946 | Reg loss: 0.018 | Tree loss: 8.946 | Accuracy: 0.339844 | 0.9 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 8.896 | Reg loss: 0.018 | Tree loss: 8.896 | Accuracy: 0.376953 | 0.9 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 8.896 | Reg loss: 0.019 | Tree loss: 8.896 | Accuracy: 0.341797 | 0.9 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 8.881 | Reg loss: 0.019 | Tree loss: 8.881 | Accuracy: 0.359375 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 8.844 | Reg loss: 0.019 | Tree loss: 8.844 | Accuracy: 0.378906 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 8.840 | Reg loss: 0.020 | Tree loss: 8.840 | Accuracy: 0.365234 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 8.822 | Reg loss: 0.020 | Tree loss: 8.822 | Accuracy: 0.361328 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 8.800 | Reg loss: 0.020 | Tree loss: 8.800 | Accuracy: 0.363281 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 8.792 | Reg loss: 0.021 | Tree loss: 8.792 | Accuracy: 0.326172 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.761 | Reg loss: 0.021 | Tree loss: 8.761 | Accuracy: 0.357422 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.751 | Reg loss: 0.022 | Tree loss: 8.751 | Accuracy: 0.371094 | 0.899 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.724 | Reg loss: 0.022 | Tree loss: 8.724 | Accuracy: 0.375000 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.722 | Reg loss: 0.023 | Tree loss: 8.722 | Accuracy: 0.359375 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.712 | Reg loss: 0.023 | Tree loss: 8.712 | Accuracy: 0.375000 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.680 | Reg loss: 0.024 | Tree loss: 8.680 | Accuracy: 0.353516 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.670 | Reg loss: 0.024 | Tree loss: 8.670 | Accuracy: 0.343750 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.634 | Reg loss: 0.024 | Tree loss: 8.634 | Accuracy: 0.371094 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.632 | Reg loss: 0.025 | Tree loss: 8.632 | Accuracy: 0.333984 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.607 | Reg loss: 0.025 | Tree loss: 8.607 | Accuracy: 0.335938 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.603 | Reg loss: 0.026 | Tree loss: 8.603 | Accuracy: 0.337891 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.566 | Reg loss: 0.026 | Tree loss: 8.566 | Accuracy: 0.355469 | 0.898 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.571 | Reg loss: 0.027 | Tree loss: 8.571 | Accuracy: 0.345703 | 0.897 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.556 | Reg loss: 0.027 | Tree loss: 8.556 | Accuracy: 0.349609 | 0.897 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.524 | Reg loss: 0.027 | Tree loss: 8.524 | Accuracy: 0.388672 | 0.897 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.522 | Reg loss: 0.028 | Tree loss: 8.522 | Accuracy: 0.359375 | 0.897 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.473 | Reg loss: 0.028 | Tree loss: 8.473 | Accuracy: 0.393939 | 0.897 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 8.775 | Reg loss: 0.021 | Tree loss: 8.775 | Accuracy: 0.369141 | 0.901 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.758 | Reg loss: 0.021 | Tree loss: 8.758 | Accuracy: 0.378906 | 0.9 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.739 | Reg loss: 0.021 | Tree loss: 8.739 | Accuracy: 0.375000 | 0.9 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.732 | Reg loss: 0.021 | Tree loss: 8.732 | Accuracy: 0.343750 | 0.9 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.696 | Reg loss: 0.021 | Tree loss: 8.696 | Accuracy: 0.349609 | 0.9 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.708 | Reg loss: 0.021 | Tree loss: 8.708 | Accuracy: 0.361328 | 0.9 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.652 | Reg loss: 0.022 | Tree loss: 8.652 | Accuracy: 0.357422 | 0.899 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.653 | Reg loss: 0.022 | Tree loss: 8.653 | Accuracy: 0.373047 | 0.899 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.636 | Reg loss: 0.022 | Tree loss: 8.636 | Accuracy: 0.312500 | 0.899 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.612 | Reg loss: 0.023 | Tree loss: 8.612 | Accuracy: 0.347656 | 0.899 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.585 | Reg loss: 0.023 | Tree loss: 8.585 | Accuracy: 0.333984 | 0.899 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.561 | Reg loss: 0.023 | Tree loss: 8.561 | Accuracy: 0.380859 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.541 | Reg loss: 0.024 | Tree loss: 8.541 | Accuracy: 0.347656 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.520 | Reg loss: 0.024 | Tree loss: 8.520 | Accuracy: 0.339844 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.498 | Reg loss: 0.024 | Tree loss: 8.498 | Accuracy: 0.333984 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.469 | Reg loss: 0.025 | Tree loss: 8.469 | Accuracy: 0.343750 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.454 | Reg loss: 0.025 | Tree loss: 8.454 | Accuracy: 0.353516 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.435 | Reg loss: 0.026 | Tree loss: 8.435 | Accuracy: 0.345703 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.432 | Reg loss: 0.026 | Tree loss: 8.432 | Accuracy: 0.365234 | 0.898 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.397 | Reg loss: 0.026 | Tree loss: 8.397 | Accuracy: 0.371094 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.373 | Reg loss: 0.027 | Tree loss: 8.373 | Accuracy: 0.375000 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.368 | Reg loss: 0.027 | Tree loss: 8.368 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.349 | Reg loss: 0.028 | Tree loss: 8.349 | Accuracy: 0.371094 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.331 | Reg loss: 0.028 | Tree loss: 8.331 | Accuracy: 0.375000 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.310 | Reg loss: 0.028 | Tree loss: 8.310 | Accuracy: 0.380859 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.290 | Reg loss: 0.029 | Tree loss: 8.290 | Accuracy: 0.361328 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.258 | Reg loss: 0.029 | Tree loss: 8.258 | Accuracy: 0.337891 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.262 | Reg loss: 0.029 | Tree loss: 8.262 | Accuracy: 0.365234 | 0.897 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.240 | Reg loss: 0.030 | Tree loss: 8.240 | Accuracy: 0.363636 | 0.897 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.538 | Reg loss: 0.023 | Tree loss: 8.538 | Accuracy: 0.349609 | 0.9 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.513 | Reg loss: 0.023 | Tree loss: 8.513 | Accuracy: 0.353516 | 0.899 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.533 | Reg loss: 0.023 | Tree loss: 8.533 | Accuracy: 0.365234 | 0.899 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.484 | Reg loss: 0.023 | Tree loss: 8.484 | Accuracy: 0.378906 | 0.899 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.473 | Reg loss: 0.023 | Tree loss: 8.473 | Accuracy: 0.328125 | 0.899 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.440 | Reg loss: 0.024 | Tree loss: 8.440 | Accuracy: 0.345703 | 0.899 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.430 | Reg loss: 0.024 | Tree loss: 8.430 | Accuracy: 0.337891 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.400 | Reg loss: 0.024 | Tree loss: 8.400 | Accuracy: 0.349609 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.393 | Reg loss: 0.024 | Tree loss: 8.393 | Accuracy: 0.375000 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.368 | Reg loss: 0.025 | Tree loss: 8.368 | Accuracy: 0.375000 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.360 | Reg loss: 0.025 | Tree loss: 8.360 | Accuracy: 0.347656 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.327 | Reg loss: 0.025 | Tree loss: 8.327 | Accuracy: 0.365234 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.282 | Reg loss: 0.026 | Tree loss: 8.282 | Accuracy: 0.382812 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.305 | Reg loss: 0.026 | Tree loss: 8.305 | Accuracy: 0.326172 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.256 | Reg loss: 0.026 | Tree loss: 8.256 | Accuracy: 0.361328 | 0.898 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.229 | Reg loss: 0.027 | Tree loss: 8.229 | Accuracy: 0.355469 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.199 | Reg loss: 0.027 | Tree loss: 8.199 | Accuracy: 0.396484 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 8.189 | Reg loss: 0.027 | Tree loss: 8.189 | Accuracy: 0.392578 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 8.158 | Reg loss: 0.028 | Tree loss: 8.158 | Accuracy: 0.332031 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 8.131 | Reg loss: 0.028 | Tree loss: 8.131 | Accuracy: 0.357422 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 8.125 | Reg loss: 0.028 | Tree loss: 8.125 | Accuracy: 0.349609 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 8.106 | Reg loss: 0.029 | Tree loss: 8.106 | Accuracy: 0.345703 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 8.098 | Reg loss: 0.029 | Tree loss: 8.098 | Accuracy: 0.363281 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 8.075 | Reg loss: 0.029 | Tree loss: 8.075 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 8.029 | Reg loss: 0.030 | Tree loss: 8.029 | Accuracy: 0.339844 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 8.021 | Reg loss: 0.030 | Tree loss: 8.021 | Accuracy: 0.376953 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 8.022 | Reg loss: 0.030 | Tree loss: 8.022 | Accuracy: 0.341797 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 7.991 | Reg loss: 0.030 | Tree loss: 7.991 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 7.932 | Reg loss: 0.031 | Tree loss: 7.932 | Accuracy: 0.363636 | 0.897 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.335 | Reg loss: 0.025 | Tree loss: 8.335 | Accuracy: 0.332031 | 0.899 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.320 | Reg loss: 0.025 | Tree loss: 8.320 | Accuracy: 0.335938 | 0.899 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.273 | Reg loss: 0.025 | Tree loss: 8.273 | Accuracy: 0.363281 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.237 | Reg loss: 0.025 | Tree loss: 8.237 | Accuracy: 0.375000 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.241 | Reg loss: 0.025 | Tree loss: 8.241 | Accuracy: 0.373047 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 8.221 | Reg loss: 0.025 | Tree loss: 8.221 | Accuracy: 0.351562 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 8.188 | Reg loss: 0.025 | Tree loss: 8.188 | Accuracy: 0.382812 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 8.129 | Reg loss: 0.026 | Tree loss: 8.129 | Accuracy: 0.365234 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 8.146 | Reg loss: 0.026 | Tree loss: 8.146 | Accuracy: 0.343750 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 8.094 | Reg loss: 0.026 | Tree loss: 8.094 | Accuracy: 0.380859 | 0.898 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 8.087 | Reg loss: 0.026 | Tree loss: 8.087 | Accuracy: 0.347656 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 8.082 | Reg loss: 0.027 | Tree loss: 8.082 | Accuracy: 0.341797 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 8.040 | Reg loss: 0.027 | Tree loss: 8.040 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 7.973 | Reg loss: 0.027 | Tree loss: 7.973 | Accuracy: 0.324219 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 7.974 | Reg loss: 0.027 | Tree loss: 7.974 | Accuracy: 0.355469 | 0.897 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 7.950 | Reg loss: 0.028 | Tree loss: 7.950 | Accuracy: 0.363281 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 7.949 | Reg loss: 0.028 | Tree loss: 7.949 | Accuracy: 0.371094 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 7.916 | Reg loss: 0.028 | Tree loss: 7.916 | Accuracy: 0.365234 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 7.935 | Reg loss: 0.029 | Tree loss: 7.935 | Accuracy: 0.332031 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 7.903 | Reg loss: 0.029 | Tree loss: 7.903 | Accuracy: 0.361328 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 7.874 | Reg loss: 0.029 | Tree loss: 7.874 | Accuracy: 0.320312 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 7.850 | Reg loss: 0.029 | Tree loss: 7.850 | Accuracy: 0.376953 | 0.897 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 7.799 | Reg loss: 0.030 | Tree loss: 7.799 | Accuracy: 0.341797 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 7.772 | Reg loss: 0.030 | Tree loss: 7.772 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.767 | Reg loss: 0.030 | Tree loss: 7.767 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.740 | Reg loss: 0.030 | Tree loss: 7.740 | Accuracy: 0.388672 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.729 | Reg loss: 0.031 | Tree loss: 7.729 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.720 | Reg loss: 0.031 | Tree loss: 7.720 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.708 | Reg loss: 0.031 | Tree loss: 7.708 | Accuracy: 0.350168 | 0.896 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 8.049 | Reg loss: 0.026 | Tree loss: 8.049 | Accuracy: 0.367188 | 0.898 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 8.021 | Reg loss: 0.026 | Tree loss: 8.021 | Accuracy: 0.388672 | 0.898 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 8.008 | Reg loss: 0.026 | Tree loss: 8.008 | Accuracy: 0.378906 | 0.898 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 7.970 | Reg loss: 0.026 | Tree loss: 7.970 | Accuracy: 0.388672 | 0.898 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 7.990 | Reg loss: 0.026 | Tree loss: 7.990 | Accuracy: 0.326172 | 0.898 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 7.940 | Reg loss: 0.026 | Tree loss: 7.940 | Accuracy: 0.376953 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 7.906 | Reg loss: 0.026 | Tree loss: 7.906 | Accuracy: 0.355469 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 7.901 | Reg loss: 0.026 | Tree loss: 7.901 | Accuracy: 0.365234 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 7.877 | Reg loss: 0.027 | Tree loss: 7.877 | Accuracy: 0.376953 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 7.841 | Reg loss: 0.027 | Tree loss: 7.841 | Accuracy: 0.347656 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 7.836 | Reg loss: 0.027 | Tree loss: 7.836 | Accuracy: 0.332031 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 7.799 | Reg loss: 0.027 | Tree loss: 7.799 | Accuracy: 0.355469 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.754 | Reg loss: 0.027 | Tree loss: 7.754 | Accuracy: 0.359375 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.783 | Reg loss: 0.028 | Tree loss: 7.783 | Accuracy: 0.351562 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.738 | Reg loss: 0.028 | Tree loss: 7.738 | Accuracy: 0.326172 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.685 | Reg loss: 0.028 | Tree loss: 7.685 | Accuracy: 0.332031 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.664 | Reg loss: 0.028 | Tree loss: 7.664 | Accuracy: 0.337891 | 0.897 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.677 | Reg loss: 0.029 | Tree loss: 7.677 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.625 | Reg loss: 0.029 | Tree loss: 7.625 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.595 | Reg loss: 0.029 | Tree loss: 7.595 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.588 | Reg loss: 0.030 | Tree loss: 7.588 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.538 | Reg loss: 0.030 | Tree loss: 7.538 | Accuracy: 0.380859 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.551 | Reg loss: 0.030 | Tree loss: 7.551 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.504 | Reg loss: 0.030 | Tree loss: 7.504 | Accuracy: 0.333984 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.493 | Reg loss: 0.030 | Tree loss: 7.493 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.488 | Reg loss: 0.031 | Tree loss: 7.488 | Accuracy: 0.361328 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.471 | Reg loss: 0.031 | Tree loss: 7.471 | Accuracy: 0.335938 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.433 | Reg loss: 0.031 | Tree loss: 7.433 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.406 | Reg loss: 0.031 | Tree loss: 7.406 | Accuracy: 0.390572 | 0.896 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.829 | Reg loss: 0.026 | Tree loss: 7.829 | Accuracy: 0.333984 | 0.898 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.782 | Reg loss: 0.026 | Tree loss: 7.782 | Accuracy: 0.341797 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.764 | Reg loss: 0.026 | Tree loss: 7.764 | Accuracy: 0.345703 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.710 | Reg loss: 0.026 | Tree loss: 7.710 | Accuracy: 0.400391 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.722 | Reg loss: 0.026 | Tree loss: 7.722 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.684 | Reg loss: 0.026 | Tree loss: 7.684 | Accuracy: 0.347656 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.650 | Reg loss: 0.027 | Tree loss: 7.650 | Accuracy: 0.345703 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.618 | Reg loss: 0.027 | Tree loss: 7.618 | Accuracy: 0.335938 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.627 | Reg loss: 0.027 | Tree loss: 7.627 | Accuracy: 0.347656 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.575 | Reg loss: 0.027 | Tree loss: 7.575 | Accuracy: 0.343750 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.565 | Reg loss: 0.027 | Tree loss: 7.565 | Accuracy: 0.351562 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.535 | Reg loss: 0.027 | Tree loss: 7.535 | Accuracy: 0.363281 | 0.897 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.466 | Reg loss: 0.028 | Tree loss: 7.466 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.480 | Reg loss: 0.028 | Tree loss: 7.480 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.426 | Reg loss: 0.028 | Tree loss: 7.426 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.406 | Reg loss: 0.028 | Tree loss: 7.406 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.355 | Reg loss: 0.029 | Tree loss: 7.355 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.373 | Reg loss: 0.029 | Tree loss: 7.373 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.312 | Reg loss: 0.029 | Tree loss: 7.312 | Accuracy: 0.363281 | 0.896 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.326 | Reg loss: 0.030 | Tree loss: 7.326 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 7.305 | Reg loss: 0.030 | Tree loss: 7.305 | Accuracy: 0.349609 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.291 | Reg loss: 0.030 | Tree loss: 7.291 | Accuracy: 0.330078 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 7.248 | Reg loss: 0.031 | Tree loss: 7.248 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 7.215 | Reg loss: 0.031 | Tree loss: 7.215 | Accuracy: 0.392578 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 7.190 | Reg loss: 0.031 | Tree loss: 7.190 | Accuracy: 0.386719 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 7.164 | Reg loss: 0.032 | Tree loss: 7.164 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 7.156 | Reg loss: 0.032 | Tree loss: 7.156 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 7.113 | Reg loss: 0.032 | Tree loss: 7.113 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 7.083 | Reg loss: 0.033 | Tree loss: 7.083 | Accuracy: 0.370370 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.530 | Reg loss: 0.026 | Tree loss: 7.530 | Accuracy: 0.359375 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.537 | Reg loss: 0.026 | Tree loss: 7.537 | Accuracy: 0.345703 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.489 | Reg loss: 0.026 | Tree loss: 7.489 | Accuracy: 0.333984 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.471 | Reg loss: 0.026 | Tree loss: 7.471 | Accuracy: 0.343750 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.443 | Reg loss: 0.026 | Tree loss: 7.443 | Accuracy: 0.359375 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.382 | Reg loss: 0.026 | Tree loss: 7.382 | Accuracy: 0.365234 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.352 | Reg loss: 0.026 | Tree loss: 7.352 | Accuracy: 0.382812 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.327 | Reg loss: 0.027 | Tree loss: 7.327 | Accuracy: 0.376953 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.327 | Reg loss: 0.027 | Tree loss: 7.327 | Accuracy: 0.351562 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.286 | Reg loss: 0.027 | Tree loss: 7.286 | Accuracy: 0.406250 | 0.897 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 7.231 | Reg loss: 0.027 | Tree loss: 7.231 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 7.230 | Reg loss: 0.028 | Tree loss: 7.230 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 7.228 | Reg loss: 0.028 | Tree loss: 7.228 | Accuracy: 0.343750 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 7.160 | Reg loss: 0.028 | Tree loss: 7.160 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 7.159 | Reg loss: 0.029 | Tree loss: 7.159 | Accuracy: 0.333984 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 7.142 | Reg loss: 0.029 | Tree loss: 7.142 | Accuracy: 0.345703 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 7.105 | Reg loss: 0.030 | Tree loss: 7.105 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 7.065 | Reg loss: 0.030 | Tree loss: 7.065 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 7.065 | Reg loss: 0.031 | Tree loss: 7.065 | Accuracy: 0.318359 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 7.002 | Reg loss: 0.031 | Tree loss: 7.002 | Accuracy: 0.339844 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 6.975 | Reg loss: 0.031 | Tree loss: 6.975 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 6.981 | Reg loss: 0.032 | Tree loss: 6.981 | Accuracy: 0.339844 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 6.954 | Reg loss: 0.032 | Tree loss: 6.954 | Accuracy: 0.375000 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 6.924 | Reg loss: 0.033 | Tree loss: 6.924 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 6.897 | Reg loss: 0.033 | Tree loss: 6.897 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 6.882 | Reg loss: 0.033 | Tree loss: 6.882 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 6.849 | Reg loss: 0.034 | Tree loss: 6.849 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 6.800 | Reg loss: 0.034 | Tree loss: 6.800 | Accuracy: 0.392578 | 0.896 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 6.806 | Reg loss: 0.035 | Tree loss: 6.806 | Accuracy: 0.333333 | 0.896 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 7.284 | Reg loss: 0.026 | Tree loss: 7.284 | Accuracy: 0.337891 | 0.897 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 7.232 | Reg loss: 0.026 | Tree loss: 7.232 | Accuracy: 0.347656 | 0.897 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 7.240 | Reg loss: 0.026 | Tree loss: 7.240 | Accuracy: 0.367188 | 0.897 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 7.198 | Reg loss: 0.026 | Tree loss: 7.198 | Accuracy: 0.310547 | 0.897 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 7.125 | Reg loss: 0.026 | Tree loss: 7.125 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 7.116 | Reg loss: 0.027 | Tree loss: 7.116 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 7.062 | Reg loss: 0.027 | Tree loss: 7.062 | Accuracy: 0.398438 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 7.044 | Reg loss: 0.027 | Tree loss: 7.044 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 7.023 | Reg loss: 0.028 | Tree loss: 7.023 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 7.015 | Reg loss: 0.028 | Tree loss: 7.015 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 6.987 | Reg loss: 0.028 | Tree loss: 6.987 | Accuracy: 0.341797 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 6.927 | Reg loss: 0.029 | Tree loss: 6.927 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 6.910 | Reg loss: 0.029 | Tree loss: 6.910 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 6.879 | Reg loss: 0.030 | Tree loss: 6.879 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 6.872 | Reg loss: 0.030 | Tree loss: 6.872 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 6.810 | Reg loss: 0.031 | Tree loss: 6.810 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 6.793 | Reg loss: 0.031 | Tree loss: 6.793 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 6.755 | Reg loss: 0.032 | Tree loss: 6.755 | Accuracy: 0.332031 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 6.731 | Reg loss: 0.032 | Tree loss: 6.731 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 6.710 | Reg loss: 0.033 | Tree loss: 6.710 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 6.650 | Reg loss: 0.033 | Tree loss: 6.650 | Accuracy: 0.375000 | 0.896 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 6.699 | Reg loss: 0.034 | Tree loss: 6.699 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 6.673 | Reg loss: 0.034 | Tree loss: 6.673 | Accuracy: 0.324219 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.615 | Reg loss: 0.034 | Tree loss: 6.615 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 6.572 | Reg loss: 0.035 | Tree loss: 6.572 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.526 | Reg loss: 0.035 | Tree loss: 6.526 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.500 | Reg loss: 0.036 | Tree loss: 6.500 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.493 | Reg loss: 0.036 | Tree loss: 6.493 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.488 | Reg loss: 0.037 | Tree loss: 6.488 | Accuracy: 0.356902 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 6.982 | Reg loss: 0.028 | Tree loss: 6.982 | Accuracy: 0.353516 | 0.897 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 6.956 | Reg loss: 0.028 | Tree loss: 6.956 | Accuracy: 0.363281 | 0.897 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 6.922 | Reg loss: 0.028 | Tree loss: 6.922 | Accuracy: 0.373047 | 0.897 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 6.896 | Reg loss: 0.028 | Tree loss: 6.896 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 6.892 | Reg loss: 0.028 | Tree loss: 6.892 | Accuracy: 0.337891 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 6.858 | Reg loss: 0.029 | Tree loss: 6.858 | Accuracy: 0.339844 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 6.818 | Reg loss: 0.029 | Tree loss: 6.818 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 6.786 | Reg loss: 0.029 | Tree loss: 6.786 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 6.705 | Reg loss: 0.029 | Tree loss: 6.705 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 6.718 | Reg loss: 0.030 | Tree loss: 6.718 | Accuracy: 0.326172 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 6.675 | Reg loss: 0.030 | Tree loss: 6.675 | Accuracy: 0.349609 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 6.673 | Reg loss: 0.031 | Tree loss: 6.673 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 6.588 | Reg loss: 0.031 | Tree loss: 6.588 | Accuracy: 0.376953 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 6.571 | Reg loss: 0.031 | Tree loss: 6.571 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 6.543 | Reg loss: 0.032 | Tree loss: 6.543 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 6.534 | Reg loss: 0.032 | Tree loss: 6.534 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.465 | Reg loss: 0.033 | Tree loss: 6.465 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 6.455 | Reg loss: 0.033 | Tree loss: 6.455 | Accuracy: 0.341797 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.423 | Reg loss: 0.034 | Tree loss: 6.423 | Accuracy: 0.328125 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.377 | Reg loss: 0.034 | Tree loss: 6.377 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.324 | Reg loss: 0.035 | Tree loss: 6.324 | Accuracy: 0.398438 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.309 | Reg loss: 0.035 | Tree loss: 6.309 | Accuracy: 0.388672 | 0.896 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.321 | Reg loss: 0.036 | Tree loss: 6.321 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.285 | Reg loss: 0.036 | Tree loss: 6.285 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.273 | Reg loss: 0.036 | Tree loss: 6.273 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.238 | Reg loss: 0.037 | Tree loss: 6.238 | Accuracy: 0.318359 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.228 | Reg loss: 0.037 | Tree loss: 6.228 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 6.169 | Reg loss: 0.038 | Tree loss: 6.169 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 6.205 | Reg loss: 0.038 | Tree loss: 6.205 | Accuracy: 0.370370 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 6.704 | Reg loss: 0.030 | Tree loss: 6.704 | Accuracy: 0.333984 | 0.897 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 6.641 | Reg loss: 0.030 | Tree loss: 6.641 | Accuracy: 0.390625 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 6.661 | Reg loss: 0.030 | Tree loss: 6.661 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 6.625 | Reg loss: 0.030 | Tree loss: 6.625 | Accuracy: 0.335938 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 6.618 | Reg loss: 0.030 | Tree loss: 6.618 | Accuracy: 0.337891 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 6.555 | Reg loss: 0.030 | Tree loss: 6.555 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 6.550 | Reg loss: 0.031 | Tree loss: 6.550 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 6.534 | Reg loss: 0.031 | Tree loss: 6.534 | Accuracy: 0.355469 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.442 | Reg loss: 0.031 | Tree loss: 6.442 | Accuracy: 0.375000 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.423 | Reg loss: 0.031 | Tree loss: 6.423 | Accuracy: 0.328125 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.403 | Reg loss: 0.032 | Tree loss: 6.403 | Accuracy: 0.375000 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.343 | Reg loss: 0.032 | Tree loss: 6.343 | Accuracy: 0.330078 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.306 | Reg loss: 0.032 | Tree loss: 6.306 | Accuracy: 0.337891 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.293 | Reg loss: 0.033 | Tree loss: 6.293 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 6.251 | Reg loss: 0.033 | Tree loss: 6.251 | Accuracy: 0.386719 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 6.248 | Reg loss: 0.034 | Tree loss: 6.248 | Accuracy: 0.330078 | 0.896 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 6.189 | Reg loss: 0.034 | Tree loss: 6.189 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 6.158 | Reg loss: 0.035 | Tree loss: 6.158 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 6.118 | Reg loss: 0.035 | Tree loss: 6.118 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 6.095 | Reg loss: 0.035 | Tree loss: 6.095 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 6.060 | Reg loss: 0.036 | Tree loss: 6.060 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 6.043 | Reg loss: 0.036 | Tree loss: 6.043 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 6.005 | Reg loss: 0.037 | Tree loss: 6.005 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 5.962 | Reg loss: 0.037 | Tree loss: 5.962 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 5.963 | Reg loss: 0.038 | Tree loss: 5.963 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 5.932 | Reg loss: 0.038 | Tree loss: 5.932 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 5.917 | Reg loss: 0.038 | Tree loss: 5.917 | Accuracy: 0.365234 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 5.861 | Reg loss: 0.039 | Tree loss: 5.861 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 5.867 | Reg loss: 0.039 | Tree loss: 5.867 | Accuracy: 0.313131 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.434 | Reg loss: 0.031 | Tree loss: 6.434 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.449 | Reg loss: 0.031 | Tree loss: 6.449 | Accuracy: 0.349609 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 6.377 | Reg loss: 0.031 | Tree loss: 6.377 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.362 | Reg loss: 0.032 | Tree loss: 6.362 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 6.293 | Reg loss: 0.032 | Tree loss: 6.293 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 6.315 | Reg loss: 0.032 | Tree loss: 6.315 | Accuracy: 0.343750 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 6.256 | Reg loss: 0.032 | Tree loss: 6.256 | Accuracy: 0.345703 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 6.226 | Reg loss: 0.032 | Tree loss: 6.226 | Accuracy: 0.339844 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 6.199 | Reg loss: 0.033 | Tree loss: 6.199 | Accuracy: 0.361328 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 6.114 | Reg loss: 0.033 | Tree loss: 6.114 | Accuracy: 0.380859 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 6.130 | Reg loss: 0.033 | Tree loss: 6.130 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 6.045 | Reg loss: 0.034 | Tree loss: 6.045 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 6.048 | Reg loss: 0.034 | Tree loss: 6.048 | Accuracy: 0.361328 | 0.896 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 5.999 | Reg loss: 0.034 | Tree loss: 5.999 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 5.982 | Reg loss: 0.035 | Tree loss: 5.982 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 5.949 | Reg loss: 0.035 | Tree loss: 5.949 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 5.883 | Reg loss: 0.035 | Tree loss: 5.883 | Accuracy: 0.417969 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 5.904 | Reg loss: 0.036 | Tree loss: 5.904 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 5.829 | Reg loss: 0.036 | Tree loss: 5.829 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 5.811 | Reg loss: 0.037 | Tree loss: 5.811 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 5.804 | Reg loss: 0.037 | Tree loss: 5.804 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 5.767 | Reg loss: 0.038 | Tree loss: 5.767 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 5.741 | Reg loss: 0.038 | Tree loss: 5.741 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 5.746 | Reg loss: 0.038 | Tree loss: 5.746 | Accuracy: 0.310547 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 5.687 | Reg loss: 0.039 | Tree loss: 5.687 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 5.636 | Reg loss: 0.039 | Tree loss: 5.636 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 5.648 | Reg loss: 0.040 | Tree loss: 5.648 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 5.602 | Reg loss: 0.040 | Tree loss: 5.602 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 5.614 | Reg loss: 0.040 | Tree loss: 5.614 | Accuracy: 0.373737 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 6.212 | Reg loss: 0.033 | Tree loss: 6.212 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 6.163 | Reg loss: 0.033 | Tree loss: 6.163 | Accuracy: 0.378906 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 6.025 | Reg loss: 0.033 | Tree loss: 6.025 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 6.059 | Reg loss: 0.033 | Tree loss: 6.059 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 6.035 | Reg loss: 0.033 | Tree loss: 6.035 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 5.995 | Reg loss: 0.033 | Tree loss: 5.995 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 5.965 | Reg loss: 0.034 | Tree loss: 5.965 | Accuracy: 0.376953 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 5.945 | Reg loss: 0.034 | Tree loss: 5.945 | Accuracy: 0.384766 | 0.896 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 5.884 | Reg loss: 0.034 | Tree loss: 5.884 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 5.862 | Reg loss: 0.034 | Tree loss: 5.862 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 5.838 | Reg loss: 0.035 | Tree loss: 5.838 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 5.796 | Reg loss: 0.035 | Tree loss: 5.796 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 5.814 | Reg loss: 0.035 | Tree loss: 5.814 | Accuracy: 0.287109 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 5.733 | Reg loss: 0.036 | Tree loss: 5.733 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 5.683 | Reg loss: 0.036 | Tree loss: 5.683 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 5.671 | Reg loss: 0.036 | Tree loss: 5.671 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 5.643 | Reg loss: 0.037 | Tree loss: 5.643 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 5.589 | Reg loss: 0.037 | Tree loss: 5.589 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 5.515 | Reg loss: 0.037 | Tree loss: 5.515 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 5.538 | Reg loss: 0.038 | Tree loss: 5.538 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 5.495 | Reg loss: 0.038 | Tree loss: 5.495 | Accuracy: 0.310547 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 5.448 | Reg loss: 0.039 | Tree loss: 5.448 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 5.469 | Reg loss: 0.039 | Tree loss: 5.469 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 5.375 | Reg loss: 0.039 | Tree loss: 5.375 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 5.406 | Reg loss: 0.040 | Tree loss: 5.406 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 5.381 | Reg loss: 0.040 | Tree loss: 5.381 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 5.347 | Reg loss: 0.041 | Tree loss: 5.347 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 5.305 | Reg loss: 0.041 | Tree loss: 5.305 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 5.312 | Reg loss: 0.041 | Tree loss: 5.312 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 5.917 | Reg loss: 0.034 | Tree loss: 5.917 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 5.839 | Reg loss: 0.034 | Tree loss: 5.839 | Accuracy: 0.367188 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 5.832 | Reg loss: 0.034 | Tree loss: 5.832 | Accuracy: 0.378906 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 5.784 | Reg loss: 0.034 | Tree loss: 5.784 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 5.727 | Reg loss: 0.034 | Tree loss: 5.727 | Accuracy: 0.376953 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 5.694 | Reg loss: 0.035 | Tree loss: 5.694 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 5.655 | Reg loss: 0.035 | Tree loss: 5.655 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 5.622 | Reg loss: 0.035 | Tree loss: 5.622 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 5.628 | Reg loss: 0.035 | Tree loss: 5.628 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 5.565 | Reg loss: 0.035 | Tree loss: 5.565 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 5.515 | Reg loss: 0.036 | Tree loss: 5.515 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 5.466 | Reg loss: 0.036 | Tree loss: 5.466 | Accuracy: 0.421875 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 5.505 | Reg loss: 0.036 | Tree loss: 5.505 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 5.427 | Reg loss: 0.037 | Tree loss: 5.427 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 5.412 | Reg loss: 0.037 | Tree loss: 5.412 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 5.374 | Reg loss: 0.037 | Tree loss: 5.374 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 5.368 | Reg loss: 0.038 | Tree loss: 5.368 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 5.316 | Reg loss: 0.038 | Tree loss: 5.316 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 5.301 | Reg loss: 0.038 | Tree loss: 5.301 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 5.238 | Reg loss: 0.039 | Tree loss: 5.238 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 5.217 | Reg loss: 0.039 | Tree loss: 5.217 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 5.157 | Reg loss: 0.040 | Tree loss: 5.157 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 5.123 | Reg loss: 0.040 | Tree loss: 5.123 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 5.114 | Reg loss: 0.040 | Tree loss: 5.114 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 5.137 | Reg loss: 0.041 | Tree loss: 5.137 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 5.087 | Reg loss: 0.041 | Tree loss: 5.087 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 5.026 | Reg loss: 0.042 | Tree loss: 5.026 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 4.950 | Reg loss: 0.042 | Tree loss: 4.950 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 4.925 | Reg loss: 0.042 | Tree loss: 4.925 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 5.572 | Reg loss: 0.035 | Tree loss: 5.572 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 5.561 | Reg loss: 0.035 | Tree loss: 5.561 | Accuracy: 0.394531 | 0.896 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 5.594 | Reg loss: 0.035 | Tree loss: 5.594 | Accuracy: 0.337891 | 0.896 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 5.458 | Reg loss: 0.035 | Tree loss: 5.458 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 5.449 | Reg loss: 0.036 | Tree loss: 5.449 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 5.420 | Reg loss: 0.036 | Tree loss: 5.420 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 5.419 | Reg loss: 0.036 | Tree loss: 5.419 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 5.338 | Reg loss: 0.036 | Tree loss: 5.338 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 5.333 | Reg loss: 0.036 | Tree loss: 5.333 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 5.251 | Reg loss: 0.036 | Tree loss: 5.251 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 5.237 | Reg loss: 0.037 | Tree loss: 5.237 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 5.207 | Reg loss: 0.037 | Tree loss: 5.207 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 5.177 | Reg loss: 0.037 | Tree loss: 5.177 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 5.156 | Reg loss: 0.038 | Tree loss: 5.156 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 5.122 | Reg loss: 0.038 | Tree loss: 5.122 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 5.058 | Reg loss: 0.038 | Tree loss: 5.058 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 5.016 | Reg loss: 0.039 | Tree loss: 5.016 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 5.018 | Reg loss: 0.039 | Tree loss: 5.018 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 4.955 | Reg loss: 0.039 | Tree loss: 4.955 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 4.914 | Reg loss: 0.040 | Tree loss: 4.914 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 4.900 | Reg loss: 0.040 | Tree loss: 4.900 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 4.884 | Reg loss: 0.041 | Tree loss: 4.884 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 4.834 | Reg loss: 0.041 | Tree loss: 4.834 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 4.850 | Reg loss: 0.041 | Tree loss: 4.850 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 4.776 | Reg loss: 0.042 | Tree loss: 4.776 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 4.749 | Reg loss: 0.042 | Tree loss: 4.749 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 4.703 | Reg loss: 0.042 | Tree loss: 4.703 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 4.720 | Reg loss: 0.043 | Tree loss: 4.720 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 4.680 | Reg loss: 0.043 | Tree loss: 4.680 | Accuracy: 0.336700 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 5.282 | Reg loss: 0.036 | Tree loss: 5.282 | Accuracy: 0.390625 | 0.896 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 5.268 | Reg loss: 0.036 | Tree loss: 5.268 | Accuracy: 0.349609 | 0.896 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 5.271 | Reg loss: 0.036 | Tree loss: 5.271 | Accuracy: 0.384766 | 0.896 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 5.222 | Reg loss: 0.036 | Tree loss: 5.222 | Accuracy: 0.388672 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 5.124 | Reg loss: 0.037 | Tree loss: 5.124 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 5.120 | Reg loss: 0.037 | Tree loss: 5.120 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 5.124 | Reg loss: 0.037 | Tree loss: 5.124 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 5.085 | Reg loss: 0.037 | Tree loss: 5.085 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 5.061 | Reg loss: 0.037 | Tree loss: 5.061 | Accuracy: 0.308594 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 4.962 | Reg loss: 0.037 | Tree loss: 4.962 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 4.938 | Reg loss: 0.038 | Tree loss: 4.938 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 4.862 | Reg loss: 0.038 | Tree loss: 4.862 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 4.873 | Reg loss: 0.038 | Tree loss: 4.873 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 4.877 | Reg loss: 0.039 | Tree loss: 4.877 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 4.803 | Reg loss: 0.039 | Tree loss: 4.803 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 4.822 | Reg loss: 0.039 | Tree loss: 4.822 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 4.729 | Reg loss: 0.040 | Tree loss: 4.729 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 4.707 | Reg loss: 0.040 | Tree loss: 4.707 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 4.657 | Reg loss: 0.040 | Tree loss: 4.657 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 4.642 | Reg loss: 0.041 | Tree loss: 4.642 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 4.576 | Reg loss: 0.041 | Tree loss: 4.576 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 4.553 | Reg loss: 0.041 | Tree loss: 4.553 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 4.575 | Reg loss: 0.042 | Tree loss: 4.575 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 4.522 | Reg loss: 0.042 | Tree loss: 4.522 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 4.483 | Reg loss: 0.042 | Tree loss: 4.483 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 4.510 | Reg loss: 0.043 | Tree loss: 4.510 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 4.433 | Reg loss: 0.043 | Tree loss: 4.433 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 4.407 | Reg loss: 0.043 | Tree loss: 4.407 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 4.422 | Reg loss: 0.044 | Tree loss: 4.422 | Accuracy: 0.292929 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 5.025 | Reg loss: 0.037 | Tree loss: 5.025 | Accuracy: 0.429688 | 0.896 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 5.079 | Reg loss: 0.037 | Tree loss: 5.079 | Accuracy: 0.361328 | 0.896 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 4.936 | Reg loss: 0.037 | Tree loss: 4.936 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 4.907 | Reg loss: 0.038 | Tree loss: 4.907 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 4.958 | Reg loss: 0.038 | Tree loss: 4.958 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 4.874 | Reg loss: 0.038 | Tree loss: 4.874 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 4.825 | Reg loss: 0.038 | Tree loss: 4.825 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 4.795 | Reg loss: 0.038 | Tree loss: 4.795 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 4.761 | Reg loss: 0.038 | Tree loss: 4.761 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 4.705 | Reg loss: 0.039 | Tree loss: 4.705 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 4.661 | Reg loss: 0.039 | Tree loss: 4.661 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 4.625 | Reg loss: 0.039 | Tree loss: 4.625 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 4.567 | Reg loss: 0.039 | Tree loss: 4.567 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 4.595 | Reg loss: 0.040 | Tree loss: 4.595 | Accuracy: 0.314453 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 4.518 | Reg loss: 0.040 | Tree loss: 4.518 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 4.517 | Reg loss: 0.040 | Tree loss: 4.517 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 4.479 | Reg loss: 0.040 | Tree loss: 4.479 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 4.400 | Reg loss: 0.041 | Tree loss: 4.400 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 4.388 | Reg loss: 0.041 | Tree loss: 4.388 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 4.400 | Reg loss: 0.041 | Tree loss: 4.400 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 4.360 | Reg loss: 0.042 | Tree loss: 4.360 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 4.314 | Reg loss: 0.042 | Tree loss: 4.314 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 4.282 | Reg loss: 0.042 | Tree loss: 4.282 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 4.262 | Reg loss: 0.043 | Tree loss: 4.262 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 4.252 | Reg loss: 0.043 | Tree loss: 4.252 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 4.191 | Reg loss: 0.043 | Tree loss: 4.191 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 4.179 | Reg loss: 0.044 | Tree loss: 4.179 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 4.146 | Reg loss: 0.044 | Tree loss: 4.146 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 4.081 | Reg loss: 0.044 | Tree loss: 4.081 | Accuracy: 0.336700 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 4.820 | Reg loss: 0.038 | Tree loss: 4.820 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 4.705 | Reg loss: 0.038 | Tree loss: 4.705 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 4.748 | Reg loss: 0.038 | Tree loss: 4.748 | Accuracy: 0.320312 | 0.896 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 4.682 | Reg loss: 0.039 | Tree loss: 4.682 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 4.664 | Reg loss: 0.039 | Tree loss: 4.664 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 4.584 | Reg loss: 0.039 | Tree loss: 4.584 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 4.538 | Reg loss: 0.039 | Tree loss: 4.538 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 4.519 | Reg loss: 0.039 | Tree loss: 4.519 | Accuracy: 0.332031 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 4.503 | Reg loss: 0.039 | Tree loss: 4.503 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 4.464 | Reg loss: 0.039 | Tree loss: 4.464 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 4.451 | Reg loss: 0.040 | Tree loss: 4.451 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 4.367 | Reg loss: 0.040 | Tree loss: 4.367 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 4.405 | Reg loss: 0.040 | Tree loss: 4.405 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 4.409 | Reg loss: 0.040 | Tree loss: 4.409 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 4.284 | Reg loss: 0.041 | Tree loss: 4.284 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 4.257 | Reg loss: 0.041 | Tree loss: 4.257 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 4.197 | Reg loss: 0.041 | Tree loss: 4.197 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 4.181 | Reg loss: 0.042 | Tree loss: 4.181 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 4.116 | Reg loss: 0.042 | Tree loss: 4.116 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 4.109 | Reg loss: 0.042 | Tree loss: 4.109 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 4.086 | Reg loss: 0.043 | Tree loss: 4.086 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 4.078 | Reg loss: 0.043 | Tree loss: 4.078 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 4.045 | Reg loss: 0.043 | Tree loss: 4.045 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 4.011 | Reg loss: 0.043 | Tree loss: 4.011 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 3.976 | Reg loss: 0.044 | Tree loss: 3.976 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 3.934 | Reg loss: 0.044 | Tree loss: 3.934 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 3.975 | Reg loss: 0.044 | Tree loss: 3.975 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 3.890 | Reg loss: 0.045 | Tree loss: 3.890 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 3.879 | Reg loss: 0.045 | Tree loss: 3.879 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 4.525 | Reg loss: 0.039 | Tree loss: 4.525 | Accuracy: 0.404297 | 0.896 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 4.501 | Reg loss: 0.039 | Tree loss: 4.501 | Accuracy: 0.376953 | 0.896 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 4.455 | Reg loss: 0.039 | Tree loss: 4.455 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 4.424 | Reg loss: 0.039 | Tree loss: 4.424 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 4.416 | Reg loss: 0.040 | Tree loss: 4.416 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 4.430 | Reg loss: 0.040 | Tree loss: 4.430 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 4.398 | Reg loss: 0.040 | Tree loss: 4.398 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 4.258 | Reg loss: 0.040 | Tree loss: 4.258 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 4.250 | Reg loss: 0.040 | Tree loss: 4.250 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 4.218 | Reg loss: 0.040 | Tree loss: 4.218 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 4.210 | Reg loss: 0.041 | Tree loss: 4.210 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 4.133 | Reg loss: 0.041 | Tree loss: 4.133 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 4.121 | Reg loss: 0.041 | Tree loss: 4.121 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 4.079 | Reg loss: 0.041 | Tree loss: 4.079 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 4.034 | Reg loss: 0.042 | Tree loss: 4.034 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 4.066 | Reg loss: 0.042 | Tree loss: 4.066 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 3.929 | Reg loss: 0.042 | Tree loss: 3.929 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 3.954 | Reg loss: 0.042 | Tree loss: 3.954 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 3.941 | Reg loss: 0.043 | Tree loss: 3.941 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 3.935 | Reg loss: 0.043 | Tree loss: 3.935 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 3.833 | Reg loss: 0.043 | Tree loss: 3.833 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 3.854 | Reg loss: 0.044 | Tree loss: 3.854 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 3.787 | Reg loss: 0.044 | Tree loss: 3.787 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 3.762 | Reg loss: 0.044 | Tree loss: 3.762 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 3.743 | Reg loss: 0.044 | Tree loss: 3.743 | Accuracy: 0.427734 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 3.770 | Reg loss: 0.045 | Tree loss: 3.770 | Accuracy: 0.318359 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 3.756 | Reg loss: 0.045 | Tree loss: 3.756 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 3.694 | Reg loss: 0.045 | Tree loss: 3.694 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 3.694 | Reg loss: 0.045 | Tree loss: 3.694 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 4.342 | Reg loss: 0.040 | Tree loss: 4.342 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 4.282 | Reg loss: 0.040 | Tree loss: 4.282 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 4.283 | Reg loss: 0.040 | Tree loss: 4.283 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 4.210 | Reg loss: 0.040 | Tree loss: 4.210 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 4.185 | Reg loss: 0.040 | Tree loss: 4.185 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 4.137 | Reg loss: 0.041 | Tree loss: 4.137 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 4.110 | Reg loss: 0.041 | Tree loss: 4.110 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 4.081 | Reg loss: 0.041 | Tree loss: 4.081 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 4.065 | Reg loss: 0.041 | Tree loss: 4.065 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 4.005 | Reg loss: 0.041 | Tree loss: 4.005 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 3.965 | Reg loss: 0.041 | Tree loss: 3.965 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 3.996 | Reg loss: 0.042 | Tree loss: 3.996 | Accuracy: 0.345703 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 3.864 | Reg loss: 0.042 | Tree loss: 3.864 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 3.773 | Reg loss: 0.042 | Tree loss: 3.773 | Accuracy: 0.414062 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 3.821 | Reg loss: 0.042 | Tree loss: 3.821 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 3.834 | Reg loss: 0.042 | Tree loss: 3.834 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 3.739 | Reg loss: 0.043 | Tree loss: 3.739 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 3.738 | Reg loss: 0.043 | Tree loss: 3.738 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 3.712 | Reg loss: 0.043 | Tree loss: 3.712 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 3.700 | Reg loss: 0.043 | Tree loss: 3.700 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 3.639 | Reg loss: 0.044 | Tree loss: 3.639 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 3.650 | Reg loss: 0.044 | Tree loss: 3.650 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 3.630 | Reg loss: 0.044 | Tree loss: 3.630 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 3.617 | Reg loss: 0.044 | Tree loss: 3.617 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 3.556 | Reg loss: 0.045 | Tree loss: 3.556 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 3.591 | Reg loss: 0.045 | Tree loss: 3.591 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 3.531 | Reg loss: 0.045 | Tree loss: 3.531 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 3.501 | Reg loss: 0.046 | Tree loss: 3.501 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 3.450 | Reg loss: 0.046 | Tree loss: 3.450 | Accuracy: 0.400673 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 4.115 | Reg loss: 0.041 | Tree loss: 4.115 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 4.078 | Reg loss: 0.041 | Tree loss: 4.078 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 4.035 | Reg loss: 0.041 | Tree loss: 4.035 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 3.997 | Reg loss: 0.041 | Tree loss: 3.997 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 3.979 | Reg loss: 0.041 | Tree loss: 3.979 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 3.961 | Reg loss: 0.041 | Tree loss: 3.961 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 3.951 | Reg loss: 0.041 | Tree loss: 3.951 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 3.841 | Reg loss: 0.042 | Tree loss: 3.841 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 3.844 | Reg loss: 0.042 | Tree loss: 3.844 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 3.782 | Reg loss: 0.042 | Tree loss: 3.782 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 3.740 | Reg loss: 0.042 | Tree loss: 3.740 | Accuracy: 0.437500 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 3.759 | Reg loss: 0.042 | Tree loss: 3.759 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 3.699 | Reg loss: 0.042 | Tree loss: 3.699 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 3.635 | Reg loss: 0.043 | Tree loss: 3.635 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 3.645 | Reg loss: 0.043 | Tree loss: 3.645 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 3.584 | Reg loss: 0.043 | Tree loss: 3.584 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 3.596 | Reg loss: 0.043 | Tree loss: 3.596 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 3.561 | Reg loss: 0.043 | Tree loss: 3.561 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 3.550 | Reg loss: 0.044 | Tree loss: 3.550 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 3.525 | Reg loss: 0.044 | Tree loss: 3.525 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 3.491 | Reg loss: 0.044 | Tree loss: 3.491 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 3.401 | Reg loss: 0.044 | Tree loss: 3.401 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 3.472 | Reg loss: 0.045 | Tree loss: 3.472 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 3.375 | Reg loss: 0.045 | Tree loss: 3.375 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 3.354 | Reg loss: 0.045 | Tree loss: 3.354 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 3.412 | Reg loss: 0.045 | Tree loss: 3.412 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 3.377 | Reg loss: 0.046 | Tree loss: 3.377 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 3.308 | Reg loss: 0.046 | Tree loss: 3.308 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 3.267 | Reg loss: 0.046 | Tree loss: 3.267 | Accuracy: 0.407407 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 3.907 | Reg loss: 0.042 | Tree loss: 3.907 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 3.886 | Reg loss: 0.042 | Tree loss: 3.886 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 3.934 | Reg loss: 0.042 | Tree loss: 3.934 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 3.881 | Reg loss: 0.042 | Tree loss: 3.881 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 3.794 | Reg loss: 0.042 | Tree loss: 3.794 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 3.740 | Reg loss: 0.042 | Tree loss: 3.740 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 3.761 | Reg loss: 0.042 | Tree loss: 3.761 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 3.698 | Reg loss: 0.042 | Tree loss: 3.698 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 3.654 | Reg loss: 0.042 | Tree loss: 3.654 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 3.632 | Reg loss: 0.042 | Tree loss: 3.632 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 3.533 | Reg loss: 0.043 | Tree loss: 3.533 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 3.532 | Reg loss: 0.043 | Tree loss: 3.532 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 3.581 | Reg loss: 0.043 | Tree loss: 3.581 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 3.491 | Reg loss: 0.043 | Tree loss: 3.491 | Accuracy: 0.318359 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 3.470 | Reg loss: 0.043 | Tree loss: 3.470 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 3.394 | Reg loss: 0.044 | Tree loss: 3.394 | Accuracy: 0.365234 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 3.415 | Reg loss: 0.044 | Tree loss: 3.415 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 3.373 | Reg loss: 0.044 | Tree loss: 3.373 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 3.329 | Reg loss: 0.044 | Tree loss: 3.329 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 3.307 | Reg loss: 0.044 | Tree loss: 3.307 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 3.261 | Reg loss: 0.045 | Tree loss: 3.261 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 3.281 | Reg loss: 0.045 | Tree loss: 3.281 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 3.269 | Reg loss: 0.045 | Tree loss: 3.269 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 3.213 | Reg loss: 0.045 | Tree loss: 3.213 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 3.229 | Reg loss: 0.045 | Tree loss: 3.229 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 3.214 | Reg loss: 0.046 | Tree loss: 3.214 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 3.143 | Reg loss: 0.046 | Tree loss: 3.143 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 3.146 | Reg loss: 0.046 | Tree loss: 3.146 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 3.145 | Reg loss: 0.046 | Tree loss: 3.145 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 3.747 | Reg loss: 0.042 | Tree loss: 3.747 | Accuracy: 0.330078 | 0.896 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 3.704 | Reg loss: 0.042 | Tree loss: 3.704 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 3.668 | Reg loss: 0.042 | Tree loss: 3.668 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 3.661 | Reg loss: 0.042 | Tree loss: 3.661 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 3.631 | Reg loss: 0.043 | Tree loss: 3.631 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 3.604 | Reg loss: 0.043 | Tree loss: 3.604 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 3.577 | Reg loss: 0.043 | Tree loss: 3.577 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 3.530 | Reg loss: 0.043 | Tree loss: 3.530 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 3.449 | Reg loss: 0.043 | Tree loss: 3.449 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 3.415 | Reg loss: 0.043 | Tree loss: 3.415 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 3.415 | Reg loss: 0.043 | Tree loss: 3.415 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 3.396 | Reg loss: 0.043 | Tree loss: 3.396 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 3.320 | Reg loss: 0.043 | Tree loss: 3.320 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 3.340 | Reg loss: 0.044 | Tree loss: 3.340 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 3.310 | Reg loss: 0.044 | Tree loss: 3.310 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 3.282 | Reg loss: 0.044 | Tree loss: 3.282 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 3.266 | Reg loss: 0.044 | Tree loss: 3.266 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 3.280 | Reg loss: 0.044 | Tree loss: 3.280 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 3.227 | Reg loss: 0.045 | Tree loss: 3.227 | Accuracy: 0.318359 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 3.150 | Reg loss: 0.045 | Tree loss: 3.150 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 3.165 | Reg loss: 0.045 | Tree loss: 3.165 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 3.132 | Reg loss: 0.045 | Tree loss: 3.132 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 3.091 | Reg loss: 0.045 | Tree loss: 3.091 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 3.092 | Reg loss: 0.046 | Tree loss: 3.092 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 3.021 | Reg loss: 0.046 | Tree loss: 3.021 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 3.054 | Reg loss: 0.046 | Tree loss: 3.054 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 3.002 | Reg loss: 0.046 | Tree loss: 3.002 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 2.999 | Reg loss: 0.046 | Tree loss: 2.999 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 2.983 | Reg loss: 0.047 | Tree loss: 2.983 | Accuracy: 0.367003 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 3.618 | Reg loss: 0.043 | Tree loss: 3.618 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 3.520 | Reg loss: 0.043 | Tree loss: 3.520 | Accuracy: 0.378906 | 0.896 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 3.462 | Reg loss: 0.043 | Tree loss: 3.462 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 3.512 | Reg loss: 0.043 | Tree loss: 3.512 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 3.475 | Reg loss: 0.043 | Tree loss: 3.475 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 3.453 | Reg loss: 0.043 | Tree loss: 3.453 | Accuracy: 0.314453 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 3.375 | Reg loss: 0.043 | Tree loss: 3.375 | Accuracy: 0.417969 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 3.362 | Reg loss: 0.043 | Tree loss: 3.362 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 3.360 | Reg loss: 0.043 | Tree loss: 3.360 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 3.246 | Reg loss: 0.043 | Tree loss: 3.246 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 3.236 | Reg loss: 0.044 | Tree loss: 3.236 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 3.268 | Reg loss: 0.044 | Tree loss: 3.268 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 3.231 | Reg loss: 0.044 | Tree loss: 3.231 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 3.212 | Reg loss: 0.044 | Tree loss: 3.212 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 3.170 | Reg loss: 0.044 | Tree loss: 3.170 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 3.151 | Reg loss: 0.044 | Tree loss: 3.151 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 3.130 | Reg loss: 0.044 | Tree loss: 3.130 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 3.065 | Reg loss: 0.045 | Tree loss: 3.065 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 3.114 | Reg loss: 0.045 | Tree loss: 3.114 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 3.009 | Reg loss: 0.045 | Tree loss: 3.009 | Accuracy: 0.394531 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 3.023 | Reg loss: 0.045 | Tree loss: 3.023 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 2.957 | Reg loss: 0.045 | Tree loss: 2.957 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 3.022 | Reg loss: 0.046 | Tree loss: 3.022 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 2.982 | Reg loss: 0.046 | Tree loss: 2.982 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 2.949 | Reg loss: 0.046 | Tree loss: 2.949 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 2.879 | Reg loss: 0.046 | Tree loss: 2.879 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 2.818 | Reg loss: 0.046 | Tree loss: 2.818 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 2.791 | Reg loss: 0.046 | Tree loss: 2.791 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 2.800 | Reg loss: 0.047 | Tree loss: 2.800 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 3.447 | Reg loss: 0.043 | Tree loss: 3.447 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 3.399 | Reg loss: 0.043 | Tree loss: 3.399 | Accuracy: 0.341797 | 0.896 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 3.404 | Reg loss: 0.043 | Tree loss: 3.404 | Accuracy: 0.347656 | 0.896 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 3.316 | Reg loss: 0.043 | Tree loss: 3.316 | Accuracy: 0.396484 | 0.896 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 3.249 | Reg loss: 0.043 | Tree loss: 3.249 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 3.248 | Reg loss: 0.044 | Tree loss: 3.248 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 3.297 | Reg loss: 0.044 | Tree loss: 3.297 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 3.257 | Reg loss: 0.044 | Tree loss: 3.257 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 3.197 | Reg loss: 0.044 | Tree loss: 3.197 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 3.154 | Reg loss: 0.044 | Tree loss: 3.154 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 3.154 | Reg loss: 0.044 | Tree loss: 3.154 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 3.141 | Reg loss: 0.044 | Tree loss: 3.141 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 3.095 | Reg loss: 0.044 | Tree loss: 3.095 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 3.057 | Reg loss: 0.044 | Tree loss: 3.057 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 2.997 | Reg loss: 0.044 | Tree loss: 2.997 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 2.994 | Reg loss: 0.045 | Tree loss: 2.994 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 2.967 | Reg loss: 0.045 | Tree loss: 2.967 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 2.941 | Reg loss: 0.045 | Tree loss: 2.941 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 2.900 | Reg loss: 0.045 | Tree loss: 2.900 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 2.880 | Reg loss: 0.045 | Tree loss: 2.880 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 2.862 | Reg loss: 0.045 | Tree loss: 2.862 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 2.881 | Reg loss: 0.046 | Tree loss: 2.881 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 2.811 | Reg loss: 0.046 | Tree loss: 2.811 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 2.811 | Reg loss: 0.046 | Tree loss: 2.811 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 2.836 | Reg loss: 0.046 | Tree loss: 2.836 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 2.780 | Reg loss: 0.046 | Tree loss: 2.780 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 2.762 | Reg loss: 0.046 | Tree loss: 2.762 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 2.734 | Reg loss: 0.047 | Tree loss: 2.734 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 2.705 | Reg loss: 0.047 | Tree loss: 2.705 | Accuracy: 0.383838 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 3.260 | Reg loss: 0.044 | Tree loss: 3.260 | Accuracy: 0.380859 | 0.896 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 3.308 | Reg loss: 0.044 | Tree loss: 3.308 | Accuracy: 0.343750 | 0.896 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 3.267 | Reg loss: 0.044 | Tree loss: 3.267 | Accuracy: 0.390625 | 0.896 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 3.177 | Reg loss: 0.044 | Tree loss: 3.177 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 3.179 | Reg loss: 0.044 | Tree loss: 3.179 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 3.219 | Reg loss: 0.044 | Tree loss: 3.219 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 3.112 | Reg loss: 0.044 | Tree loss: 3.112 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 3.062 | Reg loss: 0.044 | Tree loss: 3.062 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 3.065 | Reg loss: 0.044 | Tree loss: 3.065 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 3.048 | Reg loss: 0.044 | Tree loss: 3.048 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 3.011 | Reg loss: 0.044 | Tree loss: 3.011 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 2.956 | Reg loss: 0.044 | Tree loss: 2.956 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 2.991 | Reg loss: 0.044 | Tree loss: 2.991 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 2.935 | Reg loss: 0.045 | Tree loss: 2.935 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 2.906 | Reg loss: 0.045 | Tree loss: 2.906 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 2.855 | Reg loss: 0.045 | Tree loss: 2.855 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 2.866 | Reg loss: 0.045 | Tree loss: 2.866 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 2.762 | Reg loss: 0.045 | Tree loss: 2.762 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 2.813 | Reg loss: 0.045 | Tree loss: 2.813 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 2.748 | Reg loss: 0.045 | Tree loss: 2.748 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 2.796 | Reg loss: 0.046 | Tree loss: 2.796 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 2.732 | Reg loss: 0.046 | Tree loss: 2.732 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 2.782 | Reg loss: 0.046 | Tree loss: 2.782 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 2.690 | Reg loss: 0.046 | Tree loss: 2.690 | Accuracy: 0.365234 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 2.647 | Reg loss: 0.046 | Tree loss: 2.647 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 2.654 | Reg loss: 0.046 | Tree loss: 2.654 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 2.633 | Reg loss: 0.047 | Tree loss: 2.633 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 2.635 | Reg loss: 0.047 | Tree loss: 2.635 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 2.583 | Reg loss: 0.047 | Tree loss: 2.583 | Accuracy: 0.353535 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 3.174 | Reg loss: 0.044 | Tree loss: 3.174 | Accuracy: 0.400391 | 0.896 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 3.099 | Reg loss: 0.044 | Tree loss: 3.099 | Accuracy: 0.351562 | 0.896 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 3.077 | Reg loss: 0.044 | Tree loss: 3.077 | Accuracy: 0.363281 | 0.896 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 3.082 | Reg loss: 0.044 | Tree loss: 3.082 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 3.032 | Reg loss: 0.044 | Tree loss: 3.032 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 3.003 | Reg loss: 0.044 | Tree loss: 3.003 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 2.984 | Reg loss: 0.044 | Tree loss: 2.984 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 2.960 | Reg loss: 0.044 | Tree loss: 2.960 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 2.967 | Reg loss: 0.044 | Tree loss: 2.967 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 2.894 | Reg loss: 0.044 | Tree loss: 2.894 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 2.910 | Reg loss: 0.044 | Tree loss: 2.910 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 2.893 | Reg loss: 0.045 | Tree loss: 2.893 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 2.817 | Reg loss: 0.045 | Tree loss: 2.817 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 2.826 | Reg loss: 0.045 | Tree loss: 2.826 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 2.784 | Reg loss: 0.045 | Tree loss: 2.784 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 2.768 | Reg loss: 0.045 | Tree loss: 2.768 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 2.740 | Reg loss: 0.045 | Tree loss: 2.740 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 2.696 | Reg loss: 0.045 | Tree loss: 2.696 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 2.685 | Reg loss: 0.045 | Tree loss: 2.685 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 2.680 | Reg loss: 0.046 | Tree loss: 2.680 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 2.668 | Reg loss: 0.046 | Tree loss: 2.668 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 2.662 | Reg loss: 0.046 | Tree loss: 2.662 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 2.662 | Reg loss: 0.046 | Tree loss: 2.662 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 2.604 | Reg loss: 0.046 | Tree loss: 2.604 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 2.609 | Reg loss: 0.046 | Tree loss: 2.609 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 2.536 | Reg loss: 0.046 | Tree loss: 2.536 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 2.599 | Reg loss: 0.047 | Tree loss: 2.599 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 2.520 | Reg loss: 0.047 | Tree loss: 2.520 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 2.495 | Reg loss: 0.047 | Tree loss: 2.495 | Accuracy: 0.356902 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 3.043 | Reg loss: 0.044 | Tree loss: 3.043 | Accuracy: 0.337891 | 0.896 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 2.952 | Reg loss: 0.044 | Tree loss: 2.952 | Accuracy: 0.396484 | 0.896 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 3.016 | Reg loss: 0.044 | Tree loss: 3.016 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 2.964 | Reg loss: 0.044 | Tree loss: 2.964 | Accuracy: 0.388672 | 0.896 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 2.949 | Reg loss: 0.044 | Tree loss: 2.949 | Accuracy: 0.359375 | 0.896 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 2.927 | Reg loss: 0.044 | Tree loss: 2.927 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 2.898 | Reg loss: 0.044 | Tree loss: 2.898 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 2.892 | Reg loss: 0.044 | Tree loss: 2.892 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 2.866 | Reg loss: 0.045 | Tree loss: 2.866 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 2.728 | Reg loss: 0.045 | Tree loss: 2.728 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 2.793 | Reg loss: 0.045 | Tree loss: 2.793 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 2.748 | Reg loss: 0.045 | Tree loss: 2.748 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 2.708 | Reg loss: 0.045 | Tree loss: 2.708 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 2.663 | Reg loss: 0.045 | Tree loss: 2.663 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 2.724 | Reg loss: 0.045 | Tree loss: 2.724 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 2.678 | Reg loss: 0.045 | Tree loss: 2.678 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 2.609 | Reg loss: 0.045 | Tree loss: 2.609 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 2.643 | Reg loss: 0.045 | Tree loss: 2.643 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 2.653 | Reg loss: 0.046 | Tree loss: 2.653 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 2.552 | Reg loss: 0.046 | Tree loss: 2.552 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 2.580 | Reg loss: 0.046 | Tree loss: 2.580 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 2.528 | Reg loss: 0.046 | Tree loss: 2.528 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 2.568 | Reg loss: 0.046 | Tree loss: 2.568 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 2.486 | Reg loss: 0.046 | Tree loss: 2.486 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 2.487 | Reg loss: 0.046 | Tree loss: 2.487 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 2.466 | Reg loss: 0.047 | Tree loss: 2.466 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 2.481 | Reg loss: 0.047 | Tree loss: 2.481 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 2.409 | Reg loss: 0.047 | Tree loss: 2.409 | Accuracy: 0.369141 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 2.416 | Reg loss: 0.047 | Tree loss: 2.416 | Accuracy: 0.346801 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 2.917 | Reg loss: 0.044 | Tree loss: 2.917 | Accuracy: 0.361328 | 0.896 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 2.925 | Reg loss: 0.044 | Tree loss: 2.925 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 2.889 | Reg loss: 0.044 | Tree loss: 2.889 | Accuracy: 0.378906 | 0.896 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 2.863 | Reg loss: 0.044 | Tree loss: 2.863 | Accuracy: 0.382812 | 0.896 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 2.855 | Reg loss: 0.044 | Tree loss: 2.855 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 2.811 | Reg loss: 0.044 | Tree loss: 2.811 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 2.818 | Reg loss: 0.045 | Tree loss: 2.818 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 2.714 | Reg loss: 0.045 | Tree loss: 2.714 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 2.756 | Reg loss: 0.045 | Tree loss: 2.756 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 2.699 | Reg loss: 0.045 | Tree loss: 2.699 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 2.690 | Reg loss: 0.045 | Tree loss: 2.690 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 2.704 | Reg loss: 0.045 | Tree loss: 2.704 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 2.593 | Reg loss: 0.045 | Tree loss: 2.593 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 2.545 | Reg loss: 0.045 | Tree loss: 2.545 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 2.613 | Reg loss: 0.045 | Tree loss: 2.613 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 2.539 | Reg loss: 0.045 | Tree loss: 2.539 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 2.512 | Reg loss: 0.045 | Tree loss: 2.512 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 2.559 | Reg loss: 0.046 | Tree loss: 2.559 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 2.476 | Reg loss: 0.046 | Tree loss: 2.476 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 2.467 | Reg loss: 0.046 | Tree loss: 2.467 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 2.478 | Reg loss: 0.046 | Tree loss: 2.478 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 2.492 | Reg loss: 0.046 | Tree loss: 2.492 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 2.428 | Reg loss: 0.046 | Tree loss: 2.428 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 2.418 | Reg loss: 0.046 | Tree loss: 2.418 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 2.441 | Reg loss: 0.046 | Tree loss: 2.441 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 2.443 | Reg loss: 0.047 | Tree loss: 2.443 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 2.348 | Reg loss: 0.047 | Tree loss: 2.348 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 2.382 | Reg loss: 0.047 | Tree loss: 2.382 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 2.403 | Reg loss: 0.047 | Tree loss: 2.403 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 2.845 | Reg loss: 0.045 | Tree loss: 2.845 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 2.830 | Reg loss: 0.045 | Tree loss: 2.830 | Accuracy: 0.388672 | 0.896 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 2.761 | Reg loss: 0.045 | Tree loss: 2.761 | Accuracy: 0.353516 | 0.896 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 2.720 | Reg loss: 0.045 | Tree loss: 2.720 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 2.758 | Reg loss: 0.045 | Tree loss: 2.758 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 2.763 | Reg loss: 0.045 | Tree loss: 2.763 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 2.688 | Reg loss: 0.045 | Tree loss: 2.688 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 2.704 | Reg loss: 0.045 | Tree loss: 2.704 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 2.635 | Reg loss: 0.045 | Tree loss: 2.635 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 2.601 | Reg loss: 0.045 | Tree loss: 2.601 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 2.557 | Reg loss: 0.045 | Tree loss: 2.557 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 2.582 | Reg loss: 0.045 | Tree loss: 2.582 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 2.544 | Reg loss: 0.045 | Tree loss: 2.544 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 2.529 | Reg loss: 0.045 | Tree loss: 2.529 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 2.537 | Reg loss: 0.045 | Tree loss: 2.537 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 2.504 | Reg loss: 0.045 | Tree loss: 2.504 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 2.455 | Reg loss: 0.045 | Tree loss: 2.455 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 2.470 | Reg loss: 0.046 | Tree loss: 2.470 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 2.420 | Reg loss: 0.046 | Tree loss: 2.420 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 2.378 | Reg loss: 0.046 | Tree loss: 2.378 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 2.440 | Reg loss: 0.046 | Tree loss: 2.440 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 2.352 | Reg loss: 0.046 | Tree loss: 2.352 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 2.402 | Reg loss: 0.046 | Tree loss: 2.402 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 2.318 | Reg loss: 0.046 | Tree loss: 2.318 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 2.362 | Reg loss: 0.046 | Tree loss: 2.362 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 2.279 | Reg loss: 0.047 | Tree loss: 2.279 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 2.286 | Reg loss: 0.047 | Tree loss: 2.286 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 2.295 | Reg loss: 0.047 | Tree loss: 2.295 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 2.283 | Reg loss: 0.047 | Tree loss: 2.283 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 2.754 | Reg loss: 0.045 | Tree loss: 2.754 | Accuracy: 0.351562 | 0.896 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 2.697 | Reg loss: 0.045 | Tree loss: 2.697 | Accuracy: 0.412109 | 0.896 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 2.714 | Reg loss: 0.045 | Tree loss: 2.714 | Accuracy: 0.378906 | 0.896 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 2.652 | Reg loss: 0.045 | Tree loss: 2.652 | Accuracy: 0.410156 | 0.896 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 2.645 | Reg loss: 0.045 | Tree loss: 2.645 | Accuracy: 0.373047 | 0.896 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 2.566 | Reg loss: 0.045 | Tree loss: 2.566 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 2.647 | Reg loss: 0.045 | Tree loss: 2.647 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 2.598 | Reg loss: 0.045 | Tree loss: 2.598 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 2.551 | Reg loss: 0.045 | Tree loss: 2.551 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 2.524 | Reg loss: 0.045 | Tree loss: 2.524 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 2.565 | Reg loss: 0.045 | Tree loss: 2.565 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 2.472 | Reg loss: 0.045 | Tree loss: 2.472 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 2.547 | Reg loss: 0.045 | Tree loss: 2.547 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 2.475 | Reg loss: 0.045 | Tree loss: 2.475 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 2.389 | Reg loss: 0.045 | Tree loss: 2.389 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 2.393 | Reg loss: 0.045 | Tree loss: 2.393 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 2.439 | Reg loss: 0.045 | Tree loss: 2.439 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 2.431 | Reg loss: 0.046 | Tree loss: 2.431 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 2.350 | Reg loss: 0.046 | Tree loss: 2.350 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 2.291 | Reg loss: 0.046 | Tree loss: 2.291 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 2.308 | Reg loss: 0.046 | Tree loss: 2.308 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 2.318 | Reg loss: 0.046 | Tree loss: 2.318 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 2.263 | Reg loss: 0.046 | Tree loss: 2.263 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 2.315 | Reg loss: 0.046 | Tree loss: 2.315 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 2.290 | Reg loss: 0.046 | Tree loss: 2.290 | Accuracy: 0.306641 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 2.234 | Reg loss: 0.046 | Tree loss: 2.234 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 2.230 | Reg loss: 0.047 | Tree loss: 2.230 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 2.246 | Reg loss: 0.047 | Tree loss: 2.246 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 2.178 | Reg loss: 0.047 | Tree loss: 2.178 | Accuracy: 0.373737 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 2.697 | Reg loss: 0.045 | Tree loss: 2.697 | Accuracy: 0.318359 | 0.896 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 2.598 | Reg loss: 0.045 | Tree loss: 2.598 | Accuracy: 0.402344 | 0.896 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 2.627 | Reg loss: 0.045 | Tree loss: 2.627 | Accuracy: 0.396484 | 0.896 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 2.576 | Reg loss: 0.045 | Tree loss: 2.576 | Accuracy: 0.371094 | 0.896 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 2.615 | Reg loss: 0.045 | Tree loss: 2.615 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 2.553 | Reg loss: 0.045 | Tree loss: 2.553 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 2.525 | Reg loss: 0.045 | Tree loss: 2.525 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 2.496 | Reg loss: 0.045 | Tree loss: 2.496 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 2.429 | Reg loss: 0.045 | Tree loss: 2.429 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 2.522 | Reg loss: 0.045 | Tree loss: 2.522 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 2.465 | Reg loss: 0.045 | Tree loss: 2.465 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 2.427 | Reg loss: 0.045 | Tree loss: 2.427 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 2.417 | Reg loss: 0.045 | Tree loss: 2.417 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 2.408 | Reg loss: 0.045 | Tree loss: 2.408 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 2.385 | Reg loss: 0.045 | Tree loss: 2.385 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 2.385 | Reg loss: 0.045 | Tree loss: 2.385 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 2.295 | Reg loss: 0.045 | Tree loss: 2.295 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 2.298 | Reg loss: 0.046 | Tree loss: 2.298 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 2.311 | Reg loss: 0.046 | Tree loss: 2.311 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 2.265 | Reg loss: 0.046 | Tree loss: 2.265 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 2.304 | Reg loss: 0.046 | Tree loss: 2.304 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 2.203 | Reg loss: 0.046 | Tree loss: 2.203 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 2.266 | Reg loss: 0.046 | Tree loss: 2.266 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 2.188 | Reg loss: 0.046 | Tree loss: 2.188 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 2.195 | Reg loss: 0.046 | Tree loss: 2.195 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 2.167 | Reg loss: 0.046 | Tree loss: 2.167 | Accuracy: 0.318359 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 2.190 | Reg loss: 0.046 | Tree loss: 2.190 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 2.135 | Reg loss: 0.047 | Tree loss: 2.135 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 2.144 | Reg loss: 0.047 | Tree loss: 2.144 | Accuracy: 0.367003 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 2.615 | Reg loss: 0.045 | Tree loss: 2.615 | Accuracy: 0.365234 | 0.896 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 2.577 | Reg loss: 0.045 | Tree loss: 2.577 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 2.497 | Reg loss: 0.045 | Tree loss: 2.497 | Accuracy: 0.369141 | 0.896 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 2.579 | Reg loss: 0.045 | Tree loss: 2.579 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 2.551 | Reg loss: 0.045 | Tree loss: 2.551 | Accuracy: 0.363281 | 0.896 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 2.495 | Reg loss: 0.045 | Tree loss: 2.495 | Accuracy: 0.357422 | 0.896 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 2.489 | Reg loss: 0.045 | Tree loss: 2.489 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 2.449 | Reg loss: 0.045 | Tree loss: 2.449 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 2.458 | Reg loss: 0.045 | Tree loss: 2.458 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 2.331 | Reg loss: 0.045 | Tree loss: 2.331 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 2.315 | Reg loss: 0.045 | Tree loss: 2.315 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 2.320 | Reg loss: 0.045 | Tree loss: 2.320 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 2.328 | Reg loss: 0.045 | Tree loss: 2.328 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 2.318 | Reg loss: 0.045 | Tree loss: 2.318 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 2.259 | Reg loss: 0.045 | Tree loss: 2.259 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 2.348 | Reg loss: 0.045 | Tree loss: 2.348 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 2.251 | Reg loss: 0.045 | Tree loss: 2.251 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 2.228 | Reg loss: 0.045 | Tree loss: 2.228 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 2.233 | Reg loss: 0.046 | Tree loss: 2.233 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 2.217 | Reg loss: 0.046 | Tree loss: 2.217 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 2.210 | Reg loss: 0.046 | Tree loss: 2.210 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 2.154 | Reg loss: 0.046 | Tree loss: 2.154 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 2.192 | Reg loss: 0.046 | Tree loss: 2.192 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 2.115 | Reg loss: 0.046 | Tree loss: 2.115 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 2.120 | Reg loss: 0.046 | Tree loss: 2.120 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 2.204 | Reg loss: 0.046 | Tree loss: 2.204 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 2.125 | Reg loss: 0.046 | Tree loss: 2.125 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 2.145 | Reg loss: 0.046 | Tree loss: 2.145 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 2.150 | Reg loss: 0.046 | Tree loss: 2.150 | Accuracy: 0.343434 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 2.539 | Reg loss: 0.045 | Tree loss: 2.539 | Accuracy: 0.375000 | 0.896 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 2.536 | Reg loss: 0.045 | Tree loss: 2.536 | Accuracy: 0.326172 | 0.896 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 2.487 | Reg loss: 0.045 | Tree loss: 2.487 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 2.428 | Reg loss: 0.045 | Tree loss: 2.428 | Accuracy: 0.419922 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 2.429 | Reg loss: 0.045 | Tree loss: 2.429 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 2.401 | Reg loss: 0.045 | Tree loss: 2.401 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 2.398 | Reg loss: 0.045 | Tree loss: 2.398 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 2.442 | Reg loss: 0.045 | Tree loss: 2.442 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 2.431 | Reg loss: 0.045 | Tree loss: 2.431 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 2.369 | Reg loss: 0.045 | Tree loss: 2.369 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 2.311 | Reg loss: 0.045 | Tree loss: 2.311 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 2.291 | Reg loss: 0.045 | Tree loss: 2.291 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 2.327 | Reg loss: 0.045 | Tree loss: 2.327 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 2.239 | Reg loss: 0.045 | Tree loss: 2.239 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 2.247 | Reg loss: 0.045 | Tree loss: 2.247 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 2.239 | Reg loss: 0.045 | Tree loss: 2.239 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 2.201 | Reg loss: 0.045 | Tree loss: 2.201 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 2.177 | Reg loss: 0.045 | Tree loss: 2.177 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 2.179 | Reg loss: 0.045 | Tree loss: 2.179 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 2.192 | Reg loss: 0.046 | Tree loss: 2.192 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 2.137 | Reg loss: 0.046 | Tree loss: 2.137 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 2.145 | Reg loss: 0.046 | Tree loss: 2.145 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 2.123 | Reg loss: 0.046 | Tree loss: 2.123 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 2.121 | Reg loss: 0.046 | Tree loss: 2.121 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 2.098 | Reg loss: 0.046 | Tree loss: 2.098 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 2.029 | Reg loss: 0.046 | Tree loss: 2.029 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 2.041 | Reg loss: 0.046 | Tree loss: 2.041 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 2.044 | Reg loss: 0.046 | Tree loss: 2.044 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 1.932 | Reg loss: 0.046 | Tree loss: 1.932 | Accuracy: 0.434343 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 2.461 | Reg loss: 0.045 | Tree loss: 2.461 | Accuracy: 0.384766 | 0.896 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 2.400 | Reg loss: 0.045 | Tree loss: 2.400 | Accuracy: 0.388672 | 0.896 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 2.418 | Reg loss: 0.045 | Tree loss: 2.418 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 2.396 | Reg loss: 0.045 | Tree loss: 2.396 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 2.349 | Reg loss: 0.045 | Tree loss: 2.349 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 2.402 | Reg loss: 0.045 | Tree loss: 2.402 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 2.350 | Reg loss: 0.045 | Tree loss: 2.350 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 2.309 | Reg loss: 0.045 | Tree loss: 2.309 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 2.280 | Reg loss: 0.045 | Tree loss: 2.280 | Accuracy: 0.369141 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 2.355 | Reg loss: 0.045 | Tree loss: 2.355 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 2.281 | Reg loss: 0.045 | Tree loss: 2.281 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 2.303 | Reg loss: 0.045 | Tree loss: 2.303 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 2.235 | Reg loss: 0.045 | Tree loss: 2.235 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 2.179 | Reg loss: 0.045 | Tree loss: 2.179 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 2.247 | Reg loss: 0.045 | Tree loss: 2.247 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 2.167 | Reg loss: 0.045 | Tree loss: 2.167 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 2.199 | Reg loss: 0.045 | Tree loss: 2.199 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 2.173 | Reg loss: 0.045 | Tree loss: 2.173 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 2.122 | Reg loss: 0.045 | Tree loss: 2.122 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 2.132 | Reg loss: 0.045 | Tree loss: 2.132 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 2.072 | Reg loss: 0.046 | Tree loss: 2.072 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 2.001 | Reg loss: 0.046 | Tree loss: 2.001 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 2.017 | Reg loss: 0.046 | Tree loss: 2.017 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 2.090 | Reg loss: 0.046 | Tree loss: 2.090 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 2.074 | Reg loss: 0.046 | Tree loss: 2.074 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 2.009 | Reg loss: 0.046 | Tree loss: 2.009 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 2.036 | Reg loss: 0.046 | Tree loss: 2.036 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 2.023 | Reg loss: 0.046 | Tree loss: 2.023 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 2.013 | Reg loss: 0.046 | Tree loss: 2.013 | Accuracy: 0.367003 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 2.371 | Reg loss: 0.044 | Tree loss: 2.371 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 2.400 | Reg loss: 0.044 | Tree loss: 2.400 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 2.363 | Reg loss: 0.044 | Tree loss: 2.363 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 2.301 | Reg loss: 0.044 | Tree loss: 2.301 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 2.367 | Reg loss: 0.044 | Tree loss: 2.367 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 2.311 | Reg loss: 0.044 | Tree loss: 2.311 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 2.297 | Reg loss: 0.044 | Tree loss: 2.297 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 2.247 | Reg loss: 0.045 | Tree loss: 2.247 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 2.257 | Reg loss: 0.045 | Tree loss: 2.257 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 2.192 | Reg loss: 0.045 | Tree loss: 2.192 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 2.228 | Reg loss: 0.045 | Tree loss: 2.228 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 2.214 | Reg loss: 0.045 | Tree loss: 2.214 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 2.196 | Reg loss: 0.045 | Tree loss: 2.196 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 2.237 | Reg loss: 0.045 | Tree loss: 2.237 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 2.142 | Reg loss: 0.045 | Tree loss: 2.142 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 2.161 | Reg loss: 0.045 | Tree loss: 2.161 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 2.082 | Reg loss: 0.045 | Tree loss: 2.082 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 2.070 | Reg loss: 0.045 | Tree loss: 2.070 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 2.110 | Reg loss: 0.045 | Tree loss: 2.110 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 2.058 | Reg loss: 0.045 | Tree loss: 2.058 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 2.084 | Reg loss: 0.045 | Tree loss: 2.084 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 1.985 | Reg loss: 0.045 | Tree loss: 1.985 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 2.085 | Reg loss: 0.046 | Tree loss: 2.085 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 2.066 | Reg loss: 0.046 | Tree loss: 2.066 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 2.022 | Reg loss: 0.046 | Tree loss: 2.022 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 2.000 | Reg loss: 0.046 | Tree loss: 2.000 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 1.946 | Reg loss: 0.046 | Tree loss: 1.946 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 1.927 | Reg loss: 0.046 | Tree loss: 1.927 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 2.005 | Reg loss: 0.046 | Tree loss: 2.005 | Accuracy: 0.387205 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 2.328 | Reg loss: 0.044 | Tree loss: 2.328 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 2.348 | Reg loss: 0.044 | Tree loss: 2.348 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 2.292 | Reg loss: 0.044 | Tree loss: 2.292 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 2.307 | Reg loss: 0.044 | Tree loss: 2.307 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 2.293 | Reg loss: 0.044 | Tree loss: 2.293 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 2.190 | Reg loss: 0.044 | Tree loss: 2.190 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 2.219 | Reg loss: 0.044 | Tree loss: 2.219 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 2.233 | Reg loss: 0.044 | Tree loss: 2.233 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 2.253 | Reg loss: 0.044 | Tree loss: 2.253 | Accuracy: 0.414062 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 2.243 | Reg loss: 0.044 | Tree loss: 2.243 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 2.137 | Reg loss: 0.045 | Tree loss: 2.137 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 2.164 | Reg loss: 0.045 | Tree loss: 2.164 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 2.178 | Reg loss: 0.045 | Tree loss: 2.178 | Accuracy: 0.400391 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 2.138 | Reg loss: 0.045 | Tree loss: 2.138 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 2.105 | Reg loss: 0.045 | Tree loss: 2.105 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 2.078 | Reg loss: 0.045 | Tree loss: 2.078 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 2.043 | Reg loss: 0.045 | Tree loss: 2.043 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 2.100 | Reg loss: 0.045 | Tree loss: 2.100 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 2.037 | Reg loss: 0.045 | Tree loss: 2.037 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 2.031 | Reg loss: 0.045 | Tree loss: 2.031 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 2.031 | Reg loss: 0.045 | Tree loss: 2.031 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 1.984 | Reg loss: 0.045 | Tree loss: 1.984 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 1.974 | Reg loss: 0.045 | Tree loss: 1.974 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 2.009 | Reg loss: 0.045 | Tree loss: 2.009 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 1.969 | Reg loss: 0.045 | Tree loss: 1.969 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 1.969 | Reg loss: 0.046 | Tree loss: 1.969 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 1.944 | Reg loss: 0.046 | Tree loss: 1.944 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 1.912 | Reg loss: 0.046 | Tree loss: 1.912 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 1.960 | Reg loss: 0.046 | Tree loss: 1.960 | Accuracy: 0.397306 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 2.338 | Reg loss: 0.044 | Tree loss: 2.338 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 2.275 | Reg loss: 0.044 | Tree loss: 2.275 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 2.333 | Reg loss: 0.044 | Tree loss: 2.333 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 2.246 | Reg loss: 0.044 | Tree loss: 2.246 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 2.238 | Reg loss: 0.044 | Tree loss: 2.238 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 2.164 | Reg loss: 0.044 | Tree loss: 2.164 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 2.216 | Reg loss: 0.044 | Tree loss: 2.216 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 2.235 | Reg loss: 0.044 | Tree loss: 2.235 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 2.152 | Reg loss: 0.044 | Tree loss: 2.152 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 2.160 | Reg loss: 0.044 | Tree loss: 2.160 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 2.147 | Reg loss: 0.044 | Tree loss: 2.147 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 2.124 | Reg loss: 0.044 | Tree loss: 2.124 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 2.065 | Reg loss: 0.044 | Tree loss: 2.065 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 2.042 | Reg loss: 0.045 | Tree loss: 2.042 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 2.053 | Reg loss: 0.045 | Tree loss: 2.053 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 2.048 | Reg loss: 0.045 | Tree loss: 2.048 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 2.016 | Reg loss: 0.045 | Tree loss: 2.016 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 2.028 | Reg loss: 0.045 | Tree loss: 2.028 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 2.048 | Reg loss: 0.045 | Tree loss: 2.048 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 1.957 | Reg loss: 0.045 | Tree loss: 1.957 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 2.011 | Reg loss: 0.045 | Tree loss: 2.011 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 2.004 | Reg loss: 0.045 | Tree loss: 2.004 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 1.952 | Reg loss: 0.045 | Tree loss: 1.952 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 1.913 | Reg loss: 0.045 | Tree loss: 1.913 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 1.932 | Reg loss: 0.045 | Tree loss: 1.932 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 1.887 | Reg loss: 0.045 | Tree loss: 1.887 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 1.933 | Reg loss: 0.045 | Tree loss: 1.933 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 1.897 | Reg loss: 0.045 | Tree loss: 1.897 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 1.917 | Reg loss: 0.046 | Tree loss: 1.917 | Accuracy: 0.380471 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 2.276 | Reg loss: 0.044 | Tree loss: 2.276 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 2.191 | Reg loss: 0.044 | Tree loss: 2.191 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 2.229 | Reg loss: 0.044 | Tree loss: 2.229 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 2.242 | Reg loss: 0.044 | Tree loss: 2.242 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 2.179 | Reg loss: 0.044 | Tree loss: 2.179 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 2.224 | Reg loss: 0.044 | Tree loss: 2.224 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 2.147 | Reg loss: 0.044 | Tree loss: 2.147 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 2.100 | Reg loss: 0.044 | Tree loss: 2.100 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 2.092 | Reg loss: 0.044 | Tree loss: 2.092 | Accuracy: 0.414062 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 2.111 | Reg loss: 0.044 | Tree loss: 2.111 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 2.129 | Reg loss: 0.044 | Tree loss: 2.129 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 2.088 | Reg loss: 0.044 | Tree loss: 2.088 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 2.126 | Reg loss: 0.044 | Tree loss: 2.126 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 2.018 | Reg loss: 0.044 | Tree loss: 2.018 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 2.062 | Reg loss: 0.044 | Tree loss: 2.062 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 1.999 | Reg loss: 0.044 | Tree loss: 1.999 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 1.992 | Reg loss: 0.045 | Tree loss: 1.992 | Accuracy: 0.341797 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 1.962 | Reg loss: 0.045 | Tree loss: 1.962 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 1.985 | Reg loss: 0.045 | Tree loss: 1.985 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 1.969 | Reg loss: 0.045 | Tree loss: 1.969 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 1.953 | Reg loss: 0.045 | Tree loss: 1.953 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 1.938 | Reg loss: 0.045 | Tree loss: 1.938 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 1.978 | Reg loss: 0.045 | Tree loss: 1.978 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 1.913 | Reg loss: 0.045 | Tree loss: 1.913 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 1.948 | Reg loss: 0.045 | Tree loss: 1.948 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 1.849 | Reg loss: 0.045 | Tree loss: 1.849 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 1.874 | Reg loss: 0.045 | Tree loss: 1.874 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 1.858 | Reg loss: 0.045 | Tree loss: 1.858 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 1.872 | Reg loss: 0.045 | Tree loss: 1.872 | Accuracy: 0.370370 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 2.250 | Reg loss: 0.044 | Tree loss: 2.250 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 2.159 | Reg loss: 0.044 | Tree loss: 2.159 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 2.171 | Reg loss: 0.044 | Tree loss: 2.171 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 2.205 | Reg loss: 0.044 | Tree loss: 2.205 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 2.124 | Reg loss: 0.044 | Tree loss: 2.124 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 2.154 | Reg loss: 0.044 | Tree loss: 2.154 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 2.160 | Reg loss: 0.044 | Tree loss: 2.160 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 2.143 | Reg loss: 0.044 | Tree loss: 2.143 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 2.114 | Reg loss: 0.044 | Tree loss: 2.114 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 2.123 | Reg loss: 0.044 | Tree loss: 2.123 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 2.093 | Reg loss: 0.044 | Tree loss: 2.093 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 2.016 | Reg loss: 0.044 | Tree loss: 2.016 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 2.022 | Reg loss: 0.044 | Tree loss: 2.022 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 2.032 | Reg loss: 0.044 | Tree loss: 2.032 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 1.970 | Reg loss: 0.044 | Tree loss: 1.970 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 1.958 | Reg loss: 0.044 | Tree loss: 1.958 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 2.028 | Reg loss: 0.044 | Tree loss: 2.028 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 1.968 | Reg loss: 0.044 | Tree loss: 1.968 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 1.945 | Reg loss: 0.044 | Tree loss: 1.945 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 1.925 | Reg loss: 0.045 | Tree loss: 1.925 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 1.938 | Reg loss: 0.045 | Tree loss: 1.938 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 1.827 | Reg loss: 0.045 | Tree loss: 1.827 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 1.899 | Reg loss: 0.045 | Tree loss: 1.899 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 1.875 | Reg loss: 0.045 | Tree loss: 1.875 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 1.908 | Reg loss: 0.045 | Tree loss: 1.908 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 1.831 | Reg loss: 0.045 | Tree loss: 1.831 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 1.849 | Reg loss: 0.045 | Tree loss: 1.849 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 1.811 | Reg loss: 0.045 | Tree loss: 1.811 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 1.887 | Reg loss: 0.045 | Tree loss: 1.887 | Accuracy: 0.400673 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 2.210 | Reg loss: 0.044 | Tree loss: 2.210 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 2.166 | Reg loss: 0.044 | Tree loss: 2.166 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 2.196 | Reg loss: 0.044 | Tree loss: 2.196 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 2.110 | Reg loss: 0.044 | Tree loss: 2.110 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 2.113 | Reg loss: 0.044 | Tree loss: 2.113 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 2.152 | Reg loss: 0.044 | Tree loss: 2.152 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 2.108 | Reg loss: 0.044 | Tree loss: 2.108 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 2.105 | Reg loss: 0.044 | Tree loss: 2.105 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 2.088 | Reg loss: 0.044 | Tree loss: 2.088 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 1.996 | Reg loss: 0.044 | Tree loss: 1.996 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 1.995 | Reg loss: 0.044 | Tree loss: 1.995 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 1.977 | Reg loss: 0.044 | Tree loss: 1.977 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 1.971 | Reg loss: 0.044 | Tree loss: 1.971 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 1.999 | Reg loss: 0.044 | Tree loss: 1.999 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 1.961 | Reg loss: 0.044 | Tree loss: 1.961 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 1.946 | Reg loss: 0.044 | Tree loss: 1.946 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 1.946 | Reg loss: 0.044 | Tree loss: 1.946 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 1.963 | Reg loss: 0.044 | Tree loss: 1.963 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 1.939 | Reg loss: 0.044 | Tree loss: 1.939 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 1.873 | Reg loss: 0.044 | Tree loss: 1.873 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 1.946 | Reg loss: 0.044 | Tree loss: 1.946 | Accuracy: 0.361328 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 1.835 | Reg loss: 0.044 | Tree loss: 1.835 | Accuracy: 0.414062 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 1.868 | Reg loss: 0.045 | Tree loss: 1.868 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 1.834 | Reg loss: 0.045 | Tree loss: 1.834 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 1.900 | Reg loss: 0.045 | Tree loss: 1.900 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 1.855 | Reg loss: 0.045 | Tree loss: 1.855 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 1.814 | Reg loss: 0.045 | Tree loss: 1.814 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 1.831 | Reg loss: 0.045 | Tree loss: 1.831 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 1.801 | Reg loss: 0.045 | Tree loss: 1.801 | Accuracy: 0.390572 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 2.151 | Reg loss: 0.043 | Tree loss: 2.151 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 2.141 | Reg loss: 0.043 | Tree loss: 2.141 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 2.168 | Reg loss: 0.043 | Tree loss: 2.168 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 2.123 | Reg loss: 0.043 | Tree loss: 2.123 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 2.055 | Reg loss: 0.043 | Tree loss: 2.055 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 2.097 | Reg loss: 0.043 | Tree loss: 2.097 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 2.036 | Reg loss: 0.043 | Tree loss: 2.036 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 2.032 | Reg loss: 0.043 | Tree loss: 2.032 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 1.997 | Reg loss: 0.043 | Tree loss: 1.997 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 2.040 | Reg loss: 0.044 | Tree loss: 2.040 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 2.015 | Reg loss: 0.044 | Tree loss: 2.015 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 2.039 | Reg loss: 0.044 | Tree loss: 2.039 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 1.935 | Reg loss: 0.044 | Tree loss: 1.935 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 1.982 | Reg loss: 0.044 | Tree loss: 1.982 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 1.944 | Reg loss: 0.044 | Tree loss: 1.944 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 1.905 | Reg loss: 0.044 | Tree loss: 1.905 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 1.953 | Reg loss: 0.044 | Tree loss: 1.953 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 1.859 | Reg loss: 0.044 | Tree loss: 1.859 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 1.889 | Reg loss: 0.044 | Tree loss: 1.889 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 1.883 | Reg loss: 0.044 | Tree loss: 1.883 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 1.863 | Reg loss: 0.044 | Tree loss: 1.863 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 1.868 | Reg loss: 0.044 | Tree loss: 1.868 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 1.831 | Reg loss: 0.044 | Tree loss: 1.831 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 1.787 | Reg loss: 0.044 | Tree loss: 1.787 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 1.838 | Reg loss: 0.044 | Tree loss: 1.838 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 1.865 | Reg loss: 0.044 | Tree loss: 1.865 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 1.789 | Reg loss: 0.044 | Tree loss: 1.789 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 1.834 | Reg loss: 0.045 | Tree loss: 1.834 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 1.783 | Reg loss: 0.045 | Tree loss: 1.783 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 2.169 | Reg loss: 0.043 | Tree loss: 2.169 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 2.163 | Reg loss: 0.043 | Tree loss: 2.163 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 2.116 | Reg loss: 0.043 | Tree loss: 2.116 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 2.123 | Reg loss: 0.043 | Tree loss: 2.123 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 2.096 | Reg loss: 0.043 | Tree loss: 2.096 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 2.066 | Reg loss: 0.043 | Tree loss: 2.066 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 2.078 | Reg loss: 0.043 | Tree loss: 2.078 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 2.025 | Reg loss: 0.043 | Tree loss: 2.025 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 1.988 | Reg loss: 0.043 | Tree loss: 1.988 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 1.975 | Reg loss: 0.043 | Tree loss: 1.975 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 1.972 | Reg loss: 0.043 | Tree loss: 1.972 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 1.984 | Reg loss: 0.043 | Tree loss: 1.984 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 1.911 | Reg loss: 0.043 | Tree loss: 1.911 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 1.869 | Reg loss: 0.044 | Tree loss: 1.869 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 1.928 | Reg loss: 0.044 | Tree loss: 1.928 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 1.926 | Reg loss: 0.044 | Tree loss: 1.926 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 1.886 | Reg loss: 0.044 | Tree loss: 1.886 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 1.870 | Reg loss: 0.044 | Tree loss: 1.870 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 1.904 | Reg loss: 0.044 | Tree loss: 1.904 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 1.802 | Reg loss: 0.044 | Tree loss: 1.802 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 1.794 | Reg loss: 0.044 | Tree loss: 1.794 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 1.817 | Reg loss: 0.044 | Tree loss: 1.817 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 1.817 | Reg loss: 0.044 | Tree loss: 1.817 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 1.813 | Reg loss: 0.044 | Tree loss: 1.813 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 1.819 | Reg loss: 0.044 | Tree loss: 1.819 | Accuracy: 0.351562 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 1.800 | Reg loss: 0.044 | Tree loss: 1.800 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 1.774 | Reg loss: 0.044 | Tree loss: 1.774 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 1.786 | Reg loss: 0.044 | Tree loss: 1.786 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 1.685 | Reg loss: 0.044 | Tree loss: 1.685 | Accuracy: 0.410774 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 2.161 | Reg loss: 0.043 | Tree loss: 2.161 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 2.058 | Reg loss: 0.043 | Tree loss: 2.058 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 2.096 | Reg loss: 0.043 | Tree loss: 2.096 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 2.013 | Reg loss: 0.043 | Tree loss: 2.013 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 2.041 | Reg loss: 0.043 | Tree loss: 2.041 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 2.087 | Reg loss: 0.043 | Tree loss: 2.087 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 2.075 | Reg loss: 0.043 | Tree loss: 2.075 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 2.030 | Reg loss: 0.043 | Tree loss: 2.030 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 1.934 | Reg loss: 0.043 | Tree loss: 1.934 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 1.948 | Reg loss: 0.043 | Tree loss: 1.948 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 1.993 | Reg loss: 0.043 | Tree loss: 1.993 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 1.906 | Reg loss: 0.043 | Tree loss: 1.906 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 1.923 | Reg loss: 0.043 | Tree loss: 1.923 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 1.938 | Reg loss: 0.043 | Tree loss: 1.938 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 1.902 | Reg loss: 0.043 | Tree loss: 1.902 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 1.823 | Reg loss: 0.044 | Tree loss: 1.823 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 1.863 | Reg loss: 0.044 | Tree loss: 1.863 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 1.856 | Reg loss: 0.044 | Tree loss: 1.856 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 1.794 | Reg loss: 0.044 | Tree loss: 1.794 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 1.792 | Reg loss: 0.044 | Tree loss: 1.792 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 1.762 | Reg loss: 0.044 | Tree loss: 1.762 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 1.812 | Reg loss: 0.044 | Tree loss: 1.812 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 1.748 | Reg loss: 0.044 | Tree loss: 1.748 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 1.752 | Reg loss: 0.044 | Tree loss: 1.752 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 1.729 | Reg loss: 0.044 | Tree loss: 1.729 | Accuracy: 0.367003 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 2.057 | Reg loss: 0.043 | Tree loss: 2.057 | Accuracy: 0.419922 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 2.098 | Reg loss: 0.043 | Tree loss: 2.098 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 2.031 | Reg loss: 0.043 | Tree loss: 2.031 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 2.042 | Reg loss: 0.043 | Tree loss: 2.042 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 2.056 | Reg loss: 0.043 | Tree loss: 2.056 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 2.040 | Reg loss: 0.043 | Tree loss: 2.040 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 2.017 | Reg loss: 0.043 | Tree loss: 2.017 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 1.985 | Reg loss: 0.043 | Tree loss: 1.985 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 1.937 | Reg loss: 0.043 | Tree loss: 1.937 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 1.925 | Reg loss: 0.043 | Tree loss: 1.925 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 1.966 | Reg loss: 0.043 | Tree loss: 1.966 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 1.917 | Reg loss: 0.043 | Tree loss: 1.917 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 1.905 | Reg loss: 0.043 | Tree loss: 1.905 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 1.916 | Reg loss: 0.043 | Tree loss: 1.916 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 1.871 | Reg loss: 0.043 | Tree loss: 1.871 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 1.863 | Reg loss: 0.043 | Tree loss: 1.863 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 1.752 | Reg loss: 0.044 | Tree loss: 1.752 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 1.825 | Reg loss: 0.044 | Tree loss: 1.825 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 1.745 | Reg loss: 0.044 | Tree loss: 1.745 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 1.788 | Reg loss: 0.044 | Tree loss: 1.788 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 1.694 | Reg loss: 0.044 | Tree loss: 1.694 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 1.718 | Reg loss: 0.044 | Tree loss: 1.718 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 1.737 | Reg loss: 0.044 | Tree loss: 1.737 | Accuracy: 0.373737 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 2.086 | Reg loss: 0.042 | Tree loss: 2.086 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 2.055 | Reg loss: 0.042 | Tree loss: 2.055 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 2.040 | Reg loss: 0.042 | Tree loss: 2.040 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 2.035 | Reg loss: 0.042 | Tree loss: 2.035 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 2.018 | Reg loss: 0.042 | Tree loss: 2.018 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 1.989 | Reg loss: 0.042 | Tree loss: 1.989 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 1.976 | Reg loss: 0.042 | Tree loss: 1.976 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 1.986 | Reg loss: 0.042 | Tree loss: 1.986 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 1.978 | Reg loss: 0.043 | Tree loss: 1.978 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 1.958 | Reg loss: 0.043 | Tree loss: 1.958 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 1.875 | Reg loss: 0.043 | Tree loss: 1.875 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 1.877 | Reg loss: 0.043 | Tree loss: 1.877 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 1.869 | Reg loss: 0.043 | Tree loss: 1.869 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 1.860 | Reg loss: 0.043 | Tree loss: 1.860 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 1.821 | Reg loss: 0.043 | Tree loss: 1.821 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 1.779 | Reg loss: 0.044 | Tree loss: 1.779 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 1.653 | Reg loss: 0.044 | Tree loss: 1.653 | Accuracy: 0.424242 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 2.085 | Reg loss: 0.042 | Tree loss: 2.085 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 2.037 | Reg loss: 0.042 | Tree loss: 2.037 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 1.946 | Reg loss: 0.042 | Tree loss: 1.946 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 1.955 | Reg loss: 0.042 | Tree loss: 1.955 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 1.976 | Reg loss: 0.042 | Tree loss: 1.976 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 2.075 | Reg loss: 0.042 | Tree loss: 2.075 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 1.950 | Reg loss: 0.042 | Tree loss: 1.950 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 2.019 | Reg loss: 0.042 | Tree loss: 2.019 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 1.918 | Reg loss: 0.042 | Tree loss: 1.918 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 1.895 | Reg loss: 0.042 | Tree loss: 1.895 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 1.921 | Reg loss: 0.042 | Tree loss: 1.921 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 1.922 | Reg loss: 0.042 | Tree loss: 1.922 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 1.811 | Reg loss: 0.042 | Tree loss: 1.811 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 1.845 | Reg loss: 0.043 | Tree loss: 1.845 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.353535 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 2.019 | Reg loss: 0.042 | Tree loss: 2.019 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 1.975 | Reg loss: 0.042 | Tree loss: 1.975 | Accuracy: 0.359375 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 1.969 | Reg loss: 0.042 | Tree loss: 1.969 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 1.968 | Reg loss: 0.042 | Tree loss: 1.968 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 1.967 | Reg loss: 0.042 | Tree loss: 1.967 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 1.965 | Reg loss: 0.042 | Tree loss: 1.965 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 1.911 | Reg loss: 0.042 | Tree loss: 1.911 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 1.899 | Reg loss: 0.042 | Tree loss: 1.899 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 1.909 | Reg loss: 0.042 | Tree loss: 1.909 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 1.877 | Reg loss: 0.042 | Tree loss: 1.877 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 1.839 | Reg loss: 0.042 | Tree loss: 1.839 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 1.866 | Reg loss: 0.042 | Tree loss: 1.866 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.427734 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.346801 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 1.987 | Reg loss: 0.042 | Tree loss: 1.987 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 2.019 | Reg loss: 0.042 | Tree loss: 2.019 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 1.978 | Reg loss: 0.042 | Tree loss: 1.978 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 1.995 | Reg loss: 0.042 | Tree loss: 1.995 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 1.911 | Reg loss: 0.042 | Tree loss: 1.911 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 1.944 | Reg loss: 0.042 | Tree loss: 1.944 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 1.987 | Reg loss: 0.042 | Tree loss: 1.987 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 1.984 | Reg loss: 0.042 | Tree loss: 1.984 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 1.927 | Reg loss: 0.042 | Tree loss: 1.927 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 1.881 | Reg loss: 0.042 | Tree loss: 1.881 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.316406 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 1.792 | Reg loss: 0.042 | Tree loss: 1.792 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 1.774 | Reg loss: 0.042 | Tree loss: 1.774 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 1.738 | Reg loss: 0.042 | Tree loss: 1.738 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 1.663 | Reg loss: 0.042 | Tree loss: 1.663 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 1.660 | Reg loss: 0.043 | Tree loss: 1.660 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 1.649 | Reg loss: 0.043 | Tree loss: 1.649 | Accuracy: 0.356902 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 2.010 | Reg loss: 0.041 | Tree loss: 2.010 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 1.907 | Reg loss: 0.041 | Tree loss: 1.907 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 1.996 | Reg loss: 0.041 | Tree loss: 1.996 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 1.953 | Reg loss: 0.041 | Tree loss: 1.953 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 1.942 | Reg loss: 0.041 | Tree loss: 1.942 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 1.937 | Reg loss: 0.041 | Tree loss: 1.937 | Accuracy: 0.355469 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 1.892 | Reg loss: 0.041 | Tree loss: 1.892 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 1.871 | Reg loss: 0.041 | Tree loss: 1.871 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 1.866 | Reg loss: 0.041 | Tree loss: 1.866 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 1.884 | Reg loss: 0.041 | Tree loss: 1.884 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 1.826 | Reg loss: 0.041 | Tree loss: 1.826 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 1.666 | Reg loss: 0.042 | Tree loss: 1.666 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.387205 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 2.006 | Reg loss: 0.041 | Tree loss: 2.006 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 1.932 | Reg loss: 0.041 | Tree loss: 1.932 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 1.956 | Reg loss: 0.041 | Tree loss: 1.956 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 1.953 | Reg loss: 0.041 | Tree loss: 1.953 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 1.904 | Reg loss: 0.041 | Tree loss: 1.904 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 1.898 | Reg loss: 0.041 | Tree loss: 1.898 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 1.836 | Reg loss: 0.041 | Tree loss: 1.836 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 1.826 | Reg loss: 0.041 | Tree loss: 1.826 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 1.855 | Reg loss: 0.041 | Tree loss: 1.855 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 1.824 | Reg loss: 0.041 | Tree loss: 1.824 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 1.780 | Reg loss: 0.041 | Tree loss: 1.780 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 1.803 | Reg loss: 0.041 | Tree loss: 1.803 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 1.692 | Reg loss: 0.042 | Tree loss: 1.692 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 1.627 | Reg loss: 0.042 | Tree loss: 1.627 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 1.642 | Reg loss: 0.042 | Tree loss: 1.642 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 1.659 | Reg loss: 0.042 | Tree loss: 1.659 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 1.587 | Reg loss: 0.042 | Tree loss: 1.587 | Accuracy: 0.400673 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 1.955 | Reg loss: 0.041 | Tree loss: 1.955 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 1.946 | Reg loss: 0.041 | Tree loss: 1.946 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 1.935 | Reg loss: 0.041 | Tree loss: 1.935 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 1.928 | Reg loss: 0.041 | Tree loss: 1.928 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 1.936 | Reg loss: 0.041 | Tree loss: 1.936 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 1.814 | Reg loss: 0.041 | Tree loss: 1.814 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 1.824 | Reg loss: 0.041 | Tree loss: 1.824 | Accuracy: 0.412109 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 1.861 | Reg loss: 0.041 | Tree loss: 1.861 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.365234 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 1.756 | Reg loss: 0.041 | Tree loss: 1.756 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 1.813 | Reg loss: 0.041 | Tree loss: 1.813 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 1.784 | Reg loss: 0.041 | Tree loss: 1.784 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 1.803 | Reg loss: 0.041 | Tree loss: 1.803 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 1.748 | Reg loss: 0.041 | Tree loss: 1.748 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 1.772 | Reg loss: 0.041 | Tree loss: 1.772 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 1.675 | Reg loss: 0.042 | Tree loss: 1.675 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 1.642 | Reg loss: 0.042 | Tree loss: 1.642 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 1.666 | Reg loss: 0.042 | Tree loss: 1.666 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 1.666 | Reg loss: 0.042 | Tree loss: 1.666 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 1.665 | Reg loss: 0.042 | Tree loss: 1.665 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 1.656 | Reg loss: 0.042 | Tree loss: 1.656 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 1.660 | Reg loss: 0.042 | Tree loss: 1.660 | Accuracy: 0.333333 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 2.001 | Reg loss: 0.040 | Tree loss: 2.001 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 1.929 | Reg loss: 0.040 | Tree loss: 1.929 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 1.935 | Reg loss: 0.040 | Tree loss: 1.935 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 1.917 | Reg loss: 0.040 | Tree loss: 1.917 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 1.873 | Reg loss: 0.040 | Tree loss: 1.873 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 1.814 | Reg loss: 0.040 | Tree loss: 1.814 | Accuracy: 0.417969 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 1.849 | Reg loss: 0.040 | Tree loss: 1.849 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 1.808 | Reg loss: 0.041 | Tree loss: 1.808 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 1.775 | Reg loss: 0.041 | Tree loss: 1.775 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 1.779 | Reg loss: 0.041 | Tree loss: 1.779 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 1.769 | Reg loss: 0.041 | Tree loss: 1.769 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 1.698 | Reg loss: 0.041 | Tree loss: 1.698 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 1.755 | Reg loss: 0.041 | Tree loss: 1.755 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 1.687 | Reg loss: 0.041 | Tree loss: 1.687 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 1.685 | Reg loss: 0.041 | Tree loss: 1.685 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 1.602 | Reg loss: 0.041 | Tree loss: 1.602 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 1.665 | Reg loss: 0.041 | Tree loss: 1.665 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 1.631 | Reg loss: 0.042 | Tree loss: 1.631 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.320312 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 1.603 | Reg loss: 0.042 | Tree loss: 1.603 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 1.654 | Reg loss: 0.042 | Tree loss: 1.654 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.346801 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 1.952 | Reg loss: 0.040 | Tree loss: 1.952 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 1.954 | Reg loss: 0.040 | Tree loss: 1.954 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 1.929 | Reg loss: 0.040 | Tree loss: 1.929 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 1.907 | Reg loss: 0.040 | Tree loss: 1.907 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 1.842 | Reg loss: 0.040 | Tree loss: 1.842 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 1.814 | Reg loss: 0.040 | Tree loss: 1.814 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 1.811 | Reg loss: 0.040 | Tree loss: 1.811 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 1.805 | Reg loss: 0.040 | Tree loss: 1.805 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 1.802 | Reg loss: 0.040 | Tree loss: 1.802 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 1.783 | Reg loss: 0.040 | Tree loss: 1.783 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 1.760 | Reg loss: 0.040 | Tree loss: 1.760 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 1.763 | Reg loss: 0.040 | Tree loss: 1.763 | Accuracy: 0.371094 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 1.764 | Reg loss: 0.041 | Tree loss: 1.764 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 1.693 | Reg loss: 0.041 | Tree loss: 1.693 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 1.690 | Reg loss: 0.041 | Tree loss: 1.690 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 1.737 | Reg loss: 0.041 | Tree loss: 1.737 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 1.651 | Reg loss: 0.041 | Tree loss: 1.651 | Accuracy: 0.421875 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 1.690 | Reg loss: 0.041 | Tree loss: 1.690 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 1.693 | Reg loss: 0.041 | Tree loss: 1.693 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 1.757 | Reg loss: 0.041 | Tree loss: 1.757 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 1.640 | Reg loss: 0.041 | Tree loss: 1.640 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 1.677 | Reg loss: 0.041 | Tree loss: 1.677 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 1.669 | Reg loss: 0.041 | Tree loss: 1.669 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 1.614 | Reg loss: 0.041 | Tree loss: 1.614 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 1.592 | Reg loss: 0.042 | Tree loss: 1.592 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 1.646 | Reg loss: 0.042 | Tree loss: 1.646 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 1.609 | Reg loss: 0.042 | Tree loss: 1.609 | Accuracy: 0.383838 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 2.003 | Reg loss: 0.040 | Tree loss: 2.003 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 1.915 | Reg loss: 0.040 | Tree loss: 1.915 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 1.972 | Reg loss: 0.040 | Tree loss: 1.972 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 1.898 | Reg loss: 0.040 | Tree loss: 1.898 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 1.837 | Reg loss: 0.040 | Tree loss: 1.837 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 1.843 | Reg loss: 0.040 | Tree loss: 1.843 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 1.823 | Reg loss: 0.040 | Tree loss: 1.823 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 1.884 | Reg loss: 0.040 | Tree loss: 1.884 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 1.864 | Reg loss: 0.040 | Tree loss: 1.864 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 1.746 | Reg loss: 0.040 | Tree loss: 1.746 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 1.743 | Reg loss: 0.040 | Tree loss: 1.743 | Accuracy: 0.417969 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 1.728 | Reg loss: 0.040 | Tree loss: 1.728 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 1.705 | Reg loss: 0.040 | Tree loss: 1.705 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 1.678 | Reg loss: 0.040 | Tree loss: 1.678 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 1.714 | Reg loss: 0.040 | Tree loss: 1.714 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 1.694 | Reg loss: 0.041 | Tree loss: 1.694 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 1.690 | Reg loss: 0.041 | Tree loss: 1.690 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 1.679 | Reg loss: 0.041 | Tree loss: 1.679 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 1.685 | Reg loss: 0.041 | Tree loss: 1.685 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 1.660 | Reg loss: 0.041 | Tree loss: 1.660 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 1.671 | Reg loss: 0.041 | Tree loss: 1.671 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 1.595 | Reg loss: 0.041 | Tree loss: 1.595 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 1.645 | Reg loss: 0.041 | Tree loss: 1.645 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 1.629 | Reg loss: 0.041 | Tree loss: 1.629 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 1.642 | Reg loss: 0.041 | Tree loss: 1.642 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 1.670 | Reg loss: 0.041 | Tree loss: 1.670 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 1.586 | Reg loss: 0.041 | Tree loss: 1.586 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 1.580 | Reg loss: 0.042 | Tree loss: 1.580 | Accuracy: 0.336700 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 1.973 | Reg loss: 0.039 | Tree loss: 1.973 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 1.882 | Reg loss: 0.039 | Tree loss: 1.882 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 1.872 | Reg loss: 0.039 | Tree loss: 1.872 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 1.868 | Reg loss: 0.039 | Tree loss: 1.868 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 1.822 | Reg loss: 0.040 | Tree loss: 1.822 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 1.839 | Reg loss: 0.040 | Tree loss: 1.839 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 1.801 | Reg loss: 0.040 | Tree loss: 1.801 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 1.854 | Reg loss: 0.040 | Tree loss: 1.854 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 1.810 | Reg loss: 0.040 | Tree loss: 1.810 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 1.845 | Reg loss: 0.040 | Tree loss: 1.845 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 1.752 | Reg loss: 0.040 | Tree loss: 1.752 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 1.712 | Reg loss: 0.040 | Tree loss: 1.712 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 1.732 | Reg loss: 0.040 | Tree loss: 1.732 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 1.708 | Reg loss: 0.040 | Tree loss: 1.708 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 1.759 | Reg loss: 0.040 | Tree loss: 1.759 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 1.710 | Reg loss: 0.040 | Tree loss: 1.710 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 1.656 | Reg loss: 0.040 | Tree loss: 1.656 | Accuracy: 0.361328 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 1.651 | Reg loss: 0.041 | Tree loss: 1.651 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 1.650 | Reg loss: 0.041 | Tree loss: 1.650 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 1.641 | Reg loss: 0.041 | Tree loss: 1.641 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 1.612 | Reg loss: 0.041 | Tree loss: 1.612 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 1.608 | Reg loss: 0.041 | Tree loss: 1.608 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 1.663 | Reg loss: 0.041 | Tree loss: 1.663 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 1.612 | Reg loss: 0.041 | Tree loss: 1.612 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 1.550 | Reg loss: 0.041 | Tree loss: 1.550 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 1.603 | Reg loss: 0.041 | Tree loss: 1.603 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 1.602 | Reg loss: 0.041 | Tree loss: 1.602 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 1.579 | Reg loss: 0.041 | Tree loss: 1.579 | Accuracy: 0.326599 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 1.883 | Reg loss: 0.039 | Tree loss: 1.883 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 1.882 | Reg loss: 0.039 | Tree loss: 1.882 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 1.876 | Reg loss: 0.039 | Tree loss: 1.876 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 1.962 | Reg loss: 0.039 | Tree loss: 1.962 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 1.857 | Reg loss: 0.040 | Tree loss: 1.857 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 1.820 | Reg loss: 0.040 | Tree loss: 1.820 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 1.788 | Reg loss: 0.040 | Tree loss: 1.788 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 1.768 | Reg loss: 0.040 | Tree loss: 1.768 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 1.751 | Reg loss: 0.040 | Tree loss: 1.751 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 1.759 | Reg loss: 0.040 | Tree loss: 1.759 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 1.729 | Reg loss: 0.040 | Tree loss: 1.729 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 1.747 | Reg loss: 0.040 | Tree loss: 1.747 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 1.824 | Reg loss: 0.040 | Tree loss: 1.824 | Accuracy: 0.298828 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 1.749 | Reg loss: 0.040 | Tree loss: 1.749 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 1.674 | Reg loss: 0.040 | Tree loss: 1.674 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 1.671 | Reg loss: 0.040 | Tree loss: 1.671 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 1.617 | Reg loss: 0.041 | Tree loss: 1.617 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 1.646 | Reg loss: 0.041 | Tree loss: 1.646 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 1.620 | Reg loss: 0.041 | Tree loss: 1.620 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 1.646 | Reg loss: 0.041 | Tree loss: 1.646 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 1.636 | Reg loss: 0.041 | Tree loss: 1.636 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 1.626 | Reg loss: 0.041 | Tree loss: 1.626 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 1.545 | Reg loss: 0.041 | Tree loss: 1.545 | Accuracy: 0.419922 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 1.591 | Reg loss: 0.041 | Tree loss: 1.591 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 1.543 | Reg loss: 0.041 | Tree loss: 1.543 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 1.528 | Reg loss: 0.041 | Tree loss: 1.528 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 1.581 | Reg loss: 0.041 | Tree loss: 1.581 | Accuracy: 0.360269 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 1.897 | Reg loss: 0.040 | Tree loss: 1.897 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 1.832 | Reg loss: 0.040 | Tree loss: 1.832 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 1.907 | Reg loss: 0.040 | Tree loss: 1.907 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 1.822 | Reg loss: 0.040 | Tree loss: 1.822 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 1.782 | Reg loss: 0.040 | Tree loss: 1.782 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 1.766 | Reg loss: 0.040 | Tree loss: 1.766 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 1.779 | Reg loss: 0.040 | Tree loss: 1.779 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 1.800 | Reg loss: 0.040 | Tree loss: 1.800 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 1.789 | Reg loss: 0.040 | Tree loss: 1.789 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 1.700 | Reg loss: 0.040 | Tree loss: 1.700 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 1.711 | Reg loss: 0.040 | Tree loss: 1.711 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 1.677 | Reg loss: 0.040 | Tree loss: 1.677 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 1.701 | Reg loss: 0.040 | Tree loss: 1.701 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 1.729 | Reg loss: 0.040 | Tree loss: 1.729 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 1.672 | Reg loss: 0.040 | Tree loss: 1.672 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 1.662 | Reg loss: 0.041 | Tree loss: 1.662 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 1.648 | Reg loss: 0.041 | Tree loss: 1.648 | Accuracy: 0.316406 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 1.640 | Reg loss: 0.041 | Tree loss: 1.640 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 1.636 | Reg loss: 0.041 | Tree loss: 1.636 | Accuracy: 0.367188 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 1.631 | Reg loss: 0.041 | Tree loss: 1.631 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 1.560 | Reg loss: 0.041 | Tree loss: 1.560 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 1.580 | Reg loss: 0.041 | Tree loss: 1.580 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 1.580 | Reg loss: 0.041 | Tree loss: 1.580 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 1.589 | Reg loss: 0.041 | Tree loss: 1.589 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 1.606 | Reg loss: 0.041 | Tree loss: 1.606 | Accuracy: 0.400673 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 1.907 | Reg loss: 0.040 | Tree loss: 1.907 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 1.822 | Reg loss: 0.040 | Tree loss: 1.822 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 1.839 | Reg loss: 0.040 | Tree loss: 1.839 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 1.823 | Reg loss: 0.040 | Tree loss: 1.823 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 1.786 | Reg loss: 0.040 | Tree loss: 1.786 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 1.764 | Reg loss: 0.040 | Tree loss: 1.764 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 1.801 | Reg loss: 0.040 | Tree loss: 1.801 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 1.788 | Reg loss: 0.040 | Tree loss: 1.788 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 1.753 | Reg loss: 0.040 | Tree loss: 1.753 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 1.749 | Reg loss: 0.040 | Tree loss: 1.749 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 1.728 | Reg loss: 0.040 | Tree loss: 1.728 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 1.725 | Reg loss: 0.040 | Tree loss: 1.725 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 1.701 | Reg loss: 0.040 | Tree loss: 1.701 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 1.633 | Reg loss: 0.040 | Tree loss: 1.633 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 1.669 | Reg loss: 0.040 | Tree loss: 1.669 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 1.643 | Reg loss: 0.041 | Tree loss: 1.643 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 1.637 | Reg loss: 0.041 | Tree loss: 1.637 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 1.626 | Reg loss: 0.041 | Tree loss: 1.626 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 1.585 | Reg loss: 0.041 | Tree loss: 1.585 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 1.601 | Reg loss: 0.041 | Tree loss: 1.601 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 1.608 | Reg loss: 0.041 | Tree loss: 1.608 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 1.554 | Reg loss: 0.041 | Tree loss: 1.554 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 1.576 | Reg loss: 0.041 | Tree loss: 1.576 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 1.591 | Reg loss: 0.041 | Tree loss: 1.591 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 1.554 | Reg loss: 0.041 | Tree loss: 1.554 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 1.601 | Reg loss: 0.041 | Tree loss: 1.601 | Accuracy: 0.313131 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 1.896 | Reg loss: 0.040 | Tree loss: 1.896 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 1.942 | Reg loss: 0.040 | Tree loss: 1.942 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 1.850 | Reg loss: 0.040 | Tree loss: 1.850 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 1.776 | Reg loss: 0.040 | Tree loss: 1.776 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 1.848 | Reg loss: 0.040 | Tree loss: 1.848 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 1.793 | Reg loss: 0.040 | Tree loss: 1.793 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 1.768 | Reg loss: 0.040 | Tree loss: 1.768 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 1.790 | Reg loss: 0.040 | Tree loss: 1.790 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 1.793 | Reg loss: 0.040 | Tree loss: 1.793 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 1.698 | Reg loss: 0.040 | Tree loss: 1.698 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 1.719 | Reg loss: 0.040 | Tree loss: 1.719 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 1.677 | Reg loss: 0.040 | Tree loss: 1.677 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 1.678 | Reg loss: 0.040 | Tree loss: 1.678 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 1.643 | Reg loss: 0.040 | Tree loss: 1.643 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 1.705 | Reg loss: 0.040 | Tree loss: 1.705 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 1.650 | Reg loss: 0.040 | Tree loss: 1.650 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 1.652 | Reg loss: 0.041 | Tree loss: 1.652 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 1.635 | Reg loss: 0.041 | Tree loss: 1.635 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 1.612 | Reg loss: 0.041 | Tree loss: 1.612 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 1.642 | Reg loss: 0.041 | Tree loss: 1.642 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 1.576 | Reg loss: 0.041 | Tree loss: 1.576 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 1.595 | Reg loss: 0.041 | Tree loss: 1.595 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 1.572 | Reg loss: 0.041 | Tree loss: 1.572 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 1.577 | Reg loss: 0.041 | Tree loss: 1.577 | Accuracy: 0.367188 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 1.558 | Reg loss: 0.041 | Tree loss: 1.558 | Accuracy: 0.416016 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 1.584 | Reg loss: 0.041 | Tree loss: 1.584 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 1.540 | Reg loss: 0.041 | Tree loss: 1.540 | Accuracy: 0.343434 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 1.871 | Reg loss: 0.040 | Tree loss: 1.871 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 1.868 | Reg loss: 0.040 | Tree loss: 1.868 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 1.820 | Reg loss: 0.040 | Tree loss: 1.820 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 1.837 | Reg loss: 0.040 | Tree loss: 1.837 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 1.805 | Reg loss: 0.040 | Tree loss: 1.805 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 1.807 | Reg loss: 0.040 | Tree loss: 1.807 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 1.736 | Reg loss: 0.040 | Tree loss: 1.736 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 1.734 | Reg loss: 0.040 | Tree loss: 1.734 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 1.810 | Reg loss: 0.040 | Tree loss: 1.810 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 1.747 | Reg loss: 0.040 | Tree loss: 1.747 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 1.715 | Reg loss: 0.040 | Tree loss: 1.715 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 1.696 | Reg loss: 0.040 | Tree loss: 1.696 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 1.685 | Reg loss: 0.040 | Tree loss: 1.685 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 1.665 | Reg loss: 0.040 | Tree loss: 1.665 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 1.652 | Reg loss: 0.040 | Tree loss: 1.652 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 1.626 | Reg loss: 0.040 | Tree loss: 1.626 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 1.612 | Reg loss: 0.041 | Tree loss: 1.612 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 1.635 | Reg loss: 0.041 | Tree loss: 1.635 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 1.586 | Reg loss: 0.041 | Tree loss: 1.586 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 1.620 | Reg loss: 0.041 | Tree loss: 1.620 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 1.592 | Reg loss: 0.041 | Tree loss: 1.592 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 1.595 | Reg loss: 0.041 | Tree loss: 1.595 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 1.528 | Reg loss: 0.041 | Tree loss: 1.528 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 1.586 | Reg loss: 0.041 | Tree loss: 1.586 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 1.566 | Reg loss: 0.041 | Tree loss: 1.566 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 1.555 | Reg loss: 0.041 | Tree loss: 1.555 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 1.578 | Reg loss: 0.041 | Tree loss: 1.578 | Accuracy: 0.353535 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 1.837 | Reg loss: 0.040 | Tree loss: 1.837 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 1.841 | Reg loss: 0.040 | Tree loss: 1.841 | Accuracy: 0.396484 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 1.850 | Reg loss: 0.040 | Tree loss: 1.850 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 1.805 | Reg loss: 0.040 | Tree loss: 1.805 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 1.879 | Reg loss: 0.040 | Tree loss: 1.879 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 1.787 | Reg loss: 0.040 | Tree loss: 1.787 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.423828 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 1.718 | Reg loss: 0.040 | Tree loss: 1.718 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 1.718 | Reg loss: 0.040 | Tree loss: 1.718 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 1.702 | Reg loss: 0.040 | Tree loss: 1.702 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 1.699 | Reg loss: 0.040 | Tree loss: 1.699 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 1.680 | Reg loss: 0.040 | Tree loss: 1.680 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 1.672 | Reg loss: 0.040 | Tree loss: 1.672 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 1.657 | Reg loss: 0.040 | Tree loss: 1.657 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 1.617 | Reg loss: 0.041 | Tree loss: 1.617 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 1.613 | Reg loss: 0.041 | Tree loss: 1.613 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 1.608 | Reg loss: 0.041 | Tree loss: 1.608 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 1.605 | Reg loss: 0.041 | Tree loss: 1.605 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 1.558 | Reg loss: 0.041 | Tree loss: 1.558 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 1.560 | Reg loss: 0.041 | Tree loss: 1.560 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 1.553 | Reg loss: 0.041 | Tree loss: 1.553 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 1.556 | Reg loss: 0.041 | Tree loss: 1.556 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 1.535 | Reg loss: 0.041 | Tree loss: 1.535 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 1.501 | Reg loss: 0.041 | Tree loss: 1.501 | Accuracy: 0.363636 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 1.848 | Reg loss: 0.040 | Tree loss: 1.848 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 1.821 | Reg loss: 0.040 | Tree loss: 1.821 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 1.845 | Reg loss: 0.040 | Tree loss: 1.845 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 1.766 | Reg loss: 0.040 | Tree loss: 1.766 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 1.754 | Reg loss: 0.040 | Tree loss: 1.754 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 1.736 | Reg loss: 0.040 | Tree loss: 1.736 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 1.845 | Reg loss: 0.040 | Tree loss: 1.845 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 1.746 | Reg loss: 0.040 | Tree loss: 1.746 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 1.707 | Reg loss: 0.040 | Tree loss: 1.707 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 1.674 | Reg loss: 0.040 | Tree loss: 1.674 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 1.670 | Reg loss: 0.040 | Tree loss: 1.670 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 1.681 | Reg loss: 0.040 | Tree loss: 1.681 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 1.676 | Reg loss: 0.040 | Tree loss: 1.676 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 1.671 | Reg loss: 0.040 | Tree loss: 1.671 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 1.597 | Reg loss: 0.040 | Tree loss: 1.597 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 1.585 | Reg loss: 0.041 | Tree loss: 1.585 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 1.565 | Reg loss: 0.041 | Tree loss: 1.565 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 1.577 | Reg loss: 0.041 | Tree loss: 1.577 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 1.582 | Reg loss: 0.041 | Tree loss: 1.582 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 1.586 | Reg loss: 0.041 | Tree loss: 1.586 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 1.603 | Reg loss: 0.041 | Tree loss: 1.603 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 1.604 | Reg loss: 0.041 | Tree loss: 1.604 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 1.505 | Reg loss: 0.041 | Tree loss: 1.505 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 1.521 | Reg loss: 0.041 | Tree loss: 1.521 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 1.493 | Reg loss: 0.041 | Tree loss: 1.493 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 1.873 | Reg loss: 0.040 | Tree loss: 1.873 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 1.813 | Reg loss: 0.040 | Tree loss: 1.813 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 1.764 | Reg loss: 0.040 | Tree loss: 1.764 | Accuracy: 0.416016 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 1.799 | Reg loss: 0.040 | Tree loss: 1.799 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 1.728 | Reg loss: 0.040 | Tree loss: 1.728 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 1.748 | Reg loss: 0.040 | Tree loss: 1.748 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 1.760 | Reg loss: 0.040 | Tree loss: 1.760 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 1.684 | Reg loss: 0.040 | Tree loss: 1.684 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 1.707 | Reg loss: 0.040 | Tree loss: 1.707 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 1.694 | Reg loss: 0.040 | Tree loss: 1.694 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 1.662 | Reg loss: 0.040 | Tree loss: 1.662 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 1.639 | Reg loss: 0.040 | Tree loss: 1.639 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 1.650 | Reg loss: 0.040 | Tree loss: 1.650 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 1.666 | Reg loss: 0.040 | Tree loss: 1.666 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 1.576 | Reg loss: 0.041 | Tree loss: 1.576 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 1.626 | Reg loss: 0.041 | Tree loss: 1.626 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 1.595 | Reg loss: 0.041 | Tree loss: 1.595 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 1.622 | Reg loss: 0.041 | Tree loss: 1.622 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 1.529 | Reg loss: 0.041 | Tree loss: 1.529 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 1.553 | Reg loss: 0.041 | Tree loss: 1.553 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 1.548 | Reg loss: 0.041 | Tree loss: 1.548 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 1.564 | Reg loss: 0.041 | Tree loss: 1.564 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 1.534 | Reg loss: 0.041 | Tree loss: 1.534 | Accuracy: 0.377104 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 1.882 | Reg loss: 0.040 | Tree loss: 1.882 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.437500 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 1.841 | Reg loss: 0.040 | Tree loss: 1.841 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 1.772 | Reg loss: 0.040 | Tree loss: 1.772 | Accuracy: 0.371094 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 1.817 | Reg loss: 0.040 | Tree loss: 1.817 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 1.779 | Reg loss: 0.040 | Tree loss: 1.779 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.330078 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 1.706 | Reg loss: 0.040 | Tree loss: 1.706 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 1.696 | Reg loss: 0.040 | Tree loss: 1.696 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 1.667 | Reg loss: 0.040 | Tree loss: 1.667 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 1.663 | Reg loss: 0.040 | Tree loss: 1.663 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 1.664 | Reg loss: 0.040 | Tree loss: 1.664 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 1.649 | Reg loss: 0.040 | Tree loss: 1.649 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 1.648 | Reg loss: 0.040 | Tree loss: 1.648 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 1.633 | Reg loss: 0.040 | Tree loss: 1.633 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 1.586 | Reg loss: 0.041 | Tree loss: 1.586 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 1.587 | Reg loss: 0.041 | Tree loss: 1.587 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 1.539 | Reg loss: 0.041 | Tree loss: 1.539 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 1.556 | Reg loss: 0.041 | Tree loss: 1.556 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 1.565 | Reg loss: 0.041 | Tree loss: 1.565 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 1.583 | Reg loss: 0.041 | Tree loss: 1.583 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 1.491 | Reg loss: 0.041 | Tree loss: 1.491 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 1.534 | Reg loss: 0.041 | Tree loss: 1.534 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 1.619 | Reg loss: 0.041 | Tree loss: 1.619 | Accuracy: 0.299663 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 1.831 | Reg loss: 0.039 | Tree loss: 1.831 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 1.831 | Reg loss: 0.039 | Tree loss: 1.831 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 1.923 | Reg loss: 0.039 | Tree loss: 1.923 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 1.818 | Reg loss: 0.039 | Tree loss: 1.818 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 1.766 | Reg loss: 0.039 | Tree loss: 1.766 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 1.744 | Reg loss: 0.040 | Tree loss: 1.744 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.365234 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 1.734 | Reg loss: 0.040 | Tree loss: 1.734 | Accuracy: 0.333984 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 1.716 | Reg loss: 0.040 | Tree loss: 1.716 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 1.701 | Reg loss: 0.040 | Tree loss: 1.701 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 1.654 | Reg loss: 0.040 | Tree loss: 1.654 | Accuracy: 0.398438 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 1.660 | Reg loss: 0.040 | Tree loss: 1.660 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 1.679 | Reg loss: 0.040 | Tree loss: 1.679 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 1.634 | Reg loss: 0.040 | Tree loss: 1.634 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 1.649 | Reg loss: 0.040 | Tree loss: 1.649 | Accuracy: 0.332031 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 1.646 | Reg loss: 0.040 | Tree loss: 1.646 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 1.539 | Reg loss: 0.040 | Tree loss: 1.539 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 1.591 | Reg loss: 0.041 | Tree loss: 1.591 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 1.595 | Reg loss: 0.041 | Tree loss: 1.595 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 1.576 | Reg loss: 0.041 | Tree loss: 1.576 | Accuracy: 0.343750 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 1.552 | Reg loss: 0.041 | Tree loss: 1.552 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 1.540 | Reg loss: 0.041 | Tree loss: 1.540 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 1.540 | Reg loss: 0.041 | Tree loss: 1.540 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 1.532 | Reg loss: 0.041 | Tree loss: 1.532 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 1.514 | Reg loss: 0.041 | Tree loss: 1.514 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 1.522 | Reg loss: 0.041 | Tree loss: 1.522 | Accuracy: 0.323232 | 0.895 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 1.816 | Reg loss: 0.039 | Tree loss: 1.816 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 1.824 | Reg loss: 0.039 | Tree loss: 1.824 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 1.836 | Reg loss: 0.039 | Tree loss: 1.836 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 1.757 | Reg loss: 0.039 | Tree loss: 1.757 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 1.790 | Reg loss: 0.039 | Tree loss: 1.790 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 1.827 | Reg loss: 0.039 | Tree loss: 1.827 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 1.795 | Reg loss: 0.039 | Tree loss: 1.795 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 1.715 | Reg loss: 0.040 | Tree loss: 1.715 | Accuracy: 0.373047 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 1.746 | Reg loss: 0.040 | Tree loss: 1.746 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 1.635 | Reg loss: 0.040 | Tree loss: 1.635 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 1.666 | Reg loss: 0.040 | Tree loss: 1.666 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 1.667 | Reg loss: 0.040 | Tree loss: 1.667 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 1.669 | Reg loss: 0.040 | Tree loss: 1.669 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 1.674 | Reg loss: 0.040 | Tree loss: 1.674 | Accuracy: 0.369141 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 1.621 | Reg loss: 0.040 | Tree loss: 1.621 | Accuracy: 0.390625 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 1.645 | Reg loss: 0.040 | Tree loss: 1.645 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 1.551 | Reg loss: 0.041 | Tree loss: 1.551 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 1.571 | Reg loss: 0.041 | Tree loss: 1.571 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 1.486 | Reg loss: 0.041 | Tree loss: 1.486 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 1.520 | Reg loss: 0.041 | Tree loss: 1.520 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 1.508 | Reg loss: 0.041 | Tree loss: 1.508 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 1.445 | Reg loss: 0.041 | Tree loss: 1.445 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 1.478 | Reg loss: 0.041 | Tree loss: 1.478 | Accuracy: 0.340067 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 1.836 | Reg loss: 0.039 | Tree loss: 1.836 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 1.898 | Reg loss: 0.039 | Tree loss: 1.898 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 1.847 | Reg loss: 0.039 | Tree loss: 1.847 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 1.756 | Reg loss: 0.039 | Tree loss: 1.756 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 1.740 | Reg loss: 0.039 | Tree loss: 1.740 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 1.745 | Reg loss: 0.039 | Tree loss: 1.745 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 1.735 | Reg loss: 0.039 | Tree loss: 1.735 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 1.715 | Reg loss: 0.039 | Tree loss: 1.715 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 1.674 | Reg loss: 0.040 | Tree loss: 1.674 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 1.669 | Reg loss: 0.040 | Tree loss: 1.669 | Accuracy: 0.421875 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 1.691 | Reg loss: 0.040 | Tree loss: 1.691 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 1.638 | Reg loss: 0.040 | Tree loss: 1.638 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.429688 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 1.633 | Reg loss: 0.040 | Tree loss: 1.633 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 1.636 | Reg loss: 0.040 | Tree loss: 1.636 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.322266 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.324219 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.384766 | 0.895 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 1.562 | Reg loss: 0.041 | Tree loss: 1.562 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 1.530 | Reg loss: 0.041 | Tree loss: 1.530 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 1.533 | Reg loss: 0.041 | Tree loss: 1.533 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 1.521 | Reg loss: 0.041 | Tree loss: 1.521 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 1.550 | Reg loss: 0.041 | Tree loss: 1.550 | Accuracy: 0.322266 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 1.494 | Reg loss: 0.041 | Tree loss: 1.494 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 1.462 | Reg loss: 0.041 | Tree loss: 1.462 | Accuracy: 0.383838 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 1.867 | Reg loss: 0.039 | Tree loss: 1.867 | Accuracy: 0.349609 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 1.818 | Reg loss: 0.039 | Tree loss: 1.818 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 1.836 | Reg loss: 0.039 | Tree loss: 1.836 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 1.852 | Reg loss: 0.039 | Tree loss: 1.852 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 1.850 | Reg loss: 0.039 | Tree loss: 1.850 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 1.729 | Reg loss: 0.039 | Tree loss: 1.729 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 1.671 | Reg loss: 0.039 | Tree loss: 1.671 | Accuracy: 0.400391 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 1.651 | Reg loss: 0.039 | Tree loss: 1.651 | Accuracy: 0.406250 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 1.678 | Reg loss: 0.039 | Tree loss: 1.678 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 1.765 | Reg loss: 0.039 | Tree loss: 1.765 | Accuracy: 0.326172 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 1.747 | Reg loss: 0.040 | Tree loss: 1.747 | Accuracy: 0.363281 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 1.643 | Reg loss: 0.040 | Tree loss: 1.643 | Accuracy: 0.363281 | 0.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 1.640 | Reg loss: 0.040 | Tree loss: 1.640 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 1.624 | Reg loss: 0.040 | Tree loss: 1.624 | Accuracy: 0.355469 | 0.895 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.324219 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 1.584 | Reg loss: 0.041 | Tree loss: 1.584 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 1.509 | Reg loss: 0.041 | Tree loss: 1.509 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 1.507 | Reg loss: 0.041 | Tree loss: 1.507 | Accuracy: 0.412109 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 1.551 | Reg loss: 0.041 | Tree loss: 1.551 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 1.515 | Reg loss: 0.041 | Tree loss: 1.515 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 1.529 | Reg loss: 0.041 | Tree loss: 1.529 | Accuracy: 0.360269 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 1.817 | Reg loss: 0.039 | Tree loss: 1.817 | Accuracy: 0.382812 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 1.841 | Reg loss: 0.039 | Tree loss: 1.841 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 1.779 | Reg loss: 0.039 | Tree loss: 1.779 | Accuracy: 0.425781 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 1.784 | Reg loss: 0.039 | Tree loss: 1.784 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 1.776 | Reg loss: 0.039 | Tree loss: 1.776 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 1.756 | Reg loss: 0.039 | Tree loss: 1.756 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 1.758 | Reg loss: 0.039 | Tree loss: 1.758 | Accuracy: 0.353516 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 1.744 | Reg loss: 0.039 | Tree loss: 1.744 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 1.695 | Reg loss: 0.039 | Tree loss: 1.695 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 1.654 | Reg loss: 0.039 | Tree loss: 1.654 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 1.667 | Reg loss: 0.039 | Tree loss: 1.667 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 1.657 | Reg loss: 0.040 | Tree loss: 1.657 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 1.640 | Reg loss: 0.040 | Tree loss: 1.640 | Accuracy: 0.314453 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 1.694 | Reg loss: 0.040 | Tree loss: 1.694 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.373047 | 0.895 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.326172 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 1.522 | Reg loss: 0.040 | Tree loss: 1.522 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 1.533 | Reg loss: 0.041 | Tree loss: 1.533 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 1.542 | Reg loss: 0.041 | Tree loss: 1.542 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 1.556 | Reg loss: 0.041 | Tree loss: 1.556 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 1.506 | Reg loss: 0.041 | Tree loss: 1.506 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 1.465 | Reg loss: 0.041 | Tree loss: 1.465 | Accuracy: 0.353535 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 1.857 | Reg loss: 0.039 | Tree loss: 1.857 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 1.760 | Reg loss: 0.039 | Tree loss: 1.760 | Accuracy: 0.380859 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 1.849 | Reg loss: 0.039 | Tree loss: 1.849 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 1.791 | Reg loss: 0.039 | Tree loss: 1.791 | Accuracy: 0.404297 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 1.761 | Reg loss: 0.039 | Tree loss: 1.761 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 1.782 | Reg loss: 0.039 | Tree loss: 1.782 | Accuracy: 0.367188 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 1.693 | Reg loss: 0.039 | Tree loss: 1.693 | Accuracy: 0.417969 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 1.678 | Reg loss: 0.039 | Tree loss: 1.678 | Accuracy: 0.402344 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 1.710 | Reg loss: 0.039 | Tree loss: 1.710 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 1.652 | Reg loss: 0.039 | Tree loss: 1.652 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 1.655 | Reg loss: 0.039 | Tree loss: 1.655 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 1.639 | Reg loss: 0.039 | Tree loss: 1.639 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 1.676 | Reg loss: 0.040 | Tree loss: 1.676 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 1.653 | Reg loss: 0.040 | Tree loss: 1.653 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 1.640 | Reg loss: 0.040 | Tree loss: 1.640 | Accuracy: 0.376953 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 1.641 | Reg loss: 0.040 | Tree loss: 1.641 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 1.539 | Reg loss: 0.040 | Tree loss: 1.539 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 1.530 | Reg loss: 0.041 | Tree loss: 1.530 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 1.505 | Reg loss: 0.041 | Tree loss: 1.505 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 1.503 | Reg loss: 0.041 | Tree loss: 1.503 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 1.441 | Reg loss: 0.041 | Tree loss: 1.441 | Accuracy: 0.410774 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 1.849 | Reg loss: 0.039 | Tree loss: 1.849 | Accuracy: 0.357422 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.423828 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 1.800 | Reg loss: 0.039 | Tree loss: 1.800 | Accuracy: 0.359375 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 1.751 | Reg loss: 0.039 | Tree loss: 1.751 | Accuracy: 0.378906 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 1.854 | Reg loss: 0.039 | Tree loss: 1.854 | Accuracy: 0.339844 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 1.725 | Reg loss: 0.039 | Tree loss: 1.725 | Accuracy: 0.410156 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 1.689 | Reg loss: 0.039 | Tree loss: 1.689 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 1.692 | Reg loss: 0.039 | Tree loss: 1.692 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 1.698 | Reg loss: 0.039 | Tree loss: 1.698 | Accuracy: 0.347656 | 0.895 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 1.774 | Reg loss: 0.039 | Tree loss: 1.774 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 1.738 | Reg loss: 0.039 | Tree loss: 1.738 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 1.646 | Reg loss: 0.039 | Tree loss: 1.646 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 1.647 | Reg loss: 0.039 | Tree loss: 1.647 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.328125 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 1.551 | Reg loss: 0.040 | Tree loss: 1.551 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 1.519 | Reg loss: 0.040 | Tree loss: 1.519 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 1.494 | Reg loss: 0.041 | Tree loss: 1.494 | Accuracy: 0.322266 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 1.474 | Reg loss: 0.041 | Tree loss: 1.474 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 1.475 | Reg loss: 0.041 | Tree loss: 1.475 | Accuracy: 0.370370 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 1.821 | Reg loss: 0.039 | Tree loss: 1.821 | Accuracy: 0.386719 | 0.895 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 1.805 | Reg loss: 0.039 | Tree loss: 1.805 | Accuracy: 0.392578 | 0.895 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 1.773 | Reg loss: 0.039 | Tree loss: 1.773 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 1.764 | Reg loss: 0.039 | Tree loss: 1.764 | Accuracy: 0.341797 | 0.895 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 1.725 | Reg loss: 0.039 | Tree loss: 1.725 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 1.727 | Reg loss: 0.039 | Tree loss: 1.727 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 1.734 | Reg loss: 0.039 | Tree loss: 1.734 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 1.731 | Reg loss: 0.039 | Tree loss: 1.731 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 1.652 | Reg loss: 0.039 | Tree loss: 1.652 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 1.666 | Reg loss: 0.039 | Tree loss: 1.666 | Accuracy: 0.408203 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 1.697 | Reg loss: 0.039 | Tree loss: 1.697 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 1.639 | Reg loss: 0.039 | Tree loss: 1.639 | Accuracy: 0.414062 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 1.679 | Reg loss: 0.039 | Tree loss: 1.679 | Accuracy: 0.328125 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 1.631 | Reg loss: 0.039 | Tree loss: 1.631 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 1.630 | Reg loss: 0.040 | Tree loss: 1.630 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.392578 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 1.506 | Reg loss: 0.040 | Tree loss: 1.506 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 1.617 | Reg loss: 0.040 | Tree loss: 1.617 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 1.540 | Reg loss: 0.041 | Tree loss: 1.540 | Accuracy: 0.299663 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 1.846 | Reg loss: 0.039 | Tree loss: 1.846 | Accuracy: 0.328125 | 0.895 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 1.794 | Reg loss: 0.039 | Tree loss: 1.794 | Accuracy: 0.361328 | 0.895 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 1.819 | Reg loss: 0.039 | Tree loss: 1.819 | Accuracy: 0.351562 | 0.895 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 1.833 | Reg loss: 0.039 | Tree loss: 1.833 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 1.806 | Reg loss: 0.039 | Tree loss: 1.806 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 1.739 | Reg loss: 0.039 | Tree loss: 1.739 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 1.704 | Reg loss: 0.039 | Tree loss: 1.704 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 1.662 | Reg loss: 0.039 | Tree loss: 1.662 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 1.705 | Reg loss: 0.039 | Tree loss: 1.705 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 1.694 | Reg loss: 0.039 | Tree loss: 1.694 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 1.656 | Reg loss: 0.039 | Tree loss: 1.656 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 1.659 | Reg loss: 0.039 | Tree loss: 1.659 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 1.672 | Reg loss: 0.039 | Tree loss: 1.672 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 1.588 | Reg loss: 0.039 | Tree loss: 1.588 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 1.640 | Reg loss: 0.039 | Tree loss: 1.640 | Accuracy: 0.326172 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 1.539 | Reg loss: 0.040 | Tree loss: 1.539 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.333984 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.424242 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 1.842 | Reg loss: 0.039 | Tree loss: 1.842 | Accuracy: 0.394531 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 1.827 | Reg loss: 0.039 | Tree loss: 1.827 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 1.752 | Reg loss: 0.039 | Tree loss: 1.752 | Accuracy: 0.408203 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 1.809 | Reg loss: 0.039 | Tree loss: 1.809 | Accuracy: 0.345703 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 1.764 | Reg loss: 0.039 | Tree loss: 1.764 | Accuracy: 0.337891 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 1.780 | Reg loss: 0.039 | Tree loss: 1.780 | Accuracy: 0.375000 | 0.895 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 1.739 | Reg loss: 0.039 | Tree loss: 1.739 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 1.724 | Reg loss: 0.039 | Tree loss: 1.724 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 1.609 | Reg loss: 0.039 | Tree loss: 1.609 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 1.668 | Reg loss: 0.039 | Tree loss: 1.668 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 1.698 | Reg loss: 0.039 | Tree loss: 1.698 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 1.701 | Reg loss: 0.039 | Tree loss: 1.701 | Accuracy: 0.333984 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 1.622 | Reg loss: 0.039 | Tree loss: 1.622 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 1.631 | Reg loss: 0.039 | Tree loss: 1.631 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 1.528 | Reg loss: 0.039 | Tree loss: 1.528 | Accuracy: 0.400391 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 1.580 | Reg loss: 0.039 | Tree loss: 1.580 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.375000 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 1.510 | Reg loss: 0.040 | Tree loss: 1.510 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.387205 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 1.773 | Reg loss: 0.039 | Tree loss: 1.773 | Accuracy: 0.388672 | 0.895 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 1.870 | Reg loss: 0.039 | Tree loss: 1.870 | Accuracy: 0.335938 | 0.895 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 1.823 | Reg loss: 0.039 | Tree loss: 1.823 | Accuracy: 0.376953 | 0.895 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 1.807 | Reg loss: 0.039 | Tree loss: 1.807 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 1.762 | Reg loss: 0.039 | Tree loss: 1.762 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 1.707 | Reg loss: 0.039 | Tree loss: 1.707 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 1.713 | Reg loss: 0.039 | Tree loss: 1.713 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 1.700 | Reg loss: 0.039 | Tree loss: 1.700 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 1.708 | Reg loss: 0.039 | Tree loss: 1.708 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 1.733 | Reg loss: 0.039 | Tree loss: 1.733 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 1.729 | Reg loss: 0.039 | Tree loss: 1.729 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 1.680 | Reg loss: 0.039 | Tree loss: 1.680 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 1.638 | Reg loss: 0.039 | Tree loss: 1.638 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 1.600 | Reg loss: 0.039 | Tree loss: 1.600 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 1.561 | Reg loss: 0.039 | Tree loss: 1.561 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.383838 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 1.861 | Reg loss: 0.039 | Tree loss: 1.861 | Accuracy: 0.371094 | 0.895 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 1.791 | Reg loss: 0.039 | Tree loss: 1.791 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 1.807 | Reg loss: 0.039 | Tree loss: 1.807 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 1.736 | Reg loss: 0.039 | Tree loss: 1.736 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 1.769 | Reg loss: 0.039 | Tree loss: 1.769 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 1.716 | Reg loss: 0.039 | Tree loss: 1.716 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 1.684 | Reg loss: 0.039 | Tree loss: 1.684 | Accuracy: 0.410156 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 1.725 | Reg loss: 0.039 | Tree loss: 1.725 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 1.687 | Reg loss: 0.039 | Tree loss: 1.687 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 1.668 | Reg loss: 0.039 | Tree loss: 1.668 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 1.684 | Reg loss: 0.039 | Tree loss: 1.684 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 1.617 | Reg loss: 0.039 | Tree loss: 1.617 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 1.586 | Reg loss: 0.039 | Tree loss: 1.586 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 1.599 | Reg loss: 0.039 | Tree loss: 1.599 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 1.614 | Reg loss: 0.039 | Tree loss: 1.614 | Accuracy: 0.308594 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 1.586 | Reg loss: 0.039 | Tree loss: 1.586 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 1.642 | Reg loss: 0.039 | Tree loss: 1.642 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 1.552 | Reg loss: 0.039 | Tree loss: 1.552 | Accuracy: 0.416016 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.410156 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.369141 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.377104 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 1.850 | Reg loss: 0.039 | Tree loss: 1.850 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 1.803 | Reg loss: 0.039 | Tree loss: 1.803 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 1.787 | Reg loss: 0.039 | Tree loss: 1.787 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 1.754 | Reg loss: 0.039 | Tree loss: 1.754 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 1.823 | Reg loss: 0.039 | Tree loss: 1.823 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 1.736 | Reg loss: 0.039 | Tree loss: 1.736 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 1.713 | Reg loss: 0.039 | Tree loss: 1.713 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 1.723 | Reg loss: 0.039 | Tree loss: 1.723 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 1.649 | Reg loss: 0.039 | Tree loss: 1.649 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 1.685 | Reg loss: 0.039 | Tree loss: 1.685 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 1.680 | Reg loss: 0.039 | Tree loss: 1.680 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 1.694 | Reg loss: 0.039 | Tree loss: 1.694 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 1.580 | Reg loss: 0.039 | Tree loss: 1.580 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 1.583 | Reg loss: 0.039 | Tree loss: 1.583 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 1.656 | Reg loss: 0.039 | Tree loss: 1.656 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 1.534 | Reg loss: 0.039 | Tree loss: 1.534 | Accuracy: 0.414062 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 1.565 | Reg loss: 0.039 | Tree loss: 1.565 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 1.508 | Reg loss: 0.039 | Tree loss: 1.508 | Accuracy: 0.414062 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 1.558 | Reg loss: 0.039 | Tree loss: 1.558 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 1.563 | Reg loss: 0.039 | Tree loss: 1.563 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.326172 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.333984 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 1.474 | Reg loss: 0.040 | Tree loss: 1.474 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.380471 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 1.868 | Reg loss: 0.038 | Tree loss: 1.868 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 1.803 | Reg loss: 0.038 | Tree loss: 1.803 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 1.762 | Reg loss: 0.038 | Tree loss: 1.762 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 1.822 | Reg loss: 0.038 | Tree loss: 1.822 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 1.757 | Reg loss: 0.038 | Tree loss: 1.757 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 1.675 | Reg loss: 0.039 | Tree loss: 1.675 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 1.708 | Reg loss: 0.039 | Tree loss: 1.708 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 1.688 | Reg loss: 0.039 | Tree loss: 1.688 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 1.682 | Reg loss: 0.039 | Tree loss: 1.682 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 1.669 | Reg loss: 0.039 | Tree loss: 1.669 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 1.596 | Reg loss: 0.039 | Tree loss: 1.596 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 1.546 | Reg loss: 0.039 | Tree loss: 1.546 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 1.620 | Reg loss: 0.039 | Tree loss: 1.620 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 1.592 | Reg loss: 0.039 | Tree loss: 1.592 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 1.541 | Reg loss: 0.039 | Tree loss: 1.541 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 1.557 | Reg loss: 0.039 | Tree loss: 1.557 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 1.580 | Reg loss: 0.039 | Tree loss: 1.580 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 1.508 | Reg loss: 0.039 | Tree loss: 1.508 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 1.510 | Reg loss: 0.040 | Tree loss: 1.510 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 1.482 | Reg loss: 0.040 | Tree loss: 1.482 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.400673 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 1.847 | Reg loss: 0.038 | Tree loss: 1.847 | Accuracy: 0.373047 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 1.819 | Reg loss: 0.038 | Tree loss: 1.819 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 1.850 | Reg loss: 0.038 | Tree loss: 1.850 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 1.805 | Reg loss: 0.038 | Tree loss: 1.805 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 1.756 | Reg loss: 0.038 | Tree loss: 1.756 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 1.678 | Reg loss: 0.038 | Tree loss: 1.678 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 1.702 | Reg loss: 0.038 | Tree loss: 1.702 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 1.717 | Reg loss: 0.038 | Tree loss: 1.717 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 1.667 | Reg loss: 0.039 | Tree loss: 1.667 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 1.728 | Reg loss: 0.039 | Tree loss: 1.728 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 1.616 | Reg loss: 0.039 | Tree loss: 1.616 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 1.602 | Reg loss: 0.039 | Tree loss: 1.602 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 1.625 | Reg loss: 0.039 | Tree loss: 1.625 | Accuracy: 0.410156 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 1.594 | Reg loss: 0.039 | Tree loss: 1.594 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 1.594 | Reg loss: 0.039 | Tree loss: 1.594 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 1.642 | Reg loss: 0.039 | Tree loss: 1.642 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 1.512 | Reg loss: 0.039 | Tree loss: 1.512 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 1.556 | Reg loss: 0.039 | Tree loss: 1.556 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 1.491 | Reg loss: 0.039 | Tree loss: 1.491 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 1.526 | Reg loss: 0.039 | Tree loss: 1.526 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.419922 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.373737 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 1.822 | Reg loss: 0.038 | Tree loss: 1.822 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 1.820 | Reg loss: 0.038 | Tree loss: 1.820 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 1.758 | Reg loss: 0.038 | Tree loss: 1.758 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 1.800 | Reg loss: 0.038 | Tree loss: 1.800 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 1.769 | Reg loss: 0.038 | Tree loss: 1.769 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 1.778 | Reg loss: 0.038 | Tree loss: 1.778 | Accuracy: 0.322266 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 1.720 | Reg loss: 0.038 | Tree loss: 1.720 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.318359 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 1.638 | Reg loss: 0.038 | Tree loss: 1.638 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 1.666 | Reg loss: 0.038 | Tree loss: 1.666 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 1.643 | Reg loss: 0.039 | Tree loss: 1.643 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 1.633 | Reg loss: 0.039 | Tree loss: 1.633 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 1.625 | Reg loss: 0.039 | Tree loss: 1.625 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 1.600 | Reg loss: 0.039 | Tree loss: 1.600 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 1.529 | Reg loss: 0.039 | Tree loss: 1.529 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 1.587 | Reg loss: 0.039 | Tree loss: 1.587 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 1.541 | Reg loss: 0.039 | Tree loss: 1.541 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 1.569 | Reg loss: 0.039 | Tree loss: 1.569 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 1.523 | Reg loss: 0.039 | Tree loss: 1.523 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.416016 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 1.510 | Reg loss: 0.039 | Tree loss: 1.510 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.340067 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 1.841 | Reg loss: 0.038 | Tree loss: 1.841 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 1.788 | Reg loss: 0.038 | Tree loss: 1.788 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 1.868 | Reg loss: 0.038 | Tree loss: 1.868 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 1.800 | Reg loss: 0.038 | Tree loss: 1.800 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 1.772 | Reg loss: 0.038 | Tree loss: 1.772 | Accuracy: 0.353516 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 1.701 | Reg loss: 0.038 | Tree loss: 1.701 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 1.628 | Reg loss: 0.038 | Tree loss: 1.628 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 1.722 | Reg loss: 0.038 | Tree loss: 1.722 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 1.629 | Reg loss: 0.038 | Tree loss: 1.629 | Accuracy: 0.416016 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 1.716 | Reg loss: 0.038 | Tree loss: 1.716 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 1.633 | Reg loss: 0.038 | Tree loss: 1.633 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 1.644 | Reg loss: 0.038 | Tree loss: 1.644 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 1.589 | Reg loss: 0.039 | Tree loss: 1.589 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 1.593 | Reg loss: 0.039 | Tree loss: 1.593 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 1.611 | Reg loss: 0.039 | Tree loss: 1.611 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 1.597 | Reg loss: 0.039 | Tree loss: 1.597 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 1.547 | Reg loss: 0.039 | Tree loss: 1.547 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 1.498 | Reg loss: 0.039 | Tree loss: 1.498 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 1.512 | Reg loss: 0.039 | Tree loss: 1.512 | Accuracy: 0.316406 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 1.473 | Reg loss: 0.039 | Tree loss: 1.473 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 1.483 | Reg loss: 0.039 | Tree loss: 1.483 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 1.473 | Reg loss: 0.039 | Tree loss: 1.473 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 1.488 | Reg loss: 0.039 | Tree loss: 1.488 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.383838 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 1.762 | Reg loss: 0.038 | Tree loss: 1.762 | Accuracy: 0.417969 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 1.828 | Reg loss: 0.038 | Tree loss: 1.828 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 1.788 | Reg loss: 0.038 | Tree loss: 1.788 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 1.728 | Reg loss: 0.038 | Tree loss: 1.728 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 1.734 | Reg loss: 0.038 | Tree loss: 1.734 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 1.744 | Reg loss: 0.038 | Tree loss: 1.744 | Accuracy: 0.326172 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 1.682 | Reg loss: 0.038 | Tree loss: 1.682 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 1.698 | Reg loss: 0.038 | Tree loss: 1.698 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 1.695 | Reg loss: 0.038 | Tree loss: 1.695 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 1.665 | Reg loss: 0.038 | Tree loss: 1.665 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 1.683 | Reg loss: 0.038 | Tree loss: 1.683 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 1.650 | Reg loss: 0.038 | Tree loss: 1.650 | Accuracy: 0.322266 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 1.646 | Reg loss: 0.038 | Tree loss: 1.646 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 1.603 | Reg loss: 0.039 | Tree loss: 1.603 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 1.542 | Reg loss: 0.039 | Tree loss: 1.542 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 1.527 | Reg loss: 0.039 | Tree loss: 1.527 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 1.543 | Reg loss: 0.039 | Tree loss: 1.543 | Accuracy: 0.412109 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 1.598 | Reg loss: 0.039 | Tree loss: 1.598 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 1.485 | Reg loss: 0.039 | Tree loss: 1.485 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 1.529 | Reg loss: 0.039 | Tree loss: 1.529 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 1.462 | Reg loss: 0.039 | Tree loss: 1.462 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 1.496 | Reg loss: 0.039 | Tree loss: 1.496 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 1.407 | Reg loss: 0.039 | Tree loss: 1.407 | Accuracy: 0.445312 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 1.504 | Reg loss: 0.039 | Tree loss: 1.504 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.336700 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 1.856 | Reg loss: 0.038 | Tree loss: 1.856 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 1.821 | Reg loss: 0.038 | Tree loss: 1.821 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 1.728 | Reg loss: 0.038 | Tree loss: 1.728 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 1.756 | Reg loss: 0.038 | Tree loss: 1.756 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 1.724 | Reg loss: 0.038 | Tree loss: 1.724 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 1.700 | Reg loss: 0.038 | Tree loss: 1.700 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 1.722 | Reg loss: 0.038 | Tree loss: 1.722 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 1.738 | Reg loss: 0.038 | Tree loss: 1.738 | Accuracy: 0.357422 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 1.677 | Reg loss: 0.038 | Tree loss: 1.677 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 1.595 | Reg loss: 0.038 | Tree loss: 1.595 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 1.627 | Reg loss: 0.038 | Tree loss: 1.627 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 1.602 | Reg loss: 0.038 | Tree loss: 1.602 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 1.585 | Reg loss: 0.039 | Tree loss: 1.585 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 1.553 | Reg loss: 0.039 | Tree loss: 1.553 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 1.577 | Reg loss: 0.039 | Tree loss: 1.577 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 1.499 | Reg loss: 0.039 | Tree loss: 1.499 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 1.496 | Reg loss: 0.039 | Tree loss: 1.496 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 1.509 | Reg loss: 0.039 | Tree loss: 1.509 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 1.514 | Reg loss: 0.039 | Tree loss: 1.514 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.360269 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 1.873 | Reg loss: 0.038 | Tree loss: 1.873 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 1.792 | Reg loss: 0.038 | Tree loss: 1.792 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 1.782 | Reg loss: 0.038 | Tree loss: 1.782 | Accuracy: 0.328125 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 1.875 | Reg loss: 0.038 | Tree loss: 1.875 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 1.744 | Reg loss: 0.038 | Tree loss: 1.744 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 1.717 | Reg loss: 0.038 | Tree loss: 1.717 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 1.693 | Reg loss: 0.038 | Tree loss: 1.693 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 1.677 | Reg loss: 0.038 | Tree loss: 1.677 | Accuracy: 0.400391 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 1.637 | Reg loss: 0.038 | Tree loss: 1.637 | Accuracy: 0.400391 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 1.602 | Reg loss: 0.038 | Tree loss: 1.602 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 1.659 | Reg loss: 0.038 | Tree loss: 1.659 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 1.625 | Reg loss: 0.038 | Tree loss: 1.625 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 1.606 | Reg loss: 0.038 | Tree loss: 1.606 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 1.530 | Reg loss: 0.039 | Tree loss: 1.530 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 1.557 | Reg loss: 0.039 | Tree loss: 1.557 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 1.520 | Reg loss: 0.039 | Tree loss: 1.520 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 1.540 | Reg loss: 0.039 | Tree loss: 1.540 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 1.544 | Reg loss: 0.039 | Tree loss: 1.544 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 1.476 | Reg loss: 0.039 | Tree loss: 1.476 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 1.523 | Reg loss: 0.039 | Tree loss: 1.523 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 1.474 | Reg loss: 0.039 | Tree loss: 1.474 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 1.509 | Reg loss: 0.039 | Tree loss: 1.509 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.363636 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 1.791 | Reg loss: 0.038 | Tree loss: 1.791 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 1.838 | Reg loss: 0.038 | Tree loss: 1.838 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 1.760 | Reg loss: 0.038 | Tree loss: 1.760 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 1.815 | Reg loss: 0.038 | Tree loss: 1.815 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 1.757 | Reg loss: 0.038 | Tree loss: 1.757 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 1.748 | Reg loss: 0.038 | Tree loss: 1.748 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 1.733 | Reg loss: 0.038 | Tree loss: 1.733 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 1.751 | Reg loss: 0.038 | Tree loss: 1.751 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 1.652 | Reg loss: 0.038 | Tree loss: 1.652 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 1.660 | Reg loss: 0.038 | Tree loss: 1.660 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 1.648 | Reg loss: 0.038 | Tree loss: 1.648 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 1.607 | Reg loss: 0.038 | Tree loss: 1.607 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 1.628 | Reg loss: 0.038 | Tree loss: 1.628 | Accuracy: 0.359375 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 1.607 | Reg loss: 0.038 | Tree loss: 1.607 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 1.585 | Reg loss: 0.038 | Tree loss: 1.585 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 1.535 | Reg loss: 0.038 | Tree loss: 1.535 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 1.552 | Reg loss: 0.038 | Tree loss: 1.552 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 1.507 | Reg loss: 0.039 | Tree loss: 1.507 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 1.495 | Reg loss: 0.039 | Tree loss: 1.495 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 1.476 | Reg loss: 0.039 | Tree loss: 1.476 | Accuracy: 0.408203 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.407407 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 1.830 | Reg loss: 0.038 | Tree loss: 1.830 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 1.821 | Reg loss: 0.038 | Tree loss: 1.821 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 1.749 | Reg loss: 0.038 | Tree loss: 1.749 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 1.746 | Reg loss: 0.038 | Tree loss: 1.746 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 1.761 | Reg loss: 0.038 | Tree loss: 1.761 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 1.750 | Reg loss: 0.038 | Tree loss: 1.750 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 1.668 | Reg loss: 0.038 | Tree loss: 1.668 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 1.740 | Reg loss: 0.038 | Tree loss: 1.740 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 1.690 | Reg loss: 0.038 | Tree loss: 1.690 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 1.592 | Reg loss: 0.038 | Tree loss: 1.592 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 1.654 | Reg loss: 0.038 | Tree loss: 1.654 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 1.611 | Reg loss: 0.038 | Tree loss: 1.611 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 1.630 | Reg loss: 0.038 | Tree loss: 1.630 | Accuracy: 0.419922 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 1.564 | Reg loss: 0.038 | Tree loss: 1.564 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 1.543 | Reg loss: 0.038 | Tree loss: 1.543 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 1.573 | Reg loss: 0.038 | Tree loss: 1.573 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 1.524 | Reg loss: 0.038 | Tree loss: 1.524 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 1.531 | Reg loss: 0.039 | Tree loss: 1.531 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 1.545 | Reg loss: 0.039 | Tree loss: 1.545 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 1.455 | Reg loss: 0.039 | Tree loss: 1.455 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.410156 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 1.516 | Reg loss: 0.039 | Tree loss: 1.516 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 1.489 | Reg loss: 0.039 | Tree loss: 1.489 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 1.476 | Reg loss: 0.039 | Tree loss: 1.476 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 1.462 | Reg loss: 0.039 | Tree loss: 1.462 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 1.408 | Reg loss: 0.039 | Tree loss: 1.408 | Accuracy: 0.367003 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 1.757 | Reg loss: 0.038 | Tree loss: 1.757 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 1.768 | Reg loss: 0.038 | Tree loss: 1.768 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 1.746 | Reg loss: 0.038 | Tree loss: 1.746 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 1.751 | Reg loss: 0.038 | Tree loss: 1.751 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 1.747 | Reg loss: 0.038 | Tree loss: 1.747 | Accuracy: 0.408203 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 1.742 | Reg loss: 0.038 | Tree loss: 1.742 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 1.675 | Reg loss: 0.038 | Tree loss: 1.675 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 1.700 | Reg loss: 0.038 | Tree loss: 1.700 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 1.727 | Reg loss: 0.038 | Tree loss: 1.727 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 1.639 | Reg loss: 0.038 | Tree loss: 1.639 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 1.614 | Reg loss: 0.038 | Tree loss: 1.614 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 1.577 | Reg loss: 0.038 | Tree loss: 1.577 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 1.595 | Reg loss: 0.038 | Tree loss: 1.595 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 1.553 | Reg loss: 0.038 | Tree loss: 1.553 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 1.591 | Reg loss: 0.038 | Tree loss: 1.591 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 1.556 | Reg loss: 0.038 | Tree loss: 1.556 | Accuracy: 0.390625 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 1.547 | Reg loss: 0.038 | Tree loss: 1.547 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 1.509 | Reg loss: 0.039 | Tree loss: 1.509 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 1.510 | Reg loss: 0.039 | Tree loss: 1.510 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 1.545 | Reg loss: 0.039 | Tree loss: 1.545 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 1.469 | Reg loss: 0.039 | Tree loss: 1.469 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 1.493 | Reg loss: 0.039 | Tree loss: 1.493 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 1.492 | Reg loss: 0.039 | Tree loss: 1.492 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 1.467 | Reg loss: 0.039 | Tree loss: 1.467 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 1.473 | Reg loss: 0.039 | Tree loss: 1.473 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 1.518 | Reg loss: 0.039 | Tree loss: 1.518 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 1.448 | Reg loss: 0.039 | Tree loss: 1.448 | Accuracy: 0.417508 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 1.769 | Reg loss: 0.038 | Tree loss: 1.769 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 1.803 | Reg loss: 0.038 | Tree loss: 1.803 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 1.749 | Reg loss: 0.038 | Tree loss: 1.749 | Accuracy: 0.328125 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 1.706 | Reg loss: 0.038 | Tree loss: 1.706 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 1.731 | Reg loss: 0.038 | Tree loss: 1.731 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 1.727 | Reg loss: 0.038 | Tree loss: 1.727 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 1.652 | Reg loss: 0.038 | Tree loss: 1.652 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 1.730 | Reg loss: 0.038 | Tree loss: 1.730 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 1.651 | Reg loss: 0.038 | Tree loss: 1.651 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 1.637 | Reg loss: 0.038 | Tree loss: 1.637 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 1.577 | Reg loss: 0.038 | Tree loss: 1.577 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 1.597 | Reg loss: 0.038 | Tree loss: 1.597 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 1.605 | Reg loss: 0.038 | Tree loss: 1.605 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 1.560 | Reg loss: 0.038 | Tree loss: 1.560 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 1.584 | Reg loss: 0.038 | Tree loss: 1.584 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 1.556 | Reg loss: 0.038 | Tree loss: 1.556 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 1.499 | Reg loss: 0.038 | Tree loss: 1.499 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 1.504 | Reg loss: 0.038 | Tree loss: 1.504 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 1.531 | Reg loss: 0.039 | Tree loss: 1.531 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 1.444 | Reg loss: 0.039 | Tree loss: 1.444 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.339844 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.380471 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 1.789 | Reg loss: 0.037 | Tree loss: 1.789 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 1.770 | Reg loss: 0.037 | Tree loss: 1.770 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 1.797 | Reg loss: 0.037 | Tree loss: 1.797 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 1.778 | Reg loss: 0.037 | Tree loss: 1.778 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 1.725 | Reg loss: 0.037 | Tree loss: 1.725 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 1.756 | Reg loss: 0.037 | Tree loss: 1.756 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 1.675 | Reg loss: 0.037 | Tree loss: 1.675 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 1.703 | Reg loss: 0.038 | Tree loss: 1.703 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 1.647 | Reg loss: 0.038 | Tree loss: 1.647 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 1.621 | Reg loss: 0.038 | Tree loss: 1.621 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 1.562 | Reg loss: 0.038 | Tree loss: 1.562 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 1.646 | Reg loss: 0.038 | Tree loss: 1.646 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 1.589 | Reg loss: 0.038 | Tree loss: 1.589 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 1.534 | Reg loss: 0.038 | Tree loss: 1.534 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 1.516 | Reg loss: 0.038 | Tree loss: 1.516 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 1.543 | Reg loss: 0.038 | Tree loss: 1.543 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 1.523 | Reg loss: 0.038 | Tree loss: 1.523 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 1.444 | Reg loss: 0.038 | Tree loss: 1.444 | Accuracy: 0.425781 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 1.526 | Reg loss: 0.038 | Tree loss: 1.526 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 1.523 | Reg loss: 0.038 | Tree loss: 1.523 | Accuracy: 0.353516 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 1.484 | Reg loss: 0.039 | Tree loss: 1.484 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 1.501 | Reg loss: 0.039 | Tree loss: 1.501 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 1.474 | Reg loss: 0.039 | Tree loss: 1.474 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 1.494 | Reg loss: 0.039 | Tree loss: 1.494 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 1.467 | Reg loss: 0.039 | Tree loss: 1.467 | Accuracy: 0.370370 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 1.809 | Reg loss: 0.037 | Tree loss: 1.809 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 1.749 | Reg loss: 0.037 | Tree loss: 1.749 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 1.728 | Reg loss: 0.037 | Tree loss: 1.728 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 1.767 | Reg loss: 0.037 | Tree loss: 1.767 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 1.762 | Reg loss: 0.037 | Tree loss: 1.762 | Accuracy: 0.333984 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 1.804 | Reg loss: 0.037 | Tree loss: 1.804 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 1.726 | Reg loss: 0.037 | Tree loss: 1.726 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 1.698 | Reg loss: 0.037 | Tree loss: 1.698 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 1.610 | Reg loss: 0.037 | Tree loss: 1.610 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 1.646 | Reg loss: 0.038 | Tree loss: 1.646 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 1.656 | Reg loss: 0.038 | Tree loss: 1.656 | Accuracy: 0.324219 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 1.639 | Reg loss: 0.038 | Tree loss: 1.639 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 1.590 | Reg loss: 0.038 | Tree loss: 1.590 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 1.606 | Reg loss: 0.038 | Tree loss: 1.606 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 1.552 | Reg loss: 0.038 | Tree loss: 1.552 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 1.493 | Reg loss: 0.038 | Tree loss: 1.493 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 1.551 | Reg loss: 0.038 | Tree loss: 1.551 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.394531 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 1.473 | Reg loss: 0.038 | Tree loss: 1.473 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 1.519 | Reg loss: 0.038 | Tree loss: 1.519 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 1.435 | Reg loss: 0.038 | Tree loss: 1.435 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 1.508 | Reg loss: 0.038 | Tree loss: 1.508 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.433594 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 1.511 | Reg loss: 0.039 | Tree loss: 1.511 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 1.482 | Reg loss: 0.039 | Tree loss: 1.482 | Accuracy: 0.400673 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 1.738 | Reg loss: 0.037 | Tree loss: 1.738 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 1.808 | Reg loss: 0.037 | Tree loss: 1.808 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 1.766 | Reg loss: 0.037 | Tree loss: 1.766 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 1.783 | Reg loss: 0.037 | Tree loss: 1.783 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 1.791 | Reg loss: 0.037 | Tree loss: 1.791 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 1.752 | Reg loss: 0.037 | Tree loss: 1.752 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 1.679 | Reg loss: 0.037 | Tree loss: 1.679 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 1.707 | Reg loss: 0.037 | Tree loss: 1.707 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 1.618 | Reg loss: 0.037 | Tree loss: 1.618 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 1.643 | Reg loss: 0.037 | Tree loss: 1.643 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 1.613 | Reg loss: 0.038 | Tree loss: 1.613 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 1.617 | Reg loss: 0.038 | Tree loss: 1.617 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 1.596 | Reg loss: 0.038 | Tree loss: 1.596 | Accuracy: 0.332031 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 1.591 | Reg loss: 0.038 | Tree loss: 1.591 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 1.563 | Reg loss: 0.038 | Tree loss: 1.563 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 1.530 | Reg loss: 0.038 | Tree loss: 1.530 | Accuracy: 0.414062 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 1.515 | Reg loss: 0.038 | Tree loss: 1.515 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 1.521 | Reg loss: 0.038 | Tree loss: 1.521 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 1.516 | Reg loss: 0.038 | Tree loss: 1.516 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 1.514 | Reg loss: 0.038 | Tree loss: 1.514 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 1.506 | Reg loss: 0.038 | Tree loss: 1.506 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 1.509 | Reg loss: 0.038 | Tree loss: 1.509 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 1.453 | Reg loss: 0.039 | Tree loss: 1.453 | Accuracy: 0.392578 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.387205 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 1.843 | Reg loss: 0.037 | Tree loss: 1.843 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 1.832 | Reg loss: 0.037 | Tree loss: 1.832 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 1.795 | Reg loss: 0.037 | Tree loss: 1.795 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 1.740 | Reg loss: 0.037 | Tree loss: 1.740 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 1.765 | Reg loss: 0.037 | Tree loss: 1.765 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 1.714 | Reg loss: 0.037 | Tree loss: 1.714 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 1.754 | Reg loss: 0.037 | Tree loss: 1.754 | Accuracy: 0.330078 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 1.709 | Reg loss: 0.037 | Tree loss: 1.709 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 1.629 | Reg loss: 0.037 | Tree loss: 1.629 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 1.636 | Reg loss: 0.037 | Tree loss: 1.636 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 1.559 | Reg loss: 0.037 | Tree loss: 1.559 | Accuracy: 0.412109 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 1.620 | Reg loss: 0.037 | Tree loss: 1.620 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 1.641 | Reg loss: 0.038 | Tree loss: 1.641 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 1.557 | Reg loss: 0.038 | Tree loss: 1.557 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 1.481 | Reg loss: 0.038 | Tree loss: 1.481 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 1.504 | Reg loss: 0.038 | Tree loss: 1.504 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 1.491 | Reg loss: 0.038 | Tree loss: 1.491 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 1.508 | Reg loss: 0.038 | Tree loss: 1.508 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 1.411 | Reg loss: 0.038 | Tree loss: 1.411 | Accuracy: 0.402344 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 1.428 | Reg loss: 0.038 | Tree loss: 1.428 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 1.478 | Reg loss: 0.038 | Tree loss: 1.478 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 1.476 | Reg loss: 0.039 | Tree loss: 1.476 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 1.503 | Reg loss: 0.039 | Tree loss: 1.503 | Accuracy: 0.367003 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 1.849 | Reg loss: 0.037 | Tree loss: 1.849 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 1.779 | Reg loss: 0.037 | Tree loss: 1.779 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 1.787 | Reg loss: 0.037 | Tree loss: 1.787 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 1.748 | Reg loss: 0.037 | Tree loss: 1.748 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 1.800 | Reg loss: 0.037 | Tree loss: 1.800 | Accuracy: 0.400391 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 1.780 | Reg loss: 0.037 | Tree loss: 1.780 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 1.682 | Reg loss: 0.037 | Tree loss: 1.682 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 1.657 | Reg loss: 0.037 | Tree loss: 1.657 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 1.677 | Reg loss: 0.037 | Tree loss: 1.677 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 1.649 | Reg loss: 0.037 | Tree loss: 1.649 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 1.596 | Reg loss: 0.037 | Tree loss: 1.596 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 1.587 | Reg loss: 0.037 | Tree loss: 1.587 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 1.560 | Reg loss: 0.037 | Tree loss: 1.560 | Accuracy: 0.335938 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 1.523 | Reg loss: 0.038 | Tree loss: 1.523 | Accuracy: 0.421875 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 1.565 | Reg loss: 0.038 | Tree loss: 1.565 | Accuracy: 0.396484 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 1.506 | Reg loss: 0.038 | Tree loss: 1.506 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 1.541 | Reg loss: 0.038 | Tree loss: 1.541 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 1.541 | Reg loss: 0.038 | Tree loss: 1.541 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 1.526 | Reg loss: 0.038 | Tree loss: 1.526 | Accuracy: 0.361328 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 1.425 | Reg loss: 0.038 | Tree loss: 1.425 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 1.440 | Reg loss: 0.038 | Tree loss: 1.440 | Accuracy: 0.353516 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 1.437 | Reg loss: 0.038 | Tree loss: 1.437 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 1.492 | Reg loss: 0.038 | Tree loss: 1.492 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 1.426 | Reg loss: 0.038 | Tree loss: 1.426 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 1.387 | Reg loss: 0.039 | Tree loss: 1.387 | Accuracy: 0.387205 | 0.894 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 1.793 | Reg loss: 0.037 | Tree loss: 1.793 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 1.799 | Reg loss: 0.037 | Tree loss: 1.799 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 1.765 | Reg loss: 0.037 | Tree loss: 1.765 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 1.777 | Reg loss: 0.037 | Tree loss: 1.777 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 1.752 | Reg loss: 0.037 | Tree loss: 1.752 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 1.695 | Reg loss: 0.037 | Tree loss: 1.695 | Accuracy: 0.337891 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 1.722 | Reg loss: 0.037 | Tree loss: 1.722 | Accuracy: 0.347656 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 1.669 | Reg loss: 0.037 | Tree loss: 1.669 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 1.657 | Reg loss: 0.037 | Tree loss: 1.657 | Accuracy: 0.351562 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 1.660 | Reg loss: 0.037 | Tree loss: 1.660 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 1.546 | Reg loss: 0.037 | Tree loss: 1.546 | Accuracy: 0.376953 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 1.570 | Reg loss: 0.037 | Tree loss: 1.570 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 1.529 | Reg loss: 0.037 | Tree loss: 1.529 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 1.599 | Reg loss: 0.037 | Tree loss: 1.599 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 1.555 | Reg loss: 0.037 | Tree loss: 1.555 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 1.539 | Reg loss: 0.038 | Tree loss: 1.539 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 1.541 | Reg loss: 0.038 | Tree loss: 1.541 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 1.511 | Reg loss: 0.038 | Tree loss: 1.511 | Accuracy: 0.388672 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 1.515 | Reg loss: 0.038 | Tree loss: 1.515 | Accuracy: 0.380859 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.369141 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 1.526 | Reg loss: 0.038 | Tree loss: 1.526 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 1.509 | Reg loss: 0.038 | Tree loss: 1.509 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 1.442 | Reg loss: 0.038 | Tree loss: 1.442 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 1.443 | Reg loss: 0.038 | Tree loss: 1.443 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 1.454 | Reg loss: 0.038 | Tree loss: 1.454 | Accuracy: 0.390625 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.341797 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 1.455 | Reg loss: 0.038 | Tree loss: 1.455 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.367003 | 0.894 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 1.873 | Reg loss: 0.037 | Tree loss: 1.873 | Accuracy: 0.384766 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 1.791 | Reg loss: 0.037 | Tree loss: 1.791 | Accuracy: 0.367188 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 1.816 | Reg loss: 0.037 | Tree loss: 1.816 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 1.749 | Reg loss: 0.037 | Tree loss: 1.749 | Accuracy: 0.357422 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 1.706 | Reg loss: 0.037 | Tree loss: 1.706 | Accuracy: 0.382812 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 1.704 | Reg loss: 0.037 | Tree loss: 1.704 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 1.700 | Reg loss: 0.037 | Tree loss: 1.700 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 1.699 | Reg loss: 0.037 | Tree loss: 1.699 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 1.641 | Reg loss: 0.037 | Tree loss: 1.641 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 1.693 | Reg loss: 0.037 | Tree loss: 1.693 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 1.590 | Reg loss: 0.037 | Tree loss: 1.590 | Accuracy: 0.359375 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 1.573 | Reg loss: 0.037 | Tree loss: 1.573 | Accuracy: 0.392578 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 1.543 | Reg loss: 0.037 | Tree loss: 1.543 | Accuracy: 0.416016 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 1.555 | Reg loss: 0.037 | Tree loss: 1.555 | Accuracy: 0.378906 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 1.561 | Reg loss: 0.037 | Tree loss: 1.561 | Accuracy: 0.398438 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.345703 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 1.522 | Reg loss: 0.038 | Tree loss: 1.522 | Accuracy: 0.343750 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 1.522 | Reg loss: 0.038 | Tree loss: 1.522 | Accuracy: 0.373047 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 1.532 | Reg loss: 0.038 | Tree loss: 1.532 | Accuracy: 0.375000 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.365234 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.363281 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.333984 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 1.501 | Reg loss: 0.038 | Tree loss: 1.501 | Accuracy: 0.371094 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 1.469 | Reg loss: 0.038 | Tree loss: 1.469 | Accuracy: 0.349609 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 1.387 | Reg loss: 0.038 | Tree loss: 1.387 | Accuracy: 0.386719 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.404297 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 1.425 | Reg loss: 0.038 | Tree loss: 1.425 | Accuracy: 0.406250 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 1.436 | Reg loss: 0.038 | Tree loss: 1.436 | Accuracy: 0.355469 | 0.894 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 1.418 | Reg loss: 0.038 | Tree loss: 1.418 | Accuracy: 0.367003 | 0.894 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5894f8d006484b108b42c11cfce0a82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279e595b488843369d17af61bb5427e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f251234e2cf4f0b8bb2f01b51cdbfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6b15d1324e4f02826ef999420fdda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 9.652777777777779\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 720\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "14633\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "Average comprehensibility: 48.25555555555555\n",
      "std comprehensibility: 3.8313360336140105\n",
      "var comprehensibility: 14.679135802469137\n",
      "minimum comprehensibility: 34\n",
      "maximum comprehensibility: 56\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
