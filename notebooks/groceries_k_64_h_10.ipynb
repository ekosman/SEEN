{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.198735237121582 | KNN Loss: 6.228330612182617 | BCE Loss: 1.9704046249389648\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.193399429321289 | KNN Loss: 6.228418827056885 | BCE Loss: 1.9649808406829834\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.223655700683594 | KNN Loss: 6.228407859802246 | BCE Loss: 1.9952478408813477\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.12940502166748 | KNN Loss: 6.228500843048096 | BCE Loss: 1.9009044170379639\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.163383483886719 | KNN Loss: 6.227762699127197 | BCE Loss: 1.9356210231781006\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.170571327209473 | KNN Loss: 6.227832317352295 | BCE Loss: 1.9427390098571777\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.164538383483887 | KNN Loss: 6.22742223739624 | BCE Loss: 1.9371163845062256\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.091979026794434 | KNN Loss: 6.22742223739624 | BCE Loss: 1.864557147026062\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.138309478759766 | KNN Loss: 6.227060794830322 | BCE Loss: 1.9112484455108643\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.188477516174316 | KNN Loss: 6.226654052734375 | BCE Loss: 1.9618231058120728\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.107386589050293 | KNN Loss: 6.227078437805176 | BCE Loss: 1.8803083896636963\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.128471374511719 | KNN Loss: 6.226413726806641 | BCE Loss: 1.9020581245422363\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.06139087677002 | KNN Loss: 6.2262420654296875 | BCE Loss: 1.8351490497589111\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.11184310913086 | KNN Loss: 6.2257184982299805 | BCE Loss: 1.8861243724822998\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.119997024536133 | KNN Loss: 6.225257873535156 | BCE Loss: 1.8947396278381348\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.08238410949707 | KNN Loss: 6.224967002868652 | BCE Loss: 1.8574168682098389\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.070351600646973 | KNN Loss: 6.225057601928711 | BCE Loss: 1.8452942371368408\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.12153434753418 | KNN Loss: 6.224717617034912 | BCE Loss: 1.8968162536621094\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.024478912353516 | KNN Loss: 6.224014759063721 | BCE Loss: 1.8004636764526367\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.04930305480957 | KNN Loss: 6.224030494689941 | BCE Loss: 1.8252723217010498\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.03630256652832 | KNN Loss: 6.223483562469482 | BCE Loss: 1.8128188848495483\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.011648178100586 | KNN Loss: 6.223085403442383 | BCE Loss: 1.7885630130767822\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.991998195648193 | KNN Loss: 6.223311424255371 | BCE Loss: 1.7686866521835327\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.99554443359375 | KNN Loss: 6.221780300140381 | BCE Loss: 1.7737641334533691\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.9688239097595215 | KNN Loss: 6.222075462341309 | BCE Loss: 1.746748447418213\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.979794502258301 | KNN Loss: 6.2206902503967285 | BCE Loss: 1.7591041326522827\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.987130641937256 | KNN Loss: 6.220433712005615 | BCE Loss: 1.766696810722351\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.944937705993652 | KNN Loss: 6.219139575958252 | BCE Loss: 1.7257981300354004\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.904646396636963 | KNN Loss: 6.218780040740967 | BCE Loss: 1.685866355895996\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.9083662033081055 | KNN Loss: 6.218479633331299 | BCE Loss: 1.6898863315582275\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.929448127746582 | KNN Loss: 6.218143939971924 | BCE Loss: 1.7113043069839478\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.915332794189453 | KNN Loss: 6.215606689453125 | BCE Loss: 1.6997263431549072\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.880497455596924 | KNN Loss: 6.214993953704834 | BCE Loss: 1.6655036211013794\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.848193168640137 | KNN Loss: 6.213305950164795 | BCE Loss: 1.6348869800567627\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.900681495666504 | KNN Loss: 6.2139105796813965 | BCE Loss: 1.686771035194397\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.855765342712402 | KNN Loss: 6.21083927154541 | BCE Loss: 1.6449260711669922\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.848625183105469 | KNN Loss: 6.21248197555542 | BCE Loss: 1.6361432075500488\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.835630416870117 | KNN Loss: 6.2077317237854 | BCE Loss: 1.627898931503296\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.800005912780762 | KNN Loss: 6.205859661102295 | BCE Loss: 1.5941462516784668\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.80532169342041 | KNN Loss: 6.203985214233398 | BCE Loss: 1.6013364791870117\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.741838455200195 | KNN Loss: 6.201145172119141 | BCE Loss: 1.5406932830810547\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.782083034515381 | KNN Loss: 6.196651935577393 | BCE Loss: 1.5854312181472778\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.742209434509277 | KNN Loss: 6.195668697357178 | BCE Loss: 1.5465409755706787\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.7363481521606445 | KNN Loss: 6.189661979675293 | BCE Loss: 1.5466861724853516\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.684934616088867 | KNN Loss: 6.18641996383667 | BCE Loss: 1.4985147714614868\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.658638000488281 | KNN Loss: 6.184974193572998 | BCE Loss: 1.4736639261245728\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.662009239196777 | KNN Loss: 6.176707744598389 | BCE Loss: 1.4853014945983887\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.613367557525635 | KNN Loss: 6.171292781829834 | BCE Loss: 1.4420747756958008\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.597665786743164 | KNN Loss: 6.157472133636475 | BCE Loss: 1.4401934146881104\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.617006778717041 | KNN Loss: 6.152332782745361 | BCE Loss: 1.4646739959716797\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.5059895515441895 | KNN Loss: 6.13755464553833 | BCE Loss: 1.3684349060058594\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.5055341720581055 | KNN Loss: 6.135177135467529 | BCE Loss: 1.3703572750091553\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.479269504547119 | KNN Loss: 6.129935264587402 | BCE Loss: 1.3493342399597168\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.440084457397461 | KNN Loss: 6.105763912200928 | BCE Loss: 1.3343207836151123\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.432408332824707 | KNN Loss: 6.098694324493408 | BCE Loss: 1.3337137699127197\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.394471168518066 | KNN Loss: 6.06968879699707 | BCE Loss: 1.324782133102417\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.330428123474121 | KNN Loss: 6.037522315979004 | BCE Loss: 1.2929060459136963\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.293586730957031 | KNN Loss: 6.009475231170654 | BCE Loss: 1.2841116189956665\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.2216715812683105 | KNN Loss: 5.970983982086182 | BCE Loss: 1.250687599182129\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.1610541343688965 | KNN Loss: 5.946035861968994 | BCE Loss: 1.215018391609192\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.095639705657959 | KNN Loss: 5.911949634552002 | BCE Loss: 1.183690071105957\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.090079307556152 | KNN Loss: 5.872847080230713 | BCE Loss: 1.2172322273254395\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.979384422302246 | KNN Loss: 5.822093963623047 | BCE Loss: 1.1572903394699097\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.967721939086914 | KNN Loss: 5.777159690856934 | BCE Loss: 1.1905622482299805\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.858154773712158 | KNN Loss: 5.708370685577393 | BCE Loss: 1.149783968925476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 6.738680839538574 | KNN Loss: 5.639242649078369 | BCE Loss: 1.099438190460205\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 6.765195846557617 | KNN Loss: 5.623289108276367 | BCE Loss: 1.14190673828125\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 6.651707649230957 | KNN Loss: 5.5264081954956055 | BCE Loss: 1.1252994537353516\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.5760297775268555 | KNN Loss: 5.452567100524902 | BCE Loss: 1.1234629154205322\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.507997035980225 | KNN Loss: 5.396655082702637 | BCE Loss: 1.1113418340682983\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.445337295532227 | KNN Loss: 5.33567476272583 | BCE Loss: 1.1096622943878174\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.339080810546875 | KNN Loss: 5.23173189163208 | BCE Loss: 1.107349157333374\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.220019817352295 | KNN Loss: 5.13062858581543 | BCE Loss: 1.0893912315368652\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.198722839355469 | KNN Loss: 5.085445880889893 | BCE Loss: 1.1132769584655762\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 6.088690757751465 | KNN Loss: 4.99547815322876 | BCE Loss: 1.0932128429412842\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 6.063554763793945 | KNN Loss: 4.9732232093811035 | BCE Loss: 1.0903316736221313\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.965346336364746 | KNN Loss: 4.894357204437256 | BCE Loss: 1.0709893703460693\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.923144817352295 | KNN Loss: 4.828594207763672 | BCE Loss: 1.0945507287979126\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.845232963562012 | KNN Loss: 4.788061141967773 | BCE Loss: 1.0571715831756592\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.826160430908203 | KNN Loss: 4.719123363494873 | BCE Loss: 1.10703706741333\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.779658794403076 | KNN Loss: 4.698838233947754 | BCE Loss: 1.0808206796646118\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.774689674377441 | KNN Loss: 4.665031909942627 | BCE Loss: 1.1096580028533936\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.740680694580078 | KNN Loss: 4.648712158203125 | BCE Loss: 1.0919684171676636\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.737476348876953 | KNN Loss: 4.6277546882629395 | BCE Loss: 1.1097217798233032\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.7076921463012695 | KNN Loss: 4.620841979980469 | BCE Loss: 1.0868499279022217\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.638866424560547 | KNN Loss: 4.58184289932251 | BCE Loss: 1.057023286819458\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.664361476898193 | KNN Loss: 4.58803653717041 | BCE Loss: 1.0763249397277832\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.628719329833984 | KNN Loss: 4.551789283752441 | BCE Loss: 1.0769299268722534\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.626363277435303 | KNN Loss: 4.56543493270874 | BCE Loss: 1.060928463935852\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.613162994384766 | KNN Loss: 4.5426177978515625 | BCE Loss: 1.070544958114624\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.595886707305908 | KNN Loss: 4.542139530181885 | BCE Loss: 1.0537471771240234\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.6501922607421875 | KNN Loss: 4.545339584350586 | BCE Loss: 1.1048529148101807\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 5.6198883056640625 | KNN Loss: 4.532880783081055 | BCE Loss: 1.087007761001587\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 5.539406776428223 | KNN Loss: 4.491871356964111 | BCE Loss: 1.0475351810455322\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.575428485870361 | KNN Loss: 4.491886138916016 | BCE Loss: 1.0835423469543457\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 5.5780487060546875 | KNN Loss: 4.522240161895752 | BCE Loss: 1.0558083057403564\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 5.565576553344727 | KNN Loss: 4.4938225746154785 | BCE Loss: 1.0717542171478271\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 5.564735412597656 | KNN Loss: 4.4911580085754395 | BCE Loss: 1.0735774040222168\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 5.567206382751465 | KNN Loss: 4.469428539276123 | BCE Loss: 1.0977776050567627\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 5.52565336227417 | KNN Loss: 4.479005813598633 | BCE Loss: 1.0466474294662476\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 5.513641357421875 | KNN Loss: 4.479377269744873 | BCE Loss: 1.034264087677002\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 5.601781845092773 | KNN Loss: 4.542788505554199 | BCE Loss: 1.0589933395385742\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 5.540634632110596 | KNN Loss: 4.464162349700928 | BCE Loss: 1.0764721632003784\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 5.519537448883057 | KNN Loss: 4.4765777587890625 | BCE Loss: 1.0429595708847046\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 5.550624370574951 | KNN Loss: 4.475300312042236 | BCE Loss: 1.0753240585327148\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 5.562231540679932 | KNN Loss: 4.497986793518066 | BCE Loss: 1.0642448663711548\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 5.523497581481934 | KNN Loss: 4.4876556396484375 | BCE Loss: 1.035841703414917\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 5.502808570861816 | KNN Loss: 4.457554340362549 | BCE Loss: 1.0452543497085571\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 5.546436786651611 | KNN Loss: 4.500146865844727 | BCE Loss: 1.0462898015975952\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 5.517206192016602 | KNN Loss: 4.441792011260986 | BCE Loss: 1.0754141807556152\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 5.549900054931641 | KNN Loss: 4.469597339630127 | BCE Loss: 1.0803029537200928\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 5.518847942352295 | KNN Loss: 4.461182594299316 | BCE Loss: 1.057665228843689\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 5.508149147033691 | KNN Loss: 4.4434661865234375 | BCE Loss: 1.0646827220916748\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 5.543177127838135 | KNN Loss: 4.4866766929626465 | BCE Loss: 1.0565005540847778\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 5.535149097442627 | KNN Loss: 4.4435343742370605 | BCE Loss: 1.0916147232055664\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 5.515933036804199 | KNN Loss: 4.458346366882324 | BCE Loss: 1.057586669921875\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 5.525915622711182 | KNN Loss: 4.463797092437744 | BCE Loss: 1.0621185302734375\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 5.524281024932861 | KNN Loss: 4.440916538238525 | BCE Loss: 1.0833646059036255\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 5.522876262664795 | KNN Loss: 4.468228816986084 | BCE Loss: 1.0546475648880005\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 5.497211456298828 | KNN Loss: 4.434807777404785 | BCE Loss: 1.0624034404754639\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 5.5363264083862305 | KNN Loss: 4.466361999511719 | BCE Loss: 1.0699644088745117\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 5.464751243591309 | KNN Loss: 4.442645072937012 | BCE Loss: 1.0221059322357178\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 5.5085768699646 | KNN Loss: 4.467939376831055 | BCE Loss: 1.0406373739242554\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 5.521228790283203 | KNN Loss: 4.4385223388671875 | BCE Loss: 1.0827066898345947\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 5.52765417098999 | KNN Loss: 4.459543228149414 | BCE Loss: 1.0681109428405762\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 5.479345321655273 | KNN Loss: 4.436276912689209 | BCE Loss: 1.043068289756775\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 5.524144649505615 | KNN Loss: 4.443399906158447 | BCE Loss: 1.0807448625564575\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 5.4981536865234375 | KNN Loss: 4.453258037567139 | BCE Loss: 1.0448956489562988\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 5.531518459320068 | KNN Loss: 4.448282718658447 | BCE Loss: 1.083235740661621\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 5.495976448059082 | KNN Loss: 4.456255912780762 | BCE Loss: 1.0397205352783203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 5.498337745666504 | KNN Loss: 4.450432777404785 | BCE Loss: 1.0479047298431396\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 5.499588966369629 | KNN Loss: 4.450000286102295 | BCE Loss: 1.049588918685913\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 5.5169782638549805 | KNN Loss: 4.461717128753662 | BCE Loss: 1.0552613735198975\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 5.492061614990234 | KNN Loss: 4.438493251800537 | BCE Loss: 1.0535683631896973\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 5.5384840965271 | KNN Loss: 4.464260101318359 | BCE Loss: 1.0742239952087402\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 5.467388153076172 | KNN Loss: 4.433053493499756 | BCE Loss: 1.0343348979949951\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 5.527736663818359 | KNN Loss: 4.471822261810303 | BCE Loss: 1.0559141635894775\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 5.500675201416016 | KNN Loss: 4.458330154418945 | BCE Loss: 1.0423450469970703\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 5.488903999328613 | KNN Loss: 4.455028057098389 | BCE Loss: 1.0338759422302246\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 5.5124359130859375 | KNN Loss: 4.449385166168213 | BCE Loss: 1.0630507469177246\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 5.476551055908203 | KNN Loss: 4.42698335647583 | BCE Loss: 1.0495678186416626\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 5.480658531188965 | KNN Loss: 4.4440436363220215 | BCE Loss: 1.0366151332855225\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 5.485986709594727 | KNN Loss: 4.437436103820801 | BCE Loss: 1.0485508441925049\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 5.5131731033325195 | KNN Loss: 4.461548805236816 | BCE Loss: 1.0516242980957031\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 5.499399185180664 | KNN Loss: 4.449498176574707 | BCE Loss: 1.0499012470245361\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 5.437406539916992 | KNN Loss: 4.410968780517578 | BCE Loss: 1.026437520980835\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 5.523153305053711 | KNN Loss: 4.471043109893799 | BCE Loss: 1.052110195159912\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 5.496517181396484 | KNN Loss: 4.453275203704834 | BCE Loss: 1.0432417392730713\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 5.432376384735107 | KNN Loss: 4.405025005340576 | BCE Loss: 1.0273512601852417\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 5.526998519897461 | KNN Loss: 4.447545051574707 | BCE Loss: 1.0794533491134644\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 5.473816871643066 | KNN Loss: 4.421035289764404 | BCE Loss: 1.052781343460083\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 5.5247931480407715 | KNN Loss: 4.46336030960083 | BCE Loss: 1.0614328384399414\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 5.479473114013672 | KNN Loss: 4.4113006591796875 | BCE Loss: 1.068172574043274\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 5.508099555969238 | KNN Loss: 4.43142032623291 | BCE Loss: 1.0766792297363281\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 5.509793281555176 | KNN Loss: 4.43966817855835 | BCE Loss: 1.0701252222061157\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 5.476472854614258 | KNN Loss: 4.439229488372803 | BCE Loss: 1.037243366241455\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 5.496512413024902 | KNN Loss: 4.450018405914307 | BCE Loss: 1.0464941263198853\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 5.488073825836182 | KNN Loss: 4.438174724578857 | BCE Loss: 1.0498992204666138\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 5.456526279449463 | KNN Loss: 4.428472995758057 | BCE Loss: 1.0280532836914062\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 5.488461494445801 | KNN Loss: 4.472182273864746 | BCE Loss: 1.0162792205810547\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 5.534274101257324 | KNN Loss: 4.458035945892334 | BCE Loss: 1.0762383937835693\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 5.452680587768555 | KNN Loss: 4.401986598968506 | BCE Loss: 1.0506937503814697\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 5.46168327331543 | KNN Loss: 4.396082878112793 | BCE Loss: 1.0656001567840576\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 5.488304615020752 | KNN Loss: 4.454250335693359 | BCE Loss: 1.0340542793273926\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 5.523924827575684 | KNN Loss: 4.479297161102295 | BCE Loss: 1.0446274280548096\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 5.489473819732666 | KNN Loss: 4.450003623962402 | BCE Loss: 1.0394701957702637\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 5.481738090515137 | KNN Loss: 4.449608325958252 | BCE Loss: 1.0321297645568848\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 5.468203544616699 | KNN Loss: 4.413759231567383 | BCE Loss: 1.0544440746307373\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 5.427999973297119 | KNN Loss: 4.401965141296387 | BCE Loss: 1.026034951210022\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 5.532367706298828 | KNN Loss: 4.450295925140381 | BCE Loss: 1.0820716619491577\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 5.504716873168945 | KNN Loss: 4.450650215148926 | BCE Loss: 1.05406653881073\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 5.482645034790039 | KNN Loss: 4.448179721832275 | BCE Loss: 1.0344651937484741\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 5.456479072570801 | KNN Loss: 4.410147190093994 | BCE Loss: 1.0463316440582275\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 5.468265533447266 | KNN Loss: 4.413167476654053 | BCE Loss: 1.055098056793213\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 5.451803684234619 | KNN Loss: 4.419350624084473 | BCE Loss: 1.0324530601501465\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 5.427349090576172 | KNN Loss: 4.405901908874512 | BCE Loss: 1.021446943283081\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 5.502771377563477 | KNN Loss: 4.463503837585449 | BCE Loss: 1.0392675399780273\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 5.478598594665527 | KNN Loss: 4.423925876617432 | BCE Loss: 1.0546727180480957\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 5.565123558044434 | KNN Loss: 4.510465621948242 | BCE Loss: 1.054658055305481\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 5.483814239501953 | KNN Loss: 4.438905715942383 | BCE Loss: 1.0449085235595703\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 5.466243267059326 | KNN Loss: 4.438725471496582 | BCE Loss: 1.0275176763534546\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 5.506234169006348 | KNN Loss: 4.443036079406738 | BCE Loss: 1.0631980895996094\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 5.466581344604492 | KNN Loss: 4.432947635650635 | BCE Loss: 1.0336335897445679\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 5.447153091430664 | KNN Loss: 4.41356086730957 | BCE Loss: 1.0335921049118042\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 5.452552795410156 | KNN Loss: 4.407415866851807 | BCE Loss: 1.0451371669769287\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 5.464810371398926 | KNN Loss: 4.422998905181885 | BCE Loss: 1.041811466217041\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 5.488319396972656 | KNN Loss: 4.425764560699463 | BCE Loss: 1.062554955482483\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 5.455090045928955 | KNN Loss: 4.427516460418701 | BCE Loss: 1.0275737047195435\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 5.46333122253418 | KNN Loss: 4.424269676208496 | BCE Loss: 1.0390617847442627\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 5.532217025756836 | KNN Loss: 4.477729320526123 | BCE Loss: 1.0544874668121338\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 5.435215950012207 | KNN Loss: 4.404117107391357 | BCE Loss: 1.0310986042022705\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 5.489964485168457 | KNN Loss: 4.430435657501221 | BCE Loss: 1.0595287084579468\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 5.4876708984375 | KNN Loss: 4.443597793579102 | BCE Loss: 1.0440733432769775\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 5.484528064727783 | KNN Loss: 4.428069591522217 | BCE Loss: 1.0564584732055664\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 5.473808765411377 | KNN Loss: 4.452079772949219 | BCE Loss: 1.0217289924621582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 5.4654645919799805 | KNN Loss: 4.422767162322998 | BCE Loss: 1.0426971912384033\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 5.493514060974121 | KNN Loss: 4.415186405181885 | BCE Loss: 1.0783274173736572\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 5.479903221130371 | KNN Loss: 4.41825532913208 | BCE Loss: 1.0616480112075806\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 5.450873374938965 | KNN Loss: 4.432248592376709 | BCE Loss: 1.018625020980835\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 5.460605621337891 | KNN Loss: 4.403741359710693 | BCE Loss: 1.0568640232086182\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 5.501046180725098 | KNN Loss: 4.443245887756348 | BCE Loss: 1.05780029296875\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 5.489946365356445 | KNN Loss: 4.433122158050537 | BCE Loss: 1.0568244457244873\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 5.447597980499268 | KNN Loss: 4.405772686004639 | BCE Loss: 1.0418251752853394\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 5.465639591217041 | KNN Loss: 4.423444747924805 | BCE Loss: 1.0421949625015259\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 5.4780964851379395 | KNN Loss: 4.410581588745117 | BCE Loss: 1.0675148963928223\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 5.489006996154785 | KNN Loss: 4.438170433044434 | BCE Loss: 1.0508365631103516\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 5.465978145599365 | KNN Loss: 4.425069332122803 | BCE Loss: 1.0409088134765625\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 5.482454299926758 | KNN Loss: 4.420711517333984 | BCE Loss: 1.0617427825927734\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 5.4745869636535645 | KNN Loss: 4.417113304138184 | BCE Loss: 1.0574736595153809\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 5.449963092803955 | KNN Loss: 4.390226364135742 | BCE Loss: 1.0597366094589233\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 5.461246490478516 | KNN Loss: 4.400275230407715 | BCE Loss: 1.0609710216522217\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 5.472673416137695 | KNN Loss: 4.430796146392822 | BCE Loss: 1.0418773889541626\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 5.5475664138793945 | KNN Loss: 4.500829219818115 | BCE Loss: 1.0467369556427002\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 5.501132488250732 | KNN Loss: 4.427492618560791 | BCE Loss: 1.0736397504806519\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 5.452935218811035 | KNN Loss: 4.418550968170166 | BCE Loss: 1.0343844890594482\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 5.445436954498291 | KNN Loss: 4.421954154968262 | BCE Loss: 1.0234826803207397\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 5.448369979858398 | KNN Loss: 4.4235358238220215 | BCE Loss: 1.0248339176177979\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 5.470973014831543 | KNN Loss: 4.413471221923828 | BCE Loss: 1.057502031326294\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 5.491024017333984 | KNN Loss: 4.436496257781982 | BCE Loss: 1.0545275211334229\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 5.4979248046875 | KNN Loss: 4.4208879470825195 | BCE Loss: 1.0770366191864014\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 5.431478023529053 | KNN Loss: 4.404995441436768 | BCE Loss: 1.0264827013015747\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 5.4582366943359375 | KNN Loss: 4.415105819702148 | BCE Loss: 1.0431311130523682\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 5.532911777496338 | KNN Loss: 4.461220741271973 | BCE Loss: 1.0716910362243652\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 5.43172550201416 | KNN Loss: 4.40209436416626 | BCE Loss: 1.0296308994293213\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 5.44059944152832 | KNN Loss: 4.398336410522461 | BCE Loss: 1.0422630310058594\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 5.506889343261719 | KNN Loss: 4.450742244720459 | BCE Loss: 1.0561472177505493\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 5.465493679046631 | KNN Loss: 4.404353141784668 | BCE Loss: 1.0611406564712524\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 5.425917148590088 | KNN Loss: 4.378850936889648 | BCE Loss: 1.04706609249115\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 5.503028869628906 | KNN Loss: 4.43216609954834 | BCE Loss: 1.0708625316619873\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 5.460546493530273 | KNN Loss: 4.4430341720581055 | BCE Loss: 1.017512321472168\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 5.4860520362854 | KNN Loss: 4.43275785446167 | BCE Loss: 1.0532941818237305\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 5.498002052307129 | KNN Loss: 4.4264116287231445 | BCE Loss: 1.0715901851654053\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 5.466312408447266 | KNN Loss: 4.419142723083496 | BCE Loss: 1.0471694469451904\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 5.457663536071777 | KNN Loss: 4.4195556640625 | BCE Loss: 1.0381081104278564\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 5.439775466918945 | KNN Loss: 4.408651351928711 | BCE Loss: 1.031124234199524\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 5.458018779754639 | KNN Loss: 4.4096550941467285 | BCE Loss: 1.0483636856079102\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 5.47666597366333 | KNN Loss: 4.434770584106445 | BCE Loss: 1.0418953895568848\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 5.475202560424805 | KNN Loss: 4.42623233795166 | BCE Loss: 1.0489702224731445\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 5.442305564880371 | KNN Loss: 4.4211745262146 | BCE Loss: 1.0211312770843506\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 5.480981826782227 | KNN Loss: 4.408005237579346 | BCE Loss: 1.07297682762146\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 5.45196533203125 | KNN Loss: 4.422727584838867 | BCE Loss: 1.0292375087738037\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 5.470464706420898 | KNN Loss: 4.436991214752197 | BCE Loss: 1.0334736108779907\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 5.434208869934082 | KNN Loss: 4.408353805541992 | BCE Loss: 1.025855302810669\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 5.459940433502197 | KNN Loss: 4.432011127471924 | BCE Loss: 1.027929425239563\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 5.458143711090088 | KNN Loss: 4.425236225128174 | BCE Loss: 1.032907485961914\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 5.472146511077881 | KNN Loss: 4.4097161293029785 | BCE Loss: 1.062430500984192\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 5.416940689086914 | KNN Loss: 4.390530109405518 | BCE Loss: 1.0264105796813965\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 5.46615743637085 | KNN Loss: 4.430815696716309 | BCE Loss: 1.035341739654541\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 5.500235557556152 | KNN Loss: 4.441650867462158 | BCE Loss: 1.0585848093032837\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 5.475770950317383 | KNN Loss: 4.400707721710205 | BCE Loss: 1.0750629901885986\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 5.503639221191406 | KNN Loss: 4.423759460449219 | BCE Loss: 1.079879641532898\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 5.419504165649414 | KNN Loss: 4.3867082595825195 | BCE Loss: 1.0327961444854736\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 5.4758830070495605 | KNN Loss: 4.428852558135986 | BCE Loss: 1.0470304489135742\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 5.421320915222168 | KNN Loss: 4.379582405090332 | BCE Loss: 1.0417382717132568\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 5.455663681030273 | KNN Loss: 4.427635192871094 | BCE Loss: 1.0280282497406006\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 5.465455055236816 | KNN Loss: 4.425583839416504 | BCE Loss: 1.0398714542388916\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 5.458889484405518 | KNN Loss: 4.4246978759765625 | BCE Loss: 1.034191608428955\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 5.470462799072266 | KNN Loss: 4.42317008972168 | BCE Loss: 1.047292709350586\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 5.503080368041992 | KNN Loss: 4.432394981384277 | BCE Loss: 1.0706851482391357\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 5.4698591232299805 | KNN Loss: 4.406932353973389 | BCE Loss: 1.0629267692565918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 5.494498252868652 | KNN Loss: 4.442195892333984 | BCE Loss: 1.0523024797439575\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 5.465056896209717 | KNN Loss: 4.438093185424805 | BCE Loss: 1.0269638299942017\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 5.502208709716797 | KNN Loss: 4.44742488861084 | BCE Loss: 1.054783582687378\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 5.425683498382568 | KNN Loss: 4.4065117835998535 | BCE Loss: 1.0191717147827148\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 5.431161880493164 | KNN Loss: 4.394833564758301 | BCE Loss: 1.0363280773162842\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 5.4571638107299805 | KNN Loss: 4.421267509460449 | BCE Loss: 1.0358960628509521\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 5.449094295501709 | KNN Loss: 4.409332752227783 | BCE Loss: 1.0397614240646362\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 5.469348907470703 | KNN Loss: 4.410404205322266 | BCE Loss: 1.0589449405670166\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 5.436454772949219 | KNN Loss: 4.382169723510742 | BCE Loss: 1.054284930229187\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 5.397584438323975 | KNN Loss: 4.379769325256348 | BCE Loss: 1.0178152322769165\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 5.453821182250977 | KNN Loss: 4.4144978523254395 | BCE Loss: 1.039323091506958\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 5.414700508117676 | KNN Loss: 4.389514923095703 | BCE Loss: 1.0251855850219727\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 5.509377956390381 | KNN Loss: 4.435262203216553 | BCE Loss: 1.0741158723831177\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 5.412601470947266 | KNN Loss: 4.383430480957031 | BCE Loss: 1.0291708707809448\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 5.468573570251465 | KNN Loss: 4.398468017578125 | BCE Loss: 1.070105791091919\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 5.401574611663818 | KNN Loss: 4.3718180656433105 | BCE Loss: 1.0297565460205078\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 5.412530422210693 | KNN Loss: 4.37579870223999 | BCE Loss: 1.0367317199707031\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 5.416010856628418 | KNN Loss: 4.384945869445801 | BCE Loss: 1.031064748764038\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 5.430914878845215 | KNN Loss: 4.377959728240967 | BCE Loss: 1.052954912185669\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 5.400903224945068 | KNN Loss: 4.371387958526611 | BCE Loss: 1.0295153856277466\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 5.396029949188232 | KNN Loss: 4.393412113189697 | BCE Loss: 1.0026178359985352\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 5.401777267456055 | KNN Loss: 4.382938861846924 | BCE Loss: 1.0188384056091309\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 5.3900251388549805 | KNN Loss: 4.355106830596924 | BCE Loss: 1.0349180698394775\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 5.422190189361572 | KNN Loss: 4.378591060638428 | BCE Loss: 1.043599009513855\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 5.426884651184082 | KNN Loss: 4.388014793395996 | BCE Loss: 1.0388696193695068\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 5.397391319274902 | KNN Loss: 4.363552570343018 | BCE Loss: 1.0338389873504639\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 5.364593982696533 | KNN Loss: 4.373336315155029 | BCE Loss: 0.9912577867507935\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 5.446453094482422 | KNN Loss: 4.36730432510376 | BCE Loss: 1.079148530960083\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 5.419763088226318 | KNN Loss: 4.378377914428711 | BCE Loss: 1.0413850545883179\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 5.420393466949463 | KNN Loss: 4.366048812866211 | BCE Loss: 1.054344654083252\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 5.408888816833496 | KNN Loss: 4.378366947174072 | BCE Loss: 1.0305218696594238\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 5.403408050537109 | KNN Loss: 4.36372709274292 | BCE Loss: 1.0396811962127686\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 5.464143753051758 | KNN Loss: 4.386983871459961 | BCE Loss: 1.077160120010376\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 5.442549705505371 | KNN Loss: 4.375802516937256 | BCE Loss: 1.0667471885681152\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 5.377115249633789 | KNN Loss: 4.357266426086426 | BCE Loss: 1.0198487043380737\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 5.415144920349121 | KNN Loss: 4.3827338218688965 | BCE Loss: 1.0324113368988037\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 5.425379753112793 | KNN Loss: 4.373029708862305 | BCE Loss: 1.0523500442504883\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 5.442313194274902 | KNN Loss: 4.405066967010498 | BCE Loss: 1.0372459888458252\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 5.407066822052002 | KNN Loss: 4.371215343475342 | BCE Loss: 1.0358515977859497\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 5.406368255615234 | KNN Loss: 4.370823860168457 | BCE Loss: 1.0355446338653564\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 5.409469127655029 | KNN Loss: 4.368084907531738 | BCE Loss: 1.0413841009140015\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 5.4194817543029785 | KNN Loss: 4.391356945037842 | BCE Loss: 1.0281248092651367\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 5.407010078430176 | KNN Loss: 4.381390571594238 | BCE Loss: 1.0256192684173584\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 5.394787788391113 | KNN Loss: 4.3729023933410645 | BCE Loss: 1.0218852758407593\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 5.3939313888549805 | KNN Loss: 4.376171588897705 | BCE Loss: 1.0177597999572754\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 5.393674850463867 | KNN Loss: 4.3779706954956055 | BCE Loss: 1.0157039165496826\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 5.416482925415039 | KNN Loss: 4.377093315124512 | BCE Loss: 1.0393896102905273\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 5.415792465209961 | KNN Loss: 4.374057292938232 | BCE Loss: 1.0417351722717285\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 5.418410301208496 | KNN Loss: 4.379255294799805 | BCE Loss: 1.0391552448272705\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 5.420589923858643 | KNN Loss: 4.375635147094727 | BCE Loss: 1.0449546575546265\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 5.461987495422363 | KNN Loss: 4.391393184661865 | BCE Loss: 1.070594072341919\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 5.394202709197998 | KNN Loss: 4.361379146575928 | BCE Loss: 1.0328235626220703\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 5.435664653778076 | KNN Loss: 4.392107009887695 | BCE Loss: 1.0435576438903809\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 5.43241024017334 | KNN Loss: 4.403459072113037 | BCE Loss: 1.0289509296417236\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 5.445628643035889 | KNN Loss: 4.396621227264404 | BCE Loss: 1.0490074157714844\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 5.386765956878662 | KNN Loss: 4.365262985229492 | BCE Loss: 1.02150297164917\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 5.41737174987793 | KNN Loss: 4.3809099197387695 | BCE Loss: 1.0364620685577393\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 5.382143497467041 | KNN Loss: 4.364224433898926 | BCE Loss: 1.0179190635681152\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 5.377613067626953 | KNN Loss: 4.355838298797607 | BCE Loss: 1.0217745304107666\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 5.421835899353027 | KNN Loss: 4.375133037567139 | BCE Loss: 1.0467031002044678\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 5.407832622528076 | KNN Loss: 4.371726036071777 | BCE Loss: 1.0361067056655884\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 5.41095495223999 | KNN Loss: 4.363582134246826 | BCE Loss: 1.0473729372024536\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 5.449486255645752 | KNN Loss: 4.380287170410156 | BCE Loss: 1.0691992044448853\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 5.417025566101074 | KNN Loss: 4.381148338317871 | BCE Loss: 1.035876989364624\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 5.419245719909668 | KNN Loss: 4.355437278747559 | BCE Loss: 1.0638082027435303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 5.455676555633545 | KNN Loss: 4.403257369995117 | BCE Loss: 1.0524191856384277\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 5.373770713806152 | KNN Loss: 4.362937927246094 | BCE Loss: 1.0108327865600586\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 5.454716205596924 | KNN Loss: 4.371763229370117 | BCE Loss: 1.0829529762268066\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 5.394976615905762 | KNN Loss: 4.365742206573486 | BCE Loss: 1.0292344093322754\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 5.4025068283081055 | KNN Loss: 4.363577842712402 | BCE Loss: 1.038928747177124\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 5.432860374450684 | KNN Loss: 4.372990608215332 | BCE Loss: 1.059869647026062\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 5.394655704498291 | KNN Loss: 4.3650360107421875 | BCE Loss: 1.0296196937561035\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 5.402189254760742 | KNN Loss: 4.371366024017334 | BCE Loss: 1.0308234691619873\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 5.396960258483887 | KNN Loss: 4.369208812713623 | BCE Loss: 1.0277516841888428\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 5.363828659057617 | KNN Loss: 4.357232570648193 | BCE Loss: 1.0065960884094238\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 5.382158279418945 | KNN Loss: 4.364503860473633 | BCE Loss: 1.017654538154602\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 5.3929443359375 | KNN Loss: 4.3516035079956055 | BCE Loss: 1.0413410663604736\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 5.410003662109375 | KNN Loss: 4.3680419921875 | BCE Loss: 1.041961908340454\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 5.402987480163574 | KNN Loss: 4.398555755615234 | BCE Loss: 1.0044314861297607\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 5.409904956817627 | KNN Loss: 4.38731575012207 | BCE Loss: 1.0225893259048462\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 5.415112495422363 | KNN Loss: 4.372244834899902 | BCE Loss: 1.04286789894104\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 5.41451358795166 | KNN Loss: 4.345363616943359 | BCE Loss: 1.0691498517990112\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 5.442985534667969 | KNN Loss: 4.408997058868408 | BCE Loss: 1.0339882373809814\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 5.403781890869141 | KNN Loss: 4.3641357421875 | BCE Loss: 1.0396461486816406\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 5.409324645996094 | KNN Loss: 4.3790202140808105 | BCE Loss: 1.0303044319152832\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 5.3900957107543945 | KNN Loss: 4.359774589538574 | BCE Loss: 1.0303213596343994\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 5.380115032196045 | KNN Loss: 4.357685089111328 | BCE Loss: 1.0224299430847168\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 5.407707691192627 | KNN Loss: 4.398228168487549 | BCE Loss: 1.0094795227050781\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 5.420509338378906 | KNN Loss: 4.374175548553467 | BCE Loss: 1.04633367061615\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 5.395270347595215 | KNN Loss: 4.361728668212891 | BCE Loss: 1.0335419178009033\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 5.378434658050537 | KNN Loss: 4.368053913116455 | BCE Loss: 1.0103808641433716\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 5.422451019287109 | KNN Loss: 4.354194164276123 | BCE Loss: 1.0682568550109863\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 5.389863967895508 | KNN Loss: 4.365192890167236 | BCE Loss: 1.0246708393096924\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 5.403820037841797 | KNN Loss: 4.386721134185791 | BCE Loss: 1.0170986652374268\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 5.329811096191406 | KNN Loss: 4.348321437835693 | BCE Loss: 0.9814894199371338\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 5.425103187561035 | KNN Loss: 4.3941521644592285 | BCE Loss: 1.0309507846832275\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 5.403076171875 | KNN Loss: 4.3685221672058105 | BCE Loss: 1.0345542430877686\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 5.4131340980529785 | KNN Loss: 4.368789196014404 | BCE Loss: 1.0443449020385742\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 5.439399242401123 | KNN Loss: 4.36137580871582 | BCE Loss: 1.0780235528945923\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 5.440401077270508 | KNN Loss: 4.423233985900879 | BCE Loss: 1.0171669721603394\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 5.408477783203125 | KNN Loss: 4.349627494812012 | BCE Loss: 1.0588500499725342\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 5.41148567199707 | KNN Loss: 4.370985507965088 | BCE Loss: 1.0405001640319824\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 5.395298957824707 | KNN Loss: 4.350515365600586 | BCE Loss: 1.044783592224121\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 5.4054741859436035 | KNN Loss: 4.376567840576172 | BCE Loss: 1.0289064645767212\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 5.422967433929443 | KNN Loss: 4.399082183837891 | BCE Loss: 1.0238852500915527\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 5.377076625823975 | KNN Loss: 4.3532915115356445 | BCE Loss: 1.0237852334976196\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 5.38695764541626 | KNN Loss: 4.368627071380615 | BCE Loss: 1.018330693244934\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 5.386882781982422 | KNN Loss: 4.364599227905273 | BCE Loss: 1.0222833156585693\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 5.379798889160156 | KNN Loss: 4.368206977844238 | BCE Loss: 1.011592149734497\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 5.361792087554932 | KNN Loss: 4.352304458618164 | BCE Loss: 1.0094876289367676\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 5.438958168029785 | KNN Loss: 4.382874965667725 | BCE Loss: 1.0560834407806396\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 5.385519027709961 | KNN Loss: 4.3450164794921875 | BCE Loss: 1.0405027866363525\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 5.403470516204834 | KNN Loss: 4.3522138595581055 | BCE Loss: 1.0512566566467285\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 5.390393257141113 | KNN Loss: 4.354948043823242 | BCE Loss: 1.0354454517364502\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 5.3991289138793945 | KNN Loss: 4.3641791343688965 | BCE Loss: 1.0349500179290771\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 5.422401428222656 | KNN Loss: 4.36053466796875 | BCE Loss: 1.0618669986724854\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 5.386929035186768 | KNN Loss: 4.353886604309082 | BCE Loss: 1.033042550086975\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 5.4106245040893555 | KNN Loss: 4.397340297698975 | BCE Loss: 1.0132839679718018\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 5.347555637359619 | KNN Loss: 4.350978851318359 | BCE Loss: 0.9965767860412598\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 5.40264892578125 | KNN Loss: 4.370856761932373 | BCE Loss: 1.0317919254302979\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 5.388937950134277 | KNN Loss: 4.358841896057129 | BCE Loss: 1.0300962924957275\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 5.3957953453063965 | KNN Loss: 4.36355447769165 | BCE Loss: 1.032240867614746\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 5.407407760620117 | KNN Loss: 4.369569301605225 | BCE Loss: 1.0378384590148926\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 5.401939392089844 | KNN Loss: 4.350493431091309 | BCE Loss: 1.0514460802078247\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 5.378706932067871 | KNN Loss: 4.34922981262207 | BCE Loss: 1.0294771194458008\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 5.434507369995117 | KNN Loss: 4.387628555297852 | BCE Loss: 1.0468790531158447\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 5.425950050354004 | KNN Loss: 4.372570514678955 | BCE Loss: 1.0533796548843384\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 5.431204319000244 | KNN Loss: 4.403583526611328 | BCE Loss: 1.027620792388916\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 5.379387378692627 | KNN Loss: 4.358920097351074 | BCE Loss: 1.0204672813415527\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 5.417169570922852 | KNN Loss: 4.371950626373291 | BCE Loss: 1.045218825340271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    65: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 5.438379287719727 | KNN Loss: 4.365175247192383 | BCE Loss: 1.0732038021087646\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 5.397183418273926 | KNN Loss: 4.358837604522705 | BCE Loss: 1.0383455753326416\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 5.391772270202637 | KNN Loss: 4.379667282104492 | BCE Loss: 1.0121049880981445\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 5.393117904663086 | KNN Loss: 4.383541584014893 | BCE Loss: 1.0095760822296143\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 5.403880596160889 | KNN Loss: 4.349991798400879 | BCE Loss: 1.0538889169692993\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 5.401690483093262 | KNN Loss: 4.366482734680176 | BCE Loss: 1.0352078676223755\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 5.359391689300537 | KNN Loss: 4.335411548614502 | BCE Loss: 1.0239801406860352\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 5.352450370788574 | KNN Loss: 4.362008094787598 | BCE Loss: 0.9904420375823975\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 5.378098011016846 | KNN Loss: 4.3430023193359375 | BCE Loss: 1.0350955724716187\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 5.418315887451172 | KNN Loss: 4.353075981140137 | BCE Loss: 1.065239667892456\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 5.41557502746582 | KNN Loss: 4.359654903411865 | BCE Loss: 1.0559203624725342\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 5.373296737670898 | KNN Loss: 4.34642219543457 | BCE Loss: 1.0268746614456177\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 5.365743637084961 | KNN Loss: 4.352408409118652 | BCE Loss: 1.0133349895477295\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 5.424015522003174 | KNN Loss: 4.3772430419921875 | BCE Loss: 1.0467724800109863\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 5.387605667114258 | KNN Loss: 4.351329326629639 | BCE Loss: 1.03627610206604\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 5.444284915924072 | KNN Loss: 4.431537628173828 | BCE Loss: 1.0127474069595337\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 5.394299507141113 | KNN Loss: 4.353176116943359 | BCE Loss: 1.0411231517791748\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 5.424873352050781 | KNN Loss: 4.382445335388184 | BCE Loss: 1.0424280166625977\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 5.382128715515137 | KNN Loss: 4.357104778289795 | BCE Loss: 1.0250236988067627\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 5.373140335083008 | KNN Loss: 4.355472564697266 | BCE Loss: 1.0176677703857422\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 5.394399642944336 | KNN Loss: 4.34935188293457 | BCE Loss: 1.0450478792190552\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 5.378749847412109 | KNN Loss: 4.366300582885742 | BCE Loss: 1.0124495029449463\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 5.416452407836914 | KNN Loss: 4.372134208679199 | BCE Loss: 1.0443179607391357\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 5.374632835388184 | KNN Loss: 4.356841564178467 | BCE Loss: 1.0177913904190063\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 5.347161293029785 | KNN Loss: 4.3361005783081055 | BCE Loss: 1.0110607147216797\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 5.392019271850586 | KNN Loss: 4.353644847869873 | BCE Loss: 1.0383745431900024\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 5.383150577545166 | KNN Loss: 4.352010726928711 | BCE Loss: 1.0311397314071655\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 5.406680107116699 | KNN Loss: 4.377092361450195 | BCE Loss: 1.029587745666504\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 5.394568920135498 | KNN Loss: 4.355550765991211 | BCE Loss: 1.039018154144287\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 5.398289203643799 | KNN Loss: 4.372993469238281 | BCE Loss: 1.0252957344055176\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 5.402749061584473 | KNN Loss: 4.357285976409912 | BCE Loss: 1.0454630851745605\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 5.388854503631592 | KNN Loss: 4.356159210205078 | BCE Loss: 1.0326951742172241\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 5.402977466583252 | KNN Loss: 4.370565414428711 | BCE Loss: 1.032412052154541\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 5.388740062713623 | KNN Loss: 4.3688578605651855 | BCE Loss: 1.0198822021484375\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 5.3810715675354 | KNN Loss: 4.357137680053711 | BCE Loss: 1.0239338874816895\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 5.401026248931885 | KNN Loss: 4.346263885498047 | BCE Loss: 1.054762363433838\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 5.380454063415527 | KNN Loss: 4.378140449523926 | BCE Loss: 1.0023138523101807\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 5.397060394287109 | KNN Loss: 4.358453750610352 | BCE Loss: 1.038606882095337\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 5.412367343902588 | KNN Loss: 4.360965728759766 | BCE Loss: 1.0514017343521118\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 5.397424697875977 | KNN Loss: 4.357254981994629 | BCE Loss: 1.0401694774627686\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 5.373668670654297 | KNN Loss: 4.337041854858398 | BCE Loss: 1.0366270542144775\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 5.399534225463867 | KNN Loss: 4.361757278442383 | BCE Loss: 1.037777066230774\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 5.416736602783203 | KNN Loss: 4.366302013397217 | BCE Loss: 1.0504347085952759\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 5.418971061706543 | KNN Loss: 4.375760555267334 | BCE Loss: 1.043210506439209\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 5.403187274932861 | KNN Loss: 4.356834888458252 | BCE Loss: 1.046352505683899\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 5.446564674377441 | KNN Loss: 4.392973899841309 | BCE Loss: 1.0535907745361328\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 5.371517181396484 | KNN Loss: 4.357230186462402 | BCE Loss: 1.014286994934082\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 5.393258094787598 | KNN Loss: 4.358198165893555 | BCE Loss: 1.0350596904754639\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 5.421468734741211 | KNN Loss: 4.359923839569092 | BCE Loss: 1.0615451335906982\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 5.363711357116699 | KNN Loss: 4.345741271972656 | BCE Loss: 1.017970085144043\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 5.391985893249512 | KNN Loss: 4.370133399963379 | BCE Loss: 1.0218524932861328\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 5.383999347686768 | KNN Loss: 4.357040882110596 | BCE Loss: 1.0269585847854614\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 5.3914899826049805 | KNN Loss: 4.369420528411865 | BCE Loss: 1.0220696926116943\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 5.379165172576904 | KNN Loss: 4.348410129547119 | BCE Loss: 1.0307550430297852\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 5.339487075805664 | KNN Loss: 4.353546619415283 | BCE Loss: 0.9859404563903809\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 5.339794158935547 | KNN Loss: 4.355520725250244 | BCE Loss: 0.9842734336853027\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 5.367951393127441 | KNN Loss: 4.363083839416504 | BCE Loss: 1.0048677921295166\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 5.399224758148193 | KNN Loss: 4.3516387939453125 | BCE Loss: 1.0475859642028809\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 5.403804779052734 | KNN Loss: 4.355781078338623 | BCE Loss: 1.0480234622955322\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 5.4206223487854 | KNN Loss: 4.3791584968566895 | BCE Loss: 1.0414639711380005\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 5.406067371368408 | KNN Loss: 4.364358901977539 | BCE Loss: 1.0417084693908691\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 5.378495216369629 | KNN Loss: 4.344150066375732 | BCE Loss: 1.0343449115753174\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 5.378167629241943 | KNN Loss: 4.3632707595825195 | BCE Loss: 1.0148967504501343\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 5.394044876098633 | KNN Loss: 4.364645481109619 | BCE Loss: 1.0293996334075928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 5.373533248901367 | KNN Loss: 4.358471393585205 | BCE Loss: 1.0150617361068726\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 5.399715423583984 | KNN Loss: 4.359380722045898 | BCE Loss: 1.0403344631195068\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 5.357043266296387 | KNN Loss: 4.344985008239746 | BCE Loss: 1.0120584964752197\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 5.422059059143066 | KNN Loss: 4.387399196624756 | BCE Loss: 1.0346601009368896\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 5.3639702796936035 | KNN Loss: 4.351160526275635 | BCE Loss: 1.0128097534179688\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 5.408161163330078 | KNN Loss: 4.357083797454834 | BCE Loss: 1.0510776042938232\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 5.380096912384033 | KNN Loss: 4.3423051834106445 | BCE Loss: 1.0377917289733887\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 5.393805027008057 | KNN Loss: 4.356424808502197 | BCE Loss: 1.0373802185058594\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 5.363879203796387 | KNN Loss: 4.355027675628662 | BCE Loss: 1.0088515281677246\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 5.3940324783325195 | KNN Loss: 4.3841962814331055 | BCE Loss: 1.0098360776901245\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 5.386828422546387 | KNN Loss: 4.36094331741333 | BCE Loss: 1.0258853435516357\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 5.401072025299072 | KNN Loss: 4.381537914276123 | BCE Loss: 1.0195342302322388\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 5.401949882507324 | KNN Loss: 4.357446193695068 | BCE Loss: 1.0445034503936768\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 5.38883638381958 | KNN Loss: 4.379168510437012 | BCE Loss: 1.0096678733825684\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 5.4246063232421875 | KNN Loss: 4.362494945526123 | BCE Loss: 1.0621116161346436\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 5.395153045654297 | KNN Loss: 4.365919589996338 | BCE Loss: 1.0292333364486694\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 5.377713680267334 | KNN Loss: 4.37113618850708 | BCE Loss: 1.006577491760254\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 5.39968204498291 | KNN Loss: 4.353379726409912 | BCE Loss: 1.0463025569915771\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 5.396368980407715 | KNN Loss: 4.376692771911621 | BCE Loss: 1.0196764469146729\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 5.380467414855957 | KNN Loss: 4.337383270263672 | BCE Loss: 1.043083906173706\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 5.399941444396973 | KNN Loss: 4.367739677429199 | BCE Loss: 1.0322020053863525\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 5.396510124206543 | KNN Loss: 4.365067958831787 | BCE Loss: 1.0314419269561768\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 5.377959251403809 | KNN Loss: 4.357226371765137 | BCE Loss: 1.020733118057251\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 5.410756587982178 | KNN Loss: 4.359894275665283 | BCE Loss: 1.0508623123168945\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 5.356759071350098 | KNN Loss: 4.338117599487305 | BCE Loss: 1.0186412334442139\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 5.391345977783203 | KNN Loss: 4.346330642700195 | BCE Loss: 1.0450152158737183\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 5.428034782409668 | KNN Loss: 4.397045135498047 | BCE Loss: 1.0309898853302002\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 5.369505405426025 | KNN Loss: 4.336505889892578 | BCE Loss: 1.0329995155334473\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 5.421607971191406 | KNN Loss: 4.3681745529174805 | BCE Loss: 1.0534336566925049\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 5.39246940612793 | KNN Loss: 4.377220153808594 | BCE Loss: 1.0152493715286255\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 5.369312763214111 | KNN Loss: 4.345047473907471 | BCE Loss: 1.0242654085159302\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 5.402049541473389 | KNN Loss: 4.367492198944092 | BCE Loss: 1.0345574617385864\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 5.395852088928223 | KNN Loss: 4.349297046661377 | BCE Loss: 1.0465552806854248\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 5.394265174865723 | KNN Loss: 4.357223033905029 | BCE Loss: 1.0370421409606934\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 5.372118949890137 | KNN Loss: 4.353036403656006 | BCE Loss: 1.0190823078155518\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 5.381441116333008 | KNN Loss: 4.361057281494141 | BCE Loss: 1.0203838348388672\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 5.334877014160156 | KNN Loss: 4.329123020172119 | BCE Loss: 1.005753993988037\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 5.360557556152344 | KNN Loss: 4.361760139465332 | BCE Loss: 0.9987974762916565\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 5.357212066650391 | KNN Loss: 4.333653450012207 | BCE Loss: 1.0235587358474731\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 5.404619216918945 | KNN Loss: 4.371490478515625 | BCE Loss: 1.0331289768218994\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 5.340245723724365 | KNN Loss: 4.342613220214844 | BCE Loss: 0.9976323843002319\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 5.381374835968018 | KNN Loss: 4.3486104011535645 | BCE Loss: 1.0327644348144531\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 5.396881103515625 | KNN Loss: 4.353321075439453 | BCE Loss: 1.0435601472854614\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 5.415365695953369 | KNN Loss: 4.361255645751953 | BCE Loss: 1.0541099309921265\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 5.41276741027832 | KNN Loss: 4.38414192199707 | BCE Loss: 1.028625249862671\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 5.40376091003418 | KNN Loss: 4.371912956237793 | BCE Loss: 1.0318479537963867\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 5.390863418579102 | KNN Loss: 4.341230869293213 | BCE Loss: 1.0496323108673096\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 5.401020050048828 | KNN Loss: 4.384049415588379 | BCE Loss: 1.0169703960418701\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 5.410150527954102 | KNN Loss: 4.34241247177124 | BCE Loss: 1.0677378177642822\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 5.3677825927734375 | KNN Loss: 4.34580135345459 | BCE Loss: 1.0219813585281372\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 5.396491050720215 | KNN Loss: 4.3703389167785645 | BCE Loss: 1.0261518955230713\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 5.402194976806641 | KNN Loss: 4.360762119293213 | BCE Loss: 1.0414326190948486\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 5.371062278747559 | KNN Loss: 4.355416297912598 | BCE Loss: 1.0156457424163818\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 5.430151462554932 | KNN Loss: 4.34550666809082 | BCE Loss: 1.0846447944641113\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 5.391988754272461 | KNN Loss: 4.359023571014404 | BCE Loss: 1.0329653024673462\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 5.40794563293457 | KNN Loss: 4.359093189239502 | BCE Loss: 1.0488524436950684\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 5.350287914276123 | KNN Loss: 4.340568542480469 | BCE Loss: 1.0097194910049438\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 5.392870903015137 | KNN Loss: 4.345829486846924 | BCE Loss: 1.0470415353775024\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 5.399294853210449 | KNN Loss: 4.338985443115234 | BCE Loss: 1.0603091716766357\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 5.37537956237793 | KNN Loss: 4.354772090911865 | BCE Loss: 1.020607352256775\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 5.3894171714782715 | KNN Loss: 4.356543064117432 | BCE Loss: 1.0328741073608398\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 5.397373199462891 | KNN Loss: 4.369489669799805 | BCE Loss: 1.0278832912445068\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 5.4124345779418945 | KNN Loss: 4.366618633270264 | BCE Loss: 1.0458157062530518\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 5.369438171386719 | KNN Loss: 4.354984283447266 | BCE Loss: 1.0144541263580322\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 5.361554145812988 | KNN Loss: 4.342392921447754 | BCE Loss: 1.0191611051559448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 5.426111221313477 | KNN Loss: 4.402726650238037 | BCE Loss: 1.0233845710754395\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 5.384760856628418 | KNN Loss: 4.345700740814209 | BCE Loss: 1.039060354232788\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 5.3843889236450195 | KNN Loss: 4.349915504455566 | BCE Loss: 1.0344735383987427\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 5.376343250274658 | KNN Loss: 4.386961936950684 | BCE Loss: 0.9893814325332642\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 5.396801471710205 | KNN Loss: 4.354222297668457 | BCE Loss: 1.0425790548324585\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 5.42839241027832 | KNN Loss: 4.372525215148926 | BCE Loss: 1.0558671951293945\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 5.386249542236328 | KNN Loss: 4.344707489013672 | BCE Loss: 1.0415420532226562\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 5.3686418533325195 | KNN Loss: 4.362735271453857 | BCE Loss: 1.0059067010879517\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 5.365766525268555 | KNN Loss: 4.338094711303711 | BCE Loss: 1.0276719331741333\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 5.432338714599609 | KNN Loss: 4.383631706237793 | BCE Loss: 1.0487070083618164\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 5.3782196044921875 | KNN Loss: 4.351725101470947 | BCE Loss: 1.0264943838119507\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 5.410859107971191 | KNN Loss: 4.361563682556152 | BCE Loss: 1.04929518699646\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 5.390492916107178 | KNN Loss: 4.362542629241943 | BCE Loss: 1.0279502868652344\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 5.346284866333008 | KNN Loss: 4.352782726287842 | BCE Loss: 0.9935023784637451\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 5.407803535461426 | KNN Loss: 4.385221481323242 | BCE Loss: 1.0225821733474731\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 5.394388675689697 | KNN Loss: 4.3470563888549805 | BCE Loss: 1.0473322868347168\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 5.405928611755371 | KNN Loss: 4.372947692871094 | BCE Loss: 1.0329811573028564\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 5.393532752990723 | KNN Loss: 4.3465704917907715 | BCE Loss: 1.0469622611999512\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 5.370335578918457 | KNN Loss: 4.376105308532715 | BCE Loss: 0.9942304491996765\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 5.439748764038086 | KNN Loss: 4.383504867553711 | BCE Loss: 1.056243658065796\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 5.373715877532959 | KNN Loss: 4.341917514801025 | BCE Loss: 1.0317983627319336\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 5.380196571350098 | KNN Loss: 4.354909420013428 | BCE Loss: 1.025287389755249\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 5.452725410461426 | KNN Loss: 4.408173561096191 | BCE Loss: 1.0445518493652344\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 5.413141250610352 | KNN Loss: 4.381661891937256 | BCE Loss: 1.0314793586730957\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 5.378489971160889 | KNN Loss: 4.3479156494140625 | BCE Loss: 1.0305742025375366\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 5.4085516929626465 | KNN Loss: 4.359029293060303 | BCE Loss: 1.0495223999023438\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 5.380360126495361 | KNN Loss: 4.353233814239502 | BCE Loss: 1.0271263122558594\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 5.396458148956299 | KNN Loss: 4.341723442077637 | BCE Loss: 1.0547348260879517\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 5.402606010437012 | KNN Loss: 4.3778157234191895 | BCE Loss: 1.0247902870178223\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 5.371843338012695 | KNN Loss: 4.3465142250061035 | BCE Loss: 1.0253292322158813\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 5.440499305725098 | KNN Loss: 4.392813205718994 | BCE Loss: 1.047685980796814\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 5.428760528564453 | KNN Loss: 4.396052360534668 | BCE Loss: 1.0327080488204956\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 5.438915252685547 | KNN Loss: 4.385110378265381 | BCE Loss: 1.053804874420166\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 5.407865524291992 | KNN Loss: 4.374380111694336 | BCE Loss: 1.0334854125976562\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 5.409335136413574 | KNN Loss: 4.387894153594971 | BCE Loss: 1.0214412212371826\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 5.368135929107666 | KNN Loss: 4.359800338745117 | BCE Loss: 1.0083354711532593\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 5.397513389587402 | KNN Loss: 4.343021392822266 | BCE Loss: 1.0544917583465576\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 5.371297836303711 | KNN Loss: 4.345251083374023 | BCE Loss: 1.0260467529296875\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 5.388079643249512 | KNN Loss: 4.332510948181152 | BCE Loss: 1.0555689334869385\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 5.37601375579834 | KNN Loss: 4.3552165031433105 | BCE Loss: 1.0207974910736084\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 5.4212141036987305 | KNN Loss: 4.366473197937012 | BCE Loss: 1.0547406673431396\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 5.380436897277832 | KNN Loss: 4.360416412353516 | BCE Loss: 1.0200207233428955\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 5.366046905517578 | KNN Loss: 4.347878932952881 | BCE Loss: 1.0181677341461182\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 5.394405364990234 | KNN Loss: 4.336858749389648 | BCE Loss: 1.057546615600586\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 5.409005641937256 | KNN Loss: 4.387233734130859 | BCE Loss: 1.021771788597107\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 5.394225120544434 | KNN Loss: 4.345319747924805 | BCE Loss: 1.0489051342010498\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 5.417338848114014 | KNN Loss: 4.362790584564209 | BCE Loss: 1.0545482635498047\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 5.371108055114746 | KNN Loss: 4.352243900299072 | BCE Loss: 1.0188639163970947\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 5.40689754486084 | KNN Loss: 4.383878231048584 | BCE Loss: 1.0230190753936768\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 5.367292881011963 | KNN Loss: 4.353646278381348 | BCE Loss: 1.0136464834213257\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 5.387598514556885 | KNN Loss: 4.366622447967529 | BCE Loss: 1.0209760665893555\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 5.392814636230469 | KNN Loss: 4.357483386993408 | BCE Loss: 1.0353310108184814\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 5.403390884399414 | KNN Loss: 4.382637977600098 | BCE Loss: 1.0207526683807373\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 5.369180202484131 | KNN Loss: 4.354797840118408 | BCE Loss: 1.0143824815750122\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 5.376765251159668 | KNN Loss: 4.3270087242126465 | BCE Loss: 1.0497562885284424\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 5.3963165283203125 | KNN Loss: 4.344987392425537 | BCE Loss: 1.0513288974761963\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 5.3751983642578125 | KNN Loss: 4.366954803466797 | BCE Loss: 1.0082436800003052\n",
      "Epoch    96: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 5.403237342834473 | KNN Loss: 4.367984294891357 | BCE Loss: 1.0352528095245361\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 5.450368881225586 | KNN Loss: 4.404449462890625 | BCE Loss: 1.0459191799163818\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 5.385214328765869 | KNN Loss: 4.3425774574279785 | BCE Loss: 1.0426369905471802\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 5.39527702331543 | KNN Loss: 4.355706691741943 | BCE Loss: 1.0395700931549072\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 5.370758056640625 | KNN Loss: 4.346776485443115 | BCE Loss: 1.0239818096160889\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 5.4156928062438965 | KNN Loss: 4.3572187423706055 | BCE Loss: 1.0584741830825806\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 5.400701522827148 | KNN Loss: 4.391059398651123 | BCE Loss: 1.0096421241760254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 5.43046236038208 | KNN Loss: 4.382347106933594 | BCE Loss: 1.0481152534484863\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 5.405642509460449 | KNN Loss: 4.362426280975342 | BCE Loss: 1.0432161092758179\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 5.369318008422852 | KNN Loss: 4.351657390594482 | BCE Loss: 1.01766037940979\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 5.374583721160889 | KNN Loss: 4.366170883178711 | BCE Loss: 1.0084127187728882\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 5.343667507171631 | KNN Loss: 4.340145111083984 | BCE Loss: 1.003522276878357\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 5.357397556304932 | KNN Loss: 4.344089984893799 | BCE Loss: 1.0133075714111328\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 5.34920597076416 | KNN Loss: 4.357241153717041 | BCE Loss: 0.99196457862854\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 5.355490684509277 | KNN Loss: 4.330639839172363 | BCE Loss: 1.024850845336914\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 5.3501176834106445 | KNN Loss: 4.350877285003662 | BCE Loss: 0.9992404580116272\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 5.383106231689453 | KNN Loss: 4.347408771514893 | BCE Loss: 1.0356976985931396\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 5.4162092208862305 | KNN Loss: 4.379260540008545 | BCE Loss: 1.0369489192962646\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 5.402254104614258 | KNN Loss: 4.341574668884277 | BCE Loss: 1.0606794357299805\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 5.371313571929932 | KNN Loss: 4.34429931640625 | BCE Loss: 1.027014136314392\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 5.3751115798950195 | KNN Loss: 4.353816032409668 | BCE Loss: 1.0212957859039307\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 5.3832621574401855 | KNN Loss: 4.370858669281006 | BCE Loss: 1.0124034881591797\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 5.383062362670898 | KNN Loss: 4.374011039733887 | BCE Loss: 1.0090513229370117\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 5.408634185791016 | KNN Loss: 4.359771728515625 | BCE Loss: 1.0488624572753906\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 5.332459449768066 | KNN Loss: 4.335310459136963 | BCE Loss: 0.9971489906311035\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 5.409533500671387 | KNN Loss: 4.34193754196167 | BCE Loss: 1.0675959587097168\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 5.370046615600586 | KNN Loss: 4.345462322235107 | BCE Loss: 1.024584174156189\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 5.418038368225098 | KNN Loss: 4.367949485778809 | BCE Loss: 1.05008864402771\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 5.341561317443848 | KNN Loss: 4.346352577209473 | BCE Loss: 0.9952088594436646\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 5.4017863273620605 | KNN Loss: 4.364301681518555 | BCE Loss: 1.0374846458435059\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 5.3389434814453125 | KNN Loss: 4.326239109039307 | BCE Loss: 1.0127043724060059\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 5.37133264541626 | KNN Loss: 4.348623275756836 | BCE Loss: 1.0227093696594238\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 5.349817276000977 | KNN Loss: 4.343953609466553 | BCE Loss: 1.0058636665344238\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 5.366944789886475 | KNN Loss: 4.348485946655273 | BCE Loss: 1.0184589624404907\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 5.3988800048828125 | KNN Loss: 4.357202053070068 | BCE Loss: 1.0416779518127441\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 5.393960952758789 | KNN Loss: 4.3639960289001465 | BCE Loss: 1.0299650430679321\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 5.396813869476318 | KNN Loss: 4.396016597747803 | BCE Loss: 1.0007972717285156\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 5.365581035614014 | KNN Loss: 4.35134220123291 | BCE Loss: 1.0142388343811035\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 5.3634161949157715 | KNN Loss: 4.352558135986328 | BCE Loss: 1.0108580589294434\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 5.35639762878418 | KNN Loss: 4.337050437927246 | BCE Loss: 1.0193474292755127\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 5.351855278015137 | KNN Loss: 4.3412251472473145 | BCE Loss: 1.0106298923492432\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 5.356748104095459 | KNN Loss: 4.3360795974731445 | BCE Loss: 1.0206685066223145\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 5.378355979919434 | KNN Loss: 4.340939521789551 | BCE Loss: 1.0374162197113037\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 5.3523712158203125 | KNN Loss: 4.349681854248047 | BCE Loss: 1.0026891231536865\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 5.41748046875 | KNN Loss: 4.387922763824463 | BCE Loss: 1.029557466506958\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 5.387157440185547 | KNN Loss: 4.364402770996094 | BCE Loss: 1.0227545499801636\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 5.41288948059082 | KNN Loss: 4.353960990905762 | BCE Loss: 1.0589282512664795\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 5.381826400756836 | KNN Loss: 4.343011856079102 | BCE Loss: 1.0388147830963135\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 5.439129829406738 | KNN Loss: 4.414272308349609 | BCE Loss: 1.024857521057129\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 5.373324394226074 | KNN Loss: 4.369068145751953 | BCE Loss: 1.004256010055542\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 5.333035469055176 | KNN Loss: 4.316640853881836 | BCE Loss: 1.0163943767547607\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 5.367170333862305 | KNN Loss: 4.356332778930664 | BCE Loss: 1.0108377933502197\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 5.412030220031738 | KNN Loss: 4.359337329864502 | BCE Loss: 1.0526928901672363\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 5.38006591796875 | KNN Loss: 4.342343330383301 | BCE Loss: 1.0377223491668701\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 5.377618312835693 | KNN Loss: 4.362768650054932 | BCE Loss: 1.0148497819900513\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 5.404541969299316 | KNN Loss: 4.36952543258667 | BCE Loss: 1.0350167751312256\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 5.370061874389648 | KNN Loss: 4.34349250793457 | BCE Loss: 1.0265694856643677\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 5.3409271240234375 | KNN Loss: 4.339151382446289 | BCE Loss: 1.0017755031585693\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 5.39901876449585 | KNN Loss: 4.3699631690979 | BCE Loss: 1.0290557146072388\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 5.440010070800781 | KNN Loss: 4.383872032165527 | BCE Loss: 1.0561379194259644\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 5.376443386077881 | KNN Loss: 4.340538024902344 | BCE Loss: 1.035905361175537\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 5.385984420776367 | KNN Loss: 4.363041877746582 | BCE Loss: 1.0229424238204956\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 5.433782577514648 | KNN Loss: 4.381476879119873 | BCE Loss: 1.0523059368133545\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 5.360485076904297 | KNN Loss: 4.361324787139893 | BCE Loss: 0.9991604089736938\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 5.409747123718262 | KNN Loss: 4.35038423538208 | BCE Loss: 1.0593626499176025\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 5.38603401184082 | KNN Loss: 4.347184181213379 | BCE Loss: 1.0388495922088623\n",
      "Epoch   107: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 5.360276222229004 | KNN Loss: 4.340292930603027 | BCE Loss: 1.0199832916259766\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 5.399430274963379 | KNN Loss: 4.359176158905029 | BCE Loss: 1.0402541160583496\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 5.3779988288879395 | KNN Loss: 4.334616184234619 | BCE Loss: 1.0433826446533203\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 5.425150394439697 | KNN Loss: 4.385594844818115 | BCE Loss: 1.0395554304122925\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 5.394914627075195 | KNN Loss: 4.3427557945251465 | BCE Loss: 1.0521588325500488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 5.368741989135742 | KNN Loss: 4.339701175689697 | BCE Loss: 1.0290406942367554\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 5.372265338897705 | KNN Loss: 4.341917991638184 | BCE Loss: 1.0303473472595215\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 5.401910781860352 | KNN Loss: 4.36207914352417 | BCE Loss: 1.0398316383361816\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 5.427125930786133 | KNN Loss: 4.393468856811523 | BCE Loss: 1.033657193183899\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 5.353525638580322 | KNN Loss: 4.347275733947754 | BCE Loss: 1.0062499046325684\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 5.37416410446167 | KNN Loss: 4.356557369232178 | BCE Loss: 1.0176067352294922\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 5.3692145347595215 | KNN Loss: 4.331296443939209 | BCE Loss: 1.037918210029602\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 5.38596773147583 | KNN Loss: 4.350383281707764 | BCE Loss: 1.0355844497680664\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 5.360899925231934 | KNN Loss: 4.335970878601074 | BCE Loss: 1.0249292850494385\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 5.356007099151611 | KNN Loss: 4.355310440063477 | BCE Loss: 1.0006966590881348\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 5.380746364593506 | KNN Loss: 4.353510856628418 | BCE Loss: 1.0272353887557983\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 5.371896743774414 | KNN Loss: 4.349134922027588 | BCE Loss: 1.0227620601654053\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 5.39987850189209 | KNN Loss: 4.3637375831604 | BCE Loss: 1.036141037940979\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 5.36865758895874 | KNN Loss: 4.340304851531982 | BCE Loss: 1.0283527374267578\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 5.428624629974365 | KNN Loss: 4.401449680328369 | BCE Loss: 1.027174949645996\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 5.363865852355957 | KNN Loss: 4.339347839355469 | BCE Loss: 1.0245182514190674\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 5.364322662353516 | KNN Loss: 4.362392425537109 | BCE Loss: 1.0019301176071167\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 5.413924217224121 | KNN Loss: 4.372251033782959 | BCE Loss: 1.0416733026504517\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 5.385636806488037 | KNN Loss: 4.34763240814209 | BCE Loss: 1.0380043983459473\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 5.391790390014648 | KNN Loss: 4.3441853523254395 | BCE Loss: 1.0476049184799194\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 5.357948303222656 | KNN Loss: 4.332094669342041 | BCE Loss: 1.0258537530899048\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 5.390922546386719 | KNN Loss: 4.3533453941345215 | BCE Loss: 1.0375772714614868\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 5.356849670410156 | KNN Loss: 4.340054035186768 | BCE Loss: 1.0167956352233887\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 5.366253852844238 | KNN Loss: 4.354181289672852 | BCE Loss: 1.0120726823806763\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 5.335532188415527 | KNN Loss: 4.349819183349609 | BCE Loss: 0.9857131838798523\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 5.3633623123168945 | KNN Loss: 4.362139701843262 | BCE Loss: 1.0012226104736328\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 5.425929546356201 | KNN Loss: 4.36180305480957 | BCE Loss: 1.0641264915466309\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 5.379798412322998 | KNN Loss: 4.349181652069092 | BCE Loss: 1.0306168794631958\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 5.376192092895508 | KNN Loss: 4.321042537689209 | BCE Loss: 1.0551495552062988\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 5.3918352127075195 | KNN Loss: 4.357332706451416 | BCE Loss: 1.0345027446746826\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 5.437627792358398 | KNN Loss: 4.404005527496338 | BCE Loss: 1.0336220264434814\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 5.3731865882873535 | KNN Loss: 4.343133449554443 | BCE Loss: 1.0300531387329102\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 5.416319847106934 | KNN Loss: 4.378098964691162 | BCE Loss: 1.0382211208343506\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 5.379243850708008 | KNN Loss: 4.343432903289795 | BCE Loss: 1.035810947418213\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 5.3731889724731445 | KNN Loss: 4.355257034301758 | BCE Loss: 1.0179316997528076\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 5.434138774871826 | KNN Loss: 4.3626227378845215 | BCE Loss: 1.0715160369873047\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 5.343723773956299 | KNN Loss: 4.334720611572266 | BCE Loss: 1.0090031623840332\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 5.353297233581543 | KNN Loss: 4.356777667999268 | BCE Loss: 0.9965198040008545\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 5.353414535522461 | KNN Loss: 4.356196403503418 | BCE Loss: 0.997218132019043\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 5.382879257202148 | KNN Loss: 4.344878196716309 | BCE Loss: 1.0380008220672607\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 5.391867160797119 | KNN Loss: 4.339598655700684 | BCE Loss: 1.0522685050964355\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 5.376543998718262 | KNN Loss: 4.3354716300964355 | BCE Loss: 1.041072130203247\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 5.369794845581055 | KNN Loss: 4.336679458618164 | BCE Loss: 1.0331156253814697\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 5.452133655548096 | KNN Loss: 4.400021076202393 | BCE Loss: 1.0521126985549927\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 5.375704765319824 | KNN Loss: 4.335016250610352 | BCE Loss: 1.0406886339187622\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 5.375705718994141 | KNN Loss: 4.352592468261719 | BCE Loss: 1.0231132507324219\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 5.384661674499512 | KNN Loss: 4.345925331115723 | BCE Loss: 1.038736343383789\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 5.3840155601501465 | KNN Loss: 4.347230434417725 | BCE Loss: 1.0367851257324219\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 5.360779285430908 | KNN Loss: 4.342529773712158 | BCE Loss: 1.0182496309280396\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 5.40035343170166 | KNN Loss: 4.363847732543945 | BCE Loss: 1.036505937576294\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 5.3560943603515625 | KNN Loss: 4.351309776306152 | BCE Loss: 1.0047848224639893\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 5.391903400421143 | KNN Loss: 4.341557025909424 | BCE Loss: 1.0503463745117188\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 5.365755558013916 | KNN Loss: 4.34289026260376 | BCE Loss: 1.0228652954101562\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 5.355717658996582 | KNN Loss: 4.349945545196533 | BCE Loss: 1.005772352218628\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 5.359885215759277 | KNN Loss: 4.334164619445801 | BCE Loss: 1.0257208347320557\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 5.439432144165039 | KNN Loss: 4.402569770812988 | BCE Loss: 1.0368621349334717\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 5.376319885253906 | KNN Loss: 4.343393802642822 | BCE Loss: 1.032926082611084\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 5.38853120803833 | KNN Loss: 4.347137928009033 | BCE Loss: 1.0413932800292969\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 5.337533950805664 | KNN Loss: 4.334139823913574 | BCE Loss: 1.0033942461013794\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 5.376425743103027 | KNN Loss: 4.35084867477417 | BCE Loss: 1.0255770683288574\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 5.377071380615234 | KNN Loss: 4.359588623046875 | BCE Loss: 1.0174826383590698\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 5.380656719207764 | KNN Loss: 4.3389668464660645 | BCE Loss: 1.0416897535324097\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 5.4189229011535645 | KNN Loss: 4.379032135009766 | BCE Loss: 1.0398907661437988\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 5.36712646484375 | KNN Loss: 4.357053756713867 | BCE Loss: 1.010072946548462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 5.371885299682617 | KNN Loss: 4.3273024559021 | BCE Loss: 1.0445830821990967\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 5.398066997528076 | KNN Loss: 4.3732452392578125 | BCE Loss: 1.0248217582702637\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 5.381849765777588 | KNN Loss: 4.352941036224365 | BCE Loss: 1.028908610343933\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 5.355139255523682 | KNN Loss: 4.3359785079956055 | BCE Loss: 1.0191607475280762\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 5.3712944984436035 | KNN Loss: 4.349690914154053 | BCE Loss: 1.0216037034988403\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 5.373980522155762 | KNN Loss: 4.353891849517822 | BCE Loss: 1.0200884342193604\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 5.375360488891602 | KNN Loss: 4.337036609649658 | BCE Loss: 1.0383238792419434\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 5.370473861694336 | KNN Loss: 4.354710102081299 | BCE Loss: 1.0157639980316162\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 5.398433685302734 | KNN Loss: 4.363380432128906 | BCE Loss: 1.0350533723831177\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 5.445591926574707 | KNN Loss: 4.416028022766113 | BCE Loss: 1.0295641422271729\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 5.414454460144043 | KNN Loss: 4.377199172973633 | BCE Loss: 1.0372552871704102\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 5.402331352233887 | KNN Loss: 4.34855318069458 | BCE Loss: 1.0537781715393066\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 5.385944366455078 | KNN Loss: 4.355067253112793 | BCE Loss: 1.0308773517608643\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 5.415461540222168 | KNN Loss: 4.3957648277282715 | BCE Loss: 1.0196969509124756\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 5.380588531494141 | KNN Loss: 4.347765922546387 | BCE Loss: 1.032822608947754\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 5.421267032623291 | KNN Loss: 4.375237941741943 | BCE Loss: 1.046028971672058\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 5.3528733253479 | KNN Loss: 4.354284763336182 | BCE Loss: 0.9985884428024292\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 5.391786098480225 | KNN Loss: 4.349859237670898 | BCE Loss: 1.0419268608093262\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 5.3564276695251465 | KNN Loss: 4.330350875854492 | BCE Loss: 1.0260767936706543\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 5.387271881103516 | KNN Loss: 4.3651204109191895 | BCE Loss: 1.0221514701843262\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 5.37968635559082 | KNN Loss: 4.357503414154053 | BCE Loss: 1.0221829414367676\n",
      "Epoch   122: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 5.370445251464844 | KNN Loss: 4.331810474395752 | BCE Loss: 1.0386348962783813\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 5.414649963378906 | KNN Loss: 4.370821952819824 | BCE Loss: 1.0438281297683716\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 5.386127471923828 | KNN Loss: 4.35050630569458 | BCE Loss: 1.0356214046478271\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 5.382226943969727 | KNN Loss: 4.3362226486206055 | BCE Loss: 1.046004295349121\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 5.372283935546875 | KNN Loss: 4.348805904388428 | BCE Loss: 1.0234777927398682\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 5.367387294769287 | KNN Loss: 4.3436174392700195 | BCE Loss: 1.0237698554992676\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 5.395016670227051 | KNN Loss: 4.353370666503906 | BCE Loss: 1.0416457653045654\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 5.367114067077637 | KNN Loss: 4.345261573791504 | BCE Loss: 1.021852731704712\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 5.346738815307617 | KNN Loss: 4.337747097015381 | BCE Loss: 1.0089917182922363\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 5.3560967445373535 | KNN Loss: 4.364813804626465 | BCE Loss: 0.9912830591201782\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 5.335444450378418 | KNN Loss: 4.332748889923096 | BCE Loss: 1.0026953220367432\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 5.365793228149414 | KNN Loss: 4.338282585144043 | BCE Loss: 1.0275107622146606\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 5.350099086761475 | KNN Loss: 4.344862461090088 | BCE Loss: 1.0052367448806763\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 5.394146919250488 | KNN Loss: 4.346827507019043 | BCE Loss: 1.0473194122314453\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 5.369704246520996 | KNN Loss: 4.35340690612793 | BCE Loss: 1.0162972211837769\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 5.353619575500488 | KNN Loss: 4.3204569816589355 | BCE Loss: 1.0331628322601318\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 5.369736671447754 | KNN Loss: 4.354744911193848 | BCE Loss: 1.0149915218353271\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 5.399059772491455 | KNN Loss: 4.353825569152832 | BCE Loss: 1.045234203338623\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 5.387856483459473 | KNN Loss: 4.330927848815918 | BCE Loss: 1.0569286346435547\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 5.362823963165283 | KNN Loss: 4.337255477905273 | BCE Loss: 1.0255686044692993\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 5.447283744812012 | KNN Loss: 4.394632816314697 | BCE Loss: 1.0526509284973145\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 5.372485160827637 | KNN Loss: 4.343019962310791 | BCE Loss: 1.0294649600982666\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 5.366080284118652 | KNN Loss: 4.344368934631348 | BCE Loss: 1.0217112302780151\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 5.416816711425781 | KNN Loss: 4.3805155754089355 | BCE Loss: 1.0363008975982666\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 5.417354106903076 | KNN Loss: 4.388549327850342 | BCE Loss: 1.0288046598434448\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 5.428300380706787 | KNN Loss: 4.3681206703186035 | BCE Loss: 1.060179591178894\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 5.403653144836426 | KNN Loss: 4.379891395568848 | BCE Loss: 1.023761510848999\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 5.390804290771484 | KNN Loss: 4.365924835205078 | BCE Loss: 1.0248796939849854\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 5.3888630867004395 | KNN Loss: 4.349930286407471 | BCE Loss: 1.0389329195022583\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 5.396960258483887 | KNN Loss: 4.354758262634277 | BCE Loss: 1.0422022342681885\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 5.427032470703125 | KNN Loss: 4.402797222137451 | BCE Loss: 1.0242350101470947\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 5.382986545562744 | KNN Loss: 4.3557024002075195 | BCE Loss: 1.0272842645645142\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 5.395965576171875 | KNN Loss: 4.323446273803711 | BCE Loss: 1.072519302368164\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 5.348991394042969 | KNN Loss: 4.357175827026367 | BCE Loss: 0.9918156266212463\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 5.3721184730529785 | KNN Loss: 4.341399192810059 | BCE Loss: 1.0307193994522095\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 5.383625030517578 | KNN Loss: 4.347123622894287 | BCE Loss: 1.036501407623291\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 5.37681770324707 | KNN Loss: 4.348085880279541 | BCE Loss: 1.0287315845489502\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 5.344052791595459 | KNN Loss: 4.348953723907471 | BCE Loss: 0.9950989484786987\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 5.46107292175293 | KNN Loss: 4.43310546875 | BCE Loss: 1.0279676914215088\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 5.401291370391846 | KNN Loss: 4.357346534729004 | BCE Loss: 1.0439448356628418\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 5.346343994140625 | KNN Loss: 4.329423904418945 | BCE Loss: 1.0169200897216797\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 5.377019882202148 | KNN Loss: 4.344242095947266 | BCE Loss: 1.0327775478363037\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 5.362862586975098 | KNN Loss: 4.344387054443359 | BCE Loss: 1.0184752941131592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 5.396441459655762 | KNN Loss: 4.349708080291748 | BCE Loss: 1.0467332601547241\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 5.39109992980957 | KNN Loss: 4.377358436584473 | BCE Loss: 1.0137414932250977\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 5.4089884757995605 | KNN Loss: 4.383548736572266 | BCE Loss: 1.025439739227295\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 5.416195392608643 | KNN Loss: 4.370363235473633 | BCE Loss: 1.0458320379257202\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 5.400256633758545 | KNN Loss: 4.376749038696289 | BCE Loss: 1.0235077142715454\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 5.387774467468262 | KNN Loss: 4.352993488311768 | BCE Loss: 1.0347812175750732\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 5.346580982208252 | KNN Loss: 4.322575569152832 | BCE Loss: 1.02400541305542\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 5.333121299743652 | KNN Loss: 4.330036163330078 | BCE Loss: 1.0030848979949951\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 5.363622188568115 | KNN Loss: 4.366650104522705 | BCE Loss: 0.9969720840454102\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 5.38557767868042 | KNN Loss: 4.364622116088867 | BCE Loss: 1.0209555625915527\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 5.344671726226807 | KNN Loss: 4.323915004730225 | BCE Loss: 1.020756721496582\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 5.416195392608643 | KNN Loss: 4.344091892242432 | BCE Loss: 1.0721033811569214\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 5.401165962219238 | KNN Loss: 4.351175308227539 | BCE Loss: 1.0499908924102783\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 5.355753421783447 | KNN Loss: 4.351054668426514 | BCE Loss: 1.0046987533569336\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 5.334643363952637 | KNN Loss: 4.3394455909729 | BCE Loss: 0.9951976537704468\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 5.366508960723877 | KNN Loss: 4.340333938598633 | BCE Loss: 1.0261750221252441\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 5.361274719238281 | KNN Loss: 4.339365005493164 | BCE Loss: 1.0219099521636963\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 5.393373489379883 | KNN Loss: 4.3570733070373535 | BCE Loss: 1.0362999439239502\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 5.397228240966797 | KNN Loss: 4.3380913734436035 | BCE Loss: 1.059136986732483\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 5.375750541687012 | KNN Loss: 4.340292453765869 | BCE Loss: 1.035457968711853\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 5.368983268737793 | KNN Loss: 4.353147983551025 | BCE Loss: 1.0158354043960571\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 5.345103740692139 | KNN Loss: 4.3175458908081055 | BCE Loss: 1.0275578498840332\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 5.362639427185059 | KNN Loss: 4.3454670906066895 | BCE Loss: 1.0171722173690796\n",
      "Epoch   133: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 5.383117198944092 | KNN Loss: 4.354050159454346 | BCE Loss: 1.029067039489746\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 5.397680759429932 | KNN Loss: 4.358928680419922 | BCE Loss: 1.0387519598007202\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 5.378386497497559 | KNN Loss: 4.357814311981201 | BCE Loss: 1.0205724239349365\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 5.3987016677856445 | KNN Loss: 4.365624904632568 | BCE Loss: 1.0330770015716553\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 5.349122047424316 | KNN Loss: 4.3466010093688965 | BCE Loss: 1.0025209188461304\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 5.374535083770752 | KNN Loss: 4.340850830078125 | BCE Loss: 1.033684253692627\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 5.401230812072754 | KNN Loss: 4.344432353973389 | BCE Loss: 1.0567986965179443\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 5.400956153869629 | KNN Loss: 4.35982084274292 | BCE Loss: 1.041135311126709\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 5.381536483764648 | KNN Loss: 4.378209114074707 | BCE Loss: 1.0033271312713623\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 5.395802021026611 | KNN Loss: 4.366248607635498 | BCE Loss: 1.0295535326004028\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 5.419000148773193 | KNN Loss: 4.397978782653809 | BCE Loss: 1.0210212469100952\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 5.384066581726074 | KNN Loss: 4.3740644454956055 | BCE Loss: 1.0100018978118896\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 5.346184730529785 | KNN Loss: 4.332322120666504 | BCE Loss: 1.0138623714447021\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 5.353996276855469 | KNN Loss: 4.338214874267578 | BCE Loss: 1.0157816410064697\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 5.372297763824463 | KNN Loss: 4.339715957641602 | BCE Loss: 1.0325818061828613\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 5.376547813415527 | KNN Loss: 4.330955505371094 | BCE Loss: 1.0455925464630127\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 5.399270057678223 | KNN Loss: 4.329339027404785 | BCE Loss: 1.0699312686920166\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 5.3838090896606445 | KNN Loss: 4.346440315246582 | BCE Loss: 1.037368655204773\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 5.396716594696045 | KNN Loss: 4.369411468505859 | BCE Loss: 1.027305245399475\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 5.356036186218262 | KNN Loss: 4.336436748504639 | BCE Loss: 1.0195993185043335\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 5.373781681060791 | KNN Loss: 4.35456657409668 | BCE Loss: 1.0192152261734009\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 5.34868049621582 | KNN Loss: 4.346999168395996 | BCE Loss: 1.0016810894012451\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 5.350037574768066 | KNN Loss: 4.330636024475098 | BCE Loss: 1.0194016695022583\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 5.374304294586182 | KNN Loss: 4.341876029968262 | BCE Loss: 1.03242826461792\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 5.383785724639893 | KNN Loss: 4.340919017791748 | BCE Loss: 1.0428667068481445\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 5.375985145568848 | KNN Loss: 4.3645124435424805 | BCE Loss: 1.0114728212356567\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 5.356337547302246 | KNN Loss: 4.328750133514404 | BCE Loss: 1.0275871753692627\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 5.352784633636475 | KNN Loss: 4.3341169357299805 | BCE Loss: 1.0186675786972046\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 5.397634506225586 | KNN Loss: 4.356821537017822 | BCE Loss: 1.0408129692077637\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 5.362851619720459 | KNN Loss: 4.332581996917725 | BCE Loss: 1.0302696228027344\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 5.384774208068848 | KNN Loss: 4.358222007751465 | BCE Loss: 1.0265519618988037\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 5.422153949737549 | KNN Loss: 4.399652004241943 | BCE Loss: 1.0225019454956055\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 5.332719326019287 | KNN Loss: 4.339887619018555 | BCE Loss: 0.9928318858146667\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 5.371602535247803 | KNN Loss: 4.344937324523926 | BCE Loss: 1.026665210723877\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 5.36544132232666 | KNN Loss: 4.333835601806641 | BCE Loss: 1.0316057205200195\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 5.401486396789551 | KNN Loss: 4.385626316070557 | BCE Loss: 1.015859842300415\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 5.392004013061523 | KNN Loss: 4.385354042053223 | BCE Loss: 1.0066497325897217\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 5.357933521270752 | KNN Loss: 4.3380866050720215 | BCE Loss: 1.01984703540802\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 5.422121047973633 | KNN Loss: 4.383601665496826 | BCE Loss: 1.0385196208953857\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 5.421108245849609 | KNN Loss: 4.390819072723389 | BCE Loss: 1.0302890539169312\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 5.374145984649658 | KNN Loss: 4.346071243286133 | BCE Loss: 1.0280747413635254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 5.369327545166016 | KNN Loss: 4.333723068237305 | BCE Loss: 1.0356043577194214\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 5.357727527618408 | KNN Loss: 4.3232741355896 | BCE Loss: 1.0344535112380981\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 5.402801513671875 | KNN Loss: 4.353353977203369 | BCE Loss: 1.0494476556777954\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 5.34882926940918 | KNN Loss: 4.340889930725098 | BCE Loss: 1.007939338684082\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 5.380702018737793 | KNN Loss: 4.3462138175964355 | BCE Loss: 1.0344880819320679\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 5.354951858520508 | KNN Loss: 4.340846061706543 | BCE Loss: 1.0141057968139648\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 5.375967502593994 | KNN Loss: 4.342637538909912 | BCE Loss: 1.0333298444747925\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 5.348485946655273 | KNN Loss: 4.347087860107422 | BCE Loss: 1.0013980865478516\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 5.384380340576172 | KNN Loss: 4.3406195640563965 | BCE Loss: 1.0437605381011963\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 5.345566272735596 | KNN Loss: 4.33284854888916 | BCE Loss: 1.0127177238464355\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 5.390395164489746 | KNN Loss: 4.35133695602417 | BCE Loss: 1.0390584468841553\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 5.413233280181885 | KNN Loss: 4.381129264831543 | BCE Loss: 1.0321038961410522\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 5.375309467315674 | KNN Loss: 4.336907386779785 | BCE Loss: 1.0384021997451782\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 5.3417768478393555 | KNN Loss: 4.342044830322266 | BCE Loss: 0.999732255935669\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 5.346320152282715 | KNN Loss: 4.350545883178711 | BCE Loss: 0.9957742094993591\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 5.381636619567871 | KNN Loss: 4.367917537689209 | BCE Loss: 1.013718843460083\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 5.3333234786987305 | KNN Loss: 4.329756736755371 | BCE Loss: 1.0035667419433594\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 5.377462387084961 | KNN Loss: 4.336190700531006 | BCE Loss: 1.0412715673446655\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 5.349114418029785 | KNN Loss: 4.323649883270264 | BCE Loss: 1.0254642963409424\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 5.448644161224365 | KNN Loss: 4.428194999694824 | BCE Loss: 1.020449161529541\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 5.39849328994751 | KNN Loss: 4.382424354553223 | BCE Loss: 1.0160688161849976\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 5.352391242980957 | KNN Loss: 4.3568854331970215 | BCE Loss: 0.9955059289932251\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 5.368003845214844 | KNN Loss: 4.373814582824707 | BCE Loss: 0.9941893219947815\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 5.410812854766846 | KNN Loss: 4.371288776397705 | BCE Loss: 1.0395241975784302\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 5.374557971954346 | KNN Loss: 4.3480987548828125 | BCE Loss: 1.0264592170715332\n",
      "Epoch   144: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 5.414874076843262 | KNN Loss: 4.3679423332214355 | BCE Loss: 1.046931505203247\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 5.363976955413818 | KNN Loss: 4.34989595413208 | BCE Loss: 1.0140810012817383\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 5.4064621925354 | KNN Loss: 4.3662428855896 | BCE Loss: 1.0402194261550903\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 5.369894027709961 | KNN Loss: 4.349502086639404 | BCE Loss: 1.0203921794891357\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 5.369113922119141 | KNN Loss: 4.350709915161133 | BCE Loss: 1.0184037685394287\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 5.341605186462402 | KNN Loss: 4.328541278839111 | BCE Loss: 1.0130637884140015\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 5.386634349822998 | KNN Loss: 4.3474555015563965 | BCE Loss: 1.0391788482666016\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 5.358016014099121 | KNN Loss: 4.337507724761963 | BCE Loss: 1.0205085277557373\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 5.385067462921143 | KNN Loss: 4.332205772399902 | BCE Loss: 1.0528616905212402\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 5.39296817779541 | KNN Loss: 4.346130847930908 | BCE Loss: 1.046837568283081\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 5.376595497131348 | KNN Loss: 4.357211589813232 | BCE Loss: 1.0193836688995361\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 5.382804870605469 | KNN Loss: 4.357154369354248 | BCE Loss: 1.0256505012512207\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 5.350867748260498 | KNN Loss: 4.351312160491943 | BCE Loss: 0.9995557069778442\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 5.3653059005737305 | KNN Loss: 4.341145992279053 | BCE Loss: 1.0241601467132568\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 5.37309455871582 | KNN Loss: 4.352895259857178 | BCE Loss: 1.0201992988586426\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 5.382320404052734 | KNN Loss: 4.352931976318359 | BCE Loss: 1.029388666152954\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 5.345625877380371 | KNN Loss: 4.352224349975586 | BCE Loss: 0.9934014678001404\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 5.379850387573242 | KNN Loss: 4.351787090301514 | BCE Loss: 1.028063178062439\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 5.355113983154297 | KNN Loss: 4.326292037963867 | BCE Loss: 1.0288221836090088\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 5.392475605010986 | KNN Loss: 4.364666938781738 | BCE Loss: 1.027808666229248\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 5.3543701171875 | KNN Loss: 4.336852073669434 | BCE Loss: 1.0175182819366455\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 5.391972541809082 | KNN Loss: 4.343982696533203 | BCE Loss: 1.047989845275879\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 5.372150421142578 | KNN Loss: 4.351840496063232 | BCE Loss: 1.0203099250793457\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 5.3850932121276855 | KNN Loss: 4.35811185836792 | BCE Loss: 1.0269814729690552\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 5.417932510375977 | KNN Loss: 4.375363349914551 | BCE Loss: 1.0425689220428467\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 5.401174545288086 | KNN Loss: 4.3643927574157715 | BCE Loss: 1.0367817878723145\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 5.371574401855469 | KNN Loss: 4.364243507385254 | BCE Loss: 1.0073308944702148\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 5.394165992736816 | KNN Loss: 4.383051872253418 | BCE Loss: 1.011114239692688\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 5.368106842041016 | KNN Loss: 4.327056407928467 | BCE Loss: 1.041050672531128\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 5.357975482940674 | KNN Loss: 4.335646152496338 | BCE Loss: 1.0223294496536255\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 5.391552448272705 | KNN Loss: 4.363920211791992 | BCE Loss: 1.027632236480713\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 5.440890312194824 | KNN Loss: 4.393896102905273 | BCE Loss: 1.0469942092895508\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 5.382501602172852 | KNN Loss: 4.351714611053467 | BCE Loss: 1.0307867527008057\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 5.394604206085205 | KNN Loss: 4.336319446563721 | BCE Loss: 1.058284878730774\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 5.382508277893066 | KNN Loss: 4.340675354003906 | BCE Loss: 1.041832685470581\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 5.411263465881348 | KNN Loss: 4.389834403991699 | BCE Loss: 1.021429181098938\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 5.3895416259765625 | KNN Loss: 4.35809326171875 | BCE Loss: 1.0314483642578125\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 5.353870391845703 | KNN Loss: 4.337408542633057 | BCE Loss: 1.0164618492126465\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 5.372234344482422 | KNN Loss: 4.337431907653809 | BCE Loss: 1.0348026752471924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 5.391356468200684 | KNN Loss: 4.337191581726074 | BCE Loss: 1.0541646480560303\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 5.380334854125977 | KNN Loss: 4.352839469909668 | BCE Loss: 1.0274951457977295\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 5.37720251083374 | KNN Loss: 4.338650226593018 | BCE Loss: 1.038552165031433\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 5.391068458557129 | KNN Loss: 4.367131233215332 | BCE Loss: 1.023937463760376\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 5.3671040534973145 | KNN Loss: 4.345143795013428 | BCE Loss: 1.0219602584838867\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 5.376875877380371 | KNN Loss: 4.345682144165039 | BCE Loss: 1.0311939716339111\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 5.3774261474609375 | KNN Loss: 4.337563991546631 | BCE Loss: 1.0398619174957275\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 5.35535192489624 | KNN Loss: 4.353546619415283 | BCE Loss: 1.001805305480957\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 5.385307312011719 | KNN Loss: 4.339006423950195 | BCE Loss: 1.0463008880615234\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 5.336483001708984 | KNN Loss: 4.325623035430908 | BCE Loss: 1.0108599662780762\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 5.363640785217285 | KNN Loss: 4.327146530151367 | BCE Loss: 1.036494493484497\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 5.361547946929932 | KNN Loss: 4.3335280418396 | BCE Loss: 1.028019905090332\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 5.384323596954346 | KNN Loss: 4.362924575805664 | BCE Loss: 1.0213991403579712\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 5.421588897705078 | KNN Loss: 4.390069484710693 | BCE Loss: 1.0315194129943848\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 5.384030342102051 | KNN Loss: 4.347173690795898 | BCE Loss: 1.0368568897247314\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 5.400264263153076 | KNN Loss: 4.384980201721191 | BCE Loss: 1.0152840614318848\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 5.362170696258545 | KNN Loss: 4.362906455993652 | BCE Loss: 0.9992640614509583\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 5.414841175079346 | KNN Loss: 4.357730388641357 | BCE Loss: 1.0571107864379883\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 5.422402381896973 | KNN Loss: 4.391707897186279 | BCE Loss: 1.0306947231292725\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 5.419233798980713 | KNN Loss: 4.36765193939209 | BCE Loss: 1.051581859588623\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 5.350342273712158 | KNN Loss: 4.327209949493408 | BCE Loss: 1.0231322050094604\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 5.356797695159912 | KNN Loss: 4.349059104919434 | BCE Loss: 1.0077385902404785\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 5.349597930908203 | KNN Loss: 4.327466011047363 | BCE Loss: 1.0221316814422607\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 5.411455154418945 | KNN Loss: 4.373061656951904 | BCE Loss: 1.0383933782577515\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 5.360092639923096 | KNN Loss: 4.323665142059326 | BCE Loss: 1.0364274978637695\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 5.375573635101318 | KNN Loss: 4.367875576019287 | BCE Loss: 1.0076980590820312\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 5.392428874969482 | KNN Loss: 4.371404647827148 | BCE Loss: 1.0210241079330444\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 5.373106002807617 | KNN Loss: 4.333322048187256 | BCE Loss: 1.0397839546203613\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 5.383658409118652 | KNN Loss: 4.330377101898193 | BCE Loss: 1.0532810688018799\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 5.3794145584106445 | KNN Loss: 4.36027193069458 | BCE Loss: 1.019142508506775\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 5.386936187744141 | KNN Loss: 4.368635654449463 | BCE Loss: 1.0183007717132568\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 5.3982343673706055 | KNN Loss: 4.378489017486572 | BCE Loss: 1.0197453498840332\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 5.431930065155029 | KNN Loss: 4.375200271606445 | BCE Loss: 1.056729793548584\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 5.3985395431518555 | KNN Loss: 4.340989112854004 | BCE Loss: 1.0575505495071411\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 5.394130706787109 | KNN Loss: 4.352521896362305 | BCE Loss: 1.0416090488433838\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 5.387294769287109 | KNN Loss: 4.364110469818115 | BCE Loss: 1.023184061050415\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 5.368260383605957 | KNN Loss: 4.354146480560303 | BCE Loss: 1.0141140222549438\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 5.341181755065918 | KNN Loss: 4.340880393981934 | BCE Loss: 1.0003013610839844\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 5.421051025390625 | KNN Loss: 4.35144567489624 | BCE Loss: 1.0696052312850952\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 5.36634635925293 | KNN Loss: 4.320347309112549 | BCE Loss: 1.0459988117218018\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 5.34486722946167 | KNN Loss: 4.3382673263549805 | BCE Loss: 1.0065997838974\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 5.356695175170898 | KNN Loss: 4.339847564697266 | BCE Loss: 1.0168473720550537\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 5.368494510650635 | KNN Loss: 4.349663734436035 | BCE Loss: 1.0188308954238892\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 5.389881134033203 | KNN Loss: 4.361147880554199 | BCE Loss: 1.028733253479004\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 5.338634490966797 | KNN Loss: 4.324456214904785 | BCE Loss: 1.0141782760620117\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 5.358700752258301 | KNN Loss: 4.34836483001709 | BCE Loss: 1.01033616065979\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 5.3804473876953125 | KNN Loss: 4.352649688720703 | BCE Loss: 1.0277974605560303\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 5.405250072479248 | KNN Loss: 4.333494663238525 | BCE Loss: 1.071755290031433\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 5.366156578063965 | KNN Loss: 4.36683464050293 | BCE Loss: 0.9993220567703247\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 5.413755893707275 | KNN Loss: 4.358377456665039 | BCE Loss: 1.0553785562515259\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 5.377747058868408 | KNN Loss: 4.355477333068848 | BCE Loss: 1.0222697257995605\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 5.363762855529785 | KNN Loss: 4.337252616882324 | BCE Loss: 1.02651047706604\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 5.384490013122559 | KNN Loss: 4.336144924163818 | BCE Loss: 1.0483452081680298\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 5.374906539916992 | KNN Loss: 4.355440139770508 | BCE Loss: 1.0194666385650635\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 5.359304428100586 | KNN Loss: 4.332860469818115 | BCE Loss: 1.0264439582824707\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 5.3509063720703125 | KNN Loss: 4.331571578979492 | BCE Loss: 1.0193345546722412\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 5.379100799560547 | KNN Loss: 4.353466987609863 | BCE Loss: 1.0256338119506836\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 5.373929500579834 | KNN Loss: 4.362205982208252 | BCE Loss: 1.0117236375808716\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 5.391633987426758 | KNN Loss: 4.360864162445068 | BCE Loss: 1.0307700634002686\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 5.358901023864746 | KNN Loss: 4.344305038452148 | BCE Loss: 1.0145961046218872\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 5.382789611816406 | KNN Loss: 4.353680610656738 | BCE Loss: 1.0291087627410889\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 5.382356643676758 | KNN Loss: 4.3595051765441895 | BCE Loss: 1.0228517055511475\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 5.404209613800049 | KNN Loss: 4.36569881439209 | BCE Loss: 1.038510799407959\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 5.365184307098389 | KNN Loss: 4.3734941482543945 | BCE Loss: 0.9916899800300598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 5.384667873382568 | KNN Loss: 4.344208240509033 | BCE Loss: 1.0404596328735352\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 5.383322715759277 | KNN Loss: 4.340900421142578 | BCE Loss: 1.0424225330352783\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 5.4023237228393555 | KNN Loss: 4.3553314208984375 | BCE Loss: 1.0469920635223389\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 5.424866676330566 | KNN Loss: 4.415889263153076 | BCE Loss: 1.0089771747589111\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 5.378792762756348 | KNN Loss: 4.353711128234863 | BCE Loss: 1.0250813961029053\n",
      "Epoch   162: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 5.3864426612854 | KNN Loss: 4.3561787605285645 | BCE Loss: 1.030263900756836\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 5.338135242462158 | KNN Loss: 4.3354363441467285 | BCE Loss: 1.0026987791061401\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 5.384129524230957 | KNN Loss: 4.3607378005981445 | BCE Loss: 1.023391604423523\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 5.38844108581543 | KNN Loss: 4.371396064758301 | BCE Loss: 1.017045021057129\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 5.383110523223877 | KNN Loss: 4.364622592926025 | BCE Loss: 1.0184879302978516\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 5.431707859039307 | KNN Loss: 4.382649898529053 | BCE Loss: 1.0490578413009644\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 5.370446681976318 | KNN Loss: 4.342856407165527 | BCE Loss: 1.027590274810791\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 5.383885383605957 | KNN Loss: 4.368381023406982 | BCE Loss: 1.015504240989685\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 5.378664016723633 | KNN Loss: 4.373924255371094 | BCE Loss: 1.0047398805618286\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 5.34478235244751 | KNN Loss: 4.328939437866211 | BCE Loss: 1.0158430337905884\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 5.4205803871154785 | KNN Loss: 4.402036190032959 | BCE Loss: 1.0185441970825195\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 5.3484907150268555 | KNN Loss: 4.33249044418335 | BCE Loss: 1.016000509262085\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 5.379324913024902 | KNN Loss: 4.352328777313232 | BCE Loss: 1.0269960165023804\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 5.388716220855713 | KNN Loss: 4.361108779907227 | BCE Loss: 1.0276073217391968\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 5.400502681732178 | KNN Loss: 4.378446578979492 | BCE Loss: 1.022056221961975\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 5.403312683105469 | KNN Loss: 4.359293460845947 | BCE Loss: 1.0440192222595215\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 5.395858287811279 | KNN Loss: 4.360314846038818 | BCE Loss: 1.0355435609817505\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 5.3892316818237305 | KNN Loss: 4.356296062469482 | BCE Loss: 1.032935619354248\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 5.432692527770996 | KNN Loss: 4.419286251068115 | BCE Loss: 1.0134062767028809\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 5.363834381103516 | KNN Loss: 4.338619232177734 | BCE Loss: 1.0252151489257812\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 5.396397113800049 | KNN Loss: 4.384249210357666 | BCE Loss: 1.0121479034423828\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 5.388148784637451 | KNN Loss: 4.344934940338135 | BCE Loss: 1.0432138442993164\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 5.337132453918457 | KNN Loss: 4.349790096282959 | BCE Loss: 0.9873425960540771\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 5.439515113830566 | KNN Loss: 4.3774094581604 | BCE Loss: 1.0621057748794556\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 5.365566730499268 | KNN Loss: 4.339022159576416 | BCE Loss: 1.0265446901321411\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 5.336996078491211 | KNN Loss: 4.340228080749512 | BCE Loss: 0.9967679977416992\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 5.358898162841797 | KNN Loss: 4.343324184417725 | BCE Loss: 1.0155742168426514\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 5.389406681060791 | KNN Loss: 4.364241123199463 | BCE Loss: 1.0251654386520386\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 5.365918159484863 | KNN Loss: 4.3371262550354 | BCE Loss: 1.0287916660308838\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 5.371832847595215 | KNN Loss: 4.340149402618408 | BCE Loss: 1.0316836833953857\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 5.383240222930908 | KNN Loss: 4.351687908172607 | BCE Loss: 1.0315523147583008\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 5.36623477935791 | KNN Loss: 4.341921329498291 | BCE Loss: 1.0243134498596191\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 5.424136638641357 | KNN Loss: 4.391854286193848 | BCE Loss: 1.0322823524475098\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 5.367495536804199 | KNN Loss: 4.3539652824401855 | BCE Loss: 1.0135300159454346\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 5.387584686279297 | KNN Loss: 4.343628883361816 | BCE Loss: 1.043955683708191\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 5.398412704467773 | KNN Loss: 4.360976219177246 | BCE Loss: 1.0374367237091064\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 5.355138778686523 | KNN Loss: 4.33992338180542 | BCE Loss: 1.015215277671814\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 5.374288082122803 | KNN Loss: 4.373847961425781 | BCE Loss: 1.0004401206970215\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 5.377871036529541 | KNN Loss: 4.3410258293151855 | BCE Loss: 1.036845088005066\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 5.388238906860352 | KNN Loss: 4.368055820465088 | BCE Loss: 1.0201829671859741\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 5.403984546661377 | KNN Loss: 4.366667747497559 | BCE Loss: 1.0373167991638184\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 5.32569694519043 | KNN Loss: 4.31496524810791 | BCE Loss: 1.0107319355010986\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 5.377011299133301 | KNN Loss: 4.326711654663086 | BCE Loss: 1.0502996444702148\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 5.38020133972168 | KNN Loss: 4.366043567657471 | BCE Loss: 1.0141576528549194\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 5.381252765655518 | KNN Loss: 4.343692779541016 | BCE Loss: 1.037559986114502\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 5.394289493560791 | KNN Loss: 4.350592613220215 | BCE Loss: 1.0436967611312866\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 5.418095588684082 | KNN Loss: 4.396875381469727 | BCE Loss: 1.0212199687957764\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 5.411343574523926 | KNN Loss: 4.374022483825684 | BCE Loss: 1.0373213291168213\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 5.385021209716797 | KNN Loss: 4.364579200744629 | BCE Loss: 1.020442008972168\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 5.380683898925781 | KNN Loss: 4.339694499969482 | BCE Loss: 1.0409892797470093\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 5.381425857543945 | KNN Loss: 4.348699569702148 | BCE Loss: 1.0327264070510864\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 5.381601810455322 | KNN Loss: 4.339123249053955 | BCE Loss: 1.0424785614013672\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 5.402141571044922 | KNN Loss: 4.352287769317627 | BCE Loss: 1.0498535633087158\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 5.368366241455078 | KNN Loss: 4.347329616546631 | BCE Loss: 1.0210368633270264\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 5.4368486404418945 | KNN Loss: 4.372828960418701 | BCE Loss: 1.0640196800231934\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 5.388764381408691 | KNN Loss: 4.350469589233398 | BCE Loss: 1.0382945537567139\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 5.3826141357421875 | KNN Loss: 4.338187217712402 | BCE Loss: 1.0444267988204956\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 5.461062908172607 | KNN Loss: 4.426052093505859 | BCE Loss: 1.0350106954574585\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 5.393588542938232 | KNN Loss: 4.373196601867676 | BCE Loss: 1.020391821861267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 5.327816963195801 | KNN Loss: 4.338626861572266 | BCE Loss: 0.9891899824142456\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 5.362992286682129 | KNN Loss: 4.333486080169678 | BCE Loss: 1.0295063257217407\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 5.363020896911621 | KNN Loss: 4.3705058097839355 | BCE Loss: 0.9925153255462646\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 5.33917236328125 | KNN Loss: 4.328516960144043 | BCE Loss: 1.0106556415557861\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 5.359407424926758 | KNN Loss: 4.333460330963135 | BCE Loss: 1.025947093963623\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 5.374569892883301 | KNN Loss: 4.34463357925415 | BCE Loss: 1.0299365520477295\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 5.3525800704956055 | KNN Loss: 4.324286937713623 | BCE Loss: 1.028293251991272\n",
      "Epoch   173: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 5.389107704162598 | KNN Loss: 4.342726230621338 | BCE Loss: 1.0463813543319702\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 5.374576091766357 | KNN Loss: 4.335494518280029 | BCE Loss: 1.0390815734863281\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 5.352980136871338 | KNN Loss: 4.339038372039795 | BCE Loss: 1.013941764831543\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 5.343503952026367 | KNN Loss: 4.321287631988525 | BCE Loss: 1.0222164392471313\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 5.418974876403809 | KNN Loss: 4.379908561706543 | BCE Loss: 1.0390665531158447\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 5.380178451538086 | KNN Loss: 4.33858585357666 | BCE Loss: 1.0415923595428467\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 5.367453098297119 | KNN Loss: 4.343323707580566 | BCE Loss: 1.0241292715072632\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 5.383949279785156 | KNN Loss: 4.3336615562438965 | BCE Loss: 1.0502876043319702\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 5.376655101776123 | KNN Loss: 4.326472759246826 | BCE Loss: 1.0501823425292969\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 5.36789083480835 | KNN Loss: 4.33718204498291 | BCE Loss: 1.0307087898254395\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 5.3560075759887695 | KNN Loss: 4.334372520446777 | BCE Loss: 1.0216352939605713\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 5.39134407043457 | KNN Loss: 4.3649678230285645 | BCE Loss: 1.0263761281967163\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 5.360959053039551 | KNN Loss: 4.3342766761779785 | BCE Loss: 1.0266823768615723\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 5.363160133361816 | KNN Loss: 4.338251113891602 | BCE Loss: 1.0249090194702148\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 5.388148784637451 | KNN Loss: 4.347834587097168 | BCE Loss: 1.0403141975402832\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 5.3706231117248535 | KNN Loss: 4.339219570159912 | BCE Loss: 1.0314035415649414\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 5.372352123260498 | KNN Loss: 4.347104549407959 | BCE Loss: 1.025247573852539\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 5.347621917724609 | KNN Loss: 4.328774452209473 | BCE Loss: 1.0188472270965576\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 5.408329963684082 | KNN Loss: 4.368222236633301 | BCE Loss: 1.0401076078414917\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 5.375284194946289 | KNN Loss: 4.3497419357299805 | BCE Loss: 1.0255424976348877\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 5.42248010635376 | KNN Loss: 4.362982749938965 | BCE Loss: 1.0594972372055054\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 5.367119789123535 | KNN Loss: 4.357123374938965 | BCE Loss: 1.0099961757659912\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 5.3336405754089355 | KNN Loss: 4.322392463684082 | BCE Loss: 1.0112481117248535\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 5.389461994171143 | KNN Loss: 4.346085071563721 | BCE Loss: 1.0433769226074219\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 5.4012064933776855 | KNN Loss: 4.373880386352539 | BCE Loss: 1.0273261070251465\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 5.372793674468994 | KNN Loss: 4.343656063079834 | BCE Loss: 1.0291374921798706\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 5.419609069824219 | KNN Loss: 4.358034133911133 | BCE Loss: 1.0615746974945068\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 5.387189865112305 | KNN Loss: 4.345212459564209 | BCE Loss: 1.0419775247573853\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 5.362297534942627 | KNN Loss: 4.3519487380981445 | BCE Loss: 1.010348916053772\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 5.368185520172119 | KNN Loss: 4.349642753601074 | BCE Loss: 1.0185428857803345\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 5.382460594177246 | KNN Loss: 4.349289417266846 | BCE Loss: 1.03317129611969\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 5.398351669311523 | KNN Loss: 4.387123107910156 | BCE Loss: 1.0112285614013672\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 5.402910232543945 | KNN Loss: 4.358596324920654 | BCE Loss: 1.0443137884140015\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 5.339637279510498 | KNN Loss: 4.326685428619385 | BCE Loss: 1.0129517316818237\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 5.385308265686035 | KNN Loss: 4.357442378997803 | BCE Loss: 1.027866005897522\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 5.428348064422607 | KNN Loss: 4.347151756286621 | BCE Loss: 1.0811963081359863\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 5.391200065612793 | KNN Loss: 4.354097366333008 | BCE Loss: 1.0371025800704956\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 5.398183822631836 | KNN Loss: 4.339457988739014 | BCE Loss: 1.0587260723114014\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 5.397355079650879 | KNN Loss: 4.341536045074463 | BCE Loss: 1.0558189153671265\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 5.370224952697754 | KNN Loss: 4.327680587768555 | BCE Loss: 1.0425441265106201\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 5.393743991851807 | KNN Loss: 4.361819267272949 | BCE Loss: 1.031924843788147\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 5.380168914794922 | KNN Loss: 4.366562843322754 | BCE Loss: 1.013606309890747\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 5.4003987312316895 | KNN Loss: 4.357758522033691 | BCE Loss: 1.042640209197998\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 5.388697147369385 | KNN Loss: 4.354470729827881 | BCE Loss: 1.034226417541504\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 5.339395523071289 | KNN Loss: 4.317183017730713 | BCE Loss: 1.0222126245498657\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 5.369340896606445 | KNN Loss: 4.340338706970215 | BCE Loss: 1.029002070426941\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 5.355670928955078 | KNN Loss: 4.343195915222168 | BCE Loss: 1.0124752521514893\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 5.366349697113037 | KNN Loss: 4.347433567047119 | BCE Loss: 1.0189160108566284\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 5.364315032958984 | KNN Loss: 4.314938545227051 | BCE Loss: 1.0493764877319336\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 5.356279373168945 | KNN Loss: 4.352113723754883 | BCE Loss: 1.0041658878326416\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 5.361274242401123 | KNN Loss: 4.337752819061279 | BCE Loss: 1.0235214233398438\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 5.359182357788086 | KNN Loss: 4.349223613739014 | BCE Loss: 1.0099585056304932\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 5.385908126831055 | KNN Loss: 4.344582557678223 | BCE Loss: 1.041325569152832\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 5.387729167938232 | KNN Loss: 4.353125095367432 | BCE Loss: 1.0346040725708008\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 5.389777183532715 | KNN Loss: 4.346071243286133 | BCE Loss: 1.0437061786651611\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 5.373891830444336 | KNN Loss: 4.336194038391113 | BCE Loss: 1.0376979112625122\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 5.410897254943848 | KNN Loss: 4.351799488067627 | BCE Loss: 1.0590975284576416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 5.417591094970703 | KNN Loss: 4.371377468109131 | BCE Loss: 1.0462136268615723\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 5.36545991897583 | KNN Loss: 4.356813907623291 | BCE Loss: 1.008646011352539\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 5.343201637268066 | KNN Loss: 4.347002983093262 | BCE Loss: 0.9961984157562256\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 5.409252643585205 | KNN Loss: 4.386538028717041 | BCE Loss: 1.0227144956588745\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 5.384638786315918 | KNN Loss: 4.3416571617126465 | BCE Loss: 1.0429816246032715\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 5.389078140258789 | KNN Loss: 4.338427543640137 | BCE Loss: 1.050650715827942\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 5.3370256423950195 | KNN Loss: 4.345485687255859 | BCE Loss: 0.9915398955345154\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 5.350704193115234 | KNN Loss: 4.346925735473633 | BCE Loss: 1.0037784576416016\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 5.390233516693115 | KNN Loss: 4.32955265045166 | BCE Loss: 1.0606809854507446\n",
      "Epoch   184: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 5.385600566864014 | KNN Loss: 4.330596446990967 | BCE Loss: 1.0550041198730469\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 5.371540069580078 | KNN Loss: 4.338150978088379 | BCE Loss: 1.0333892107009888\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 5.382675647735596 | KNN Loss: 4.377434253692627 | BCE Loss: 1.0052413940429688\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 5.379523277282715 | KNN Loss: 4.358558654785156 | BCE Loss: 1.0209643840789795\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 5.376828670501709 | KNN Loss: 4.348324775695801 | BCE Loss: 1.0285040140151978\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 5.349970817565918 | KNN Loss: 4.3224592208862305 | BCE Loss: 1.0275115966796875\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 5.378347396850586 | KNN Loss: 4.348830223083496 | BCE Loss: 1.0295172929763794\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 5.347043991088867 | KNN Loss: 4.337309837341309 | BCE Loss: 1.0097343921661377\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.371569633483887 | KNN Loss: 4.3382158279418945 | BCE Loss: 1.0333540439605713\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 5.3864922523498535 | KNN Loss: 4.327797889709473 | BCE Loss: 1.0586942434310913\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 5.361086845397949 | KNN Loss: 4.340327262878418 | BCE Loss: 1.0207593441009521\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 5.405022621154785 | KNN Loss: 4.379476547241211 | BCE Loss: 1.0255463123321533\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 5.3833112716674805 | KNN Loss: 4.336338996887207 | BCE Loss: 1.0469725131988525\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 5.373635292053223 | KNN Loss: 4.346402645111084 | BCE Loss: 1.0272324085235596\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 5.407071113586426 | KNN Loss: 4.343238830566406 | BCE Loss: 1.0638320446014404\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 5.395842552185059 | KNN Loss: 4.35371732711792 | BCE Loss: 1.0421249866485596\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 5.373373031616211 | KNN Loss: 4.352713584899902 | BCE Loss: 1.020659327507019\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 5.372476100921631 | KNN Loss: 4.365839958190918 | BCE Loss: 1.0066360235214233\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 5.405575752258301 | KNN Loss: 4.363893032073975 | BCE Loss: 1.0416826009750366\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 5.3763427734375 | KNN Loss: 4.355755805969238 | BCE Loss: 1.0205867290496826\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 5.362869739532471 | KNN Loss: 4.336277484893799 | BCE Loss: 1.0265923738479614\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 5.366174697875977 | KNN Loss: 4.3483171463012695 | BCE Loss: 1.0178577899932861\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 5.342973709106445 | KNN Loss: 4.342294216156006 | BCE Loss: 1.0006794929504395\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 5.361785888671875 | KNN Loss: 4.334813594818115 | BCE Loss: 1.0269720554351807\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 5.391909599304199 | KNN Loss: 4.359747409820557 | BCE Loss: 1.0321619510650635\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 5.379565238952637 | KNN Loss: 4.347097873687744 | BCE Loss: 1.0324676036834717\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 5.3621110916137695 | KNN Loss: 4.351438999176025 | BCE Loss: 1.0106720924377441\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 5.398979187011719 | KNN Loss: 4.355986595153809 | BCE Loss: 1.042992353439331\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 5.397294044494629 | KNN Loss: 4.36368989944458 | BCE Loss: 1.0336039066314697\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 5.397195816040039 | KNN Loss: 4.353446006774902 | BCE Loss: 1.0437500476837158\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 5.412981986999512 | KNN Loss: 4.357944488525391 | BCE Loss: 1.055037260055542\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 5.337569236755371 | KNN Loss: 4.347648620605469 | BCE Loss: 0.9899206757545471\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 5.388700485229492 | KNN Loss: 4.355635643005371 | BCE Loss: 1.0330647230148315\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 5.3816399574279785 | KNN Loss: 4.353059768676758 | BCE Loss: 1.0285800695419312\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 5.38250732421875 | KNN Loss: 4.3664631843566895 | BCE Loss: 1.0160439014434814\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 5.360340595245361 | KNN Loss: 4.3517255783081055 | BCE Loss: 1.0086150169372559\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 5.377203464508057 | KNN Loss: 4.341240882873535 | BCE Loss: 1.035962462425232\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 5.437252521514893 | KNN Loss: 4.400176525115967 | BCE Loss: 1.0370758771896362\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 5.3753509521484375 | KNN Loss: 4.35498571395874 | BCE Loss: 1.0203652381896973\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 5.365382194519043 | KNN Loss: 4.344306945800781 | BCE Loss: 1.0210754871368408\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 5.353001117706299 | KNN Loss: 4.33081579208374 | BCE Loss: 1.0221853256225586\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 5.35391902923584 | KNN Loss: 4.3423004150390625 | BCE Loss: 1.0116188526153564\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 5.383616924285889 | KNN Loss: 4.337894439697266 | BCE Loss: 1.0457226037979126\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 5.399165153503418 | KNN Loss: 4.352377891540527 | BCE Loss: 1.0467870235443115\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 5.392967224121094 | KNN Loss: 4.350579261779785 | BCE Loss: 1.0423877239227295\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 5.358053684234619 | KNN Loss: 4.339951515197754 | BCE Loss: 1.0181021690368652\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 5.36244010925293 | KNN Loss: 4.335650444030762 | BCE Loss: 1.026789665222168\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 5.381057262420654 | KNN Loss: 4.353306293487549 | BCE Loss: 1.027751088142395\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 5.375528335571289 | KNN Loss: 4.34108304977417 | BCE Loss: 1.0344454050064087\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 5.368191242218018 | KNN Loss: 4.345634937286377 | BCE Loss: 1.0225563049316406\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 5.407365798950195 | KNN Loss: 4.372463226318359 | BCE Loss: 1.0349026918411255\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 5.359167098999023 | KNN Loss: 4.359801292419434 | BCE Loss: 0.999366044998169\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 5.3630781173706055 | KNN Loss: 4.340334415435791 | BCE Loss: 1.0227437019348145\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 5.412073612213135 | KNN Loss: 4.375475883483887 | BCE Loss: 1.036597728729248\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 5.383190155029297 | KNN Loss: 4.355630874633789 | BCE Loss: 1.0275593996047974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 5.410998344421387 | KNN Loss: 4.408833980560303 | BCE Loss: 1.0021641254425049\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 5.330832004547119 | KNN Loss: 4.340184688568115 | BCE Loss: 0.9906473159790039\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 5.348545074462891 | KNN Loss: 4.33862829208374 | BCE Loss: 1.0099165439605713\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 5.408653736114502 | KNN Loss: 4.331836223602295 | BCE Loss: 1.0768176317214966\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 5.385564804077148 | KNN Loss: 4.343106746673584 | BCE Loss: 1.042458176612854\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 5.412370681762695 | KNN Loss: 4.3549346923828125 | BCE Loss: 1.0574359893798828\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 5.393054008483887 | KNN Loss: 4.359588146209717 | BCE Loss: 1.0334656238555908\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 5.380929946899414 | KNN Loss: 4.35763692855835 | BCE Loss: 1.0232927799224854\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 5.388210296630859 | KNN Loss: 4.378328800201416 | BCE Loss: 1.0098817348480225\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 5.3488874435424805 | KNN Loss: 4.335126876831055 | BCE Loss: 1.0137605667114258\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 5.339480400085449 | KNN Loss: 4.3194193840026855 | BCE Loss: 1.0200612545013428\n",
      "Epoch   195: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 5.401812553405762 | KNN Loss: 4.3829827308654785 | BCE Loss: 1.0188297033309937\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 5.350983619689941 | KNN Loss: 4.354377746582031 | BCE Loss: 0.9966061115264893\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 5.336233615875244 | KNN Loss: 4.319903373718262 | BCE Loss: 1.016330361366272\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 5.333239555358887 | KNN Loss: 4.333426475524902 | BCE Loss: 0.99981290102005\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 5.380008697509766 | KNN Loss: 4.348977565765381 | BCE Loss: 1.0310311317443848\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 5.434508323669434 | KNN Loss: 4.393248558044434 | BCE Loss: 1.041259765625\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 5.443492889404297 | KNN Loss: 4.347772121429443 | BCE Loss: 1.0957207679748535\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 5.40402889251709 | KNN Loss: 4.37409782409668 | BCE Loss: 1.029930830001831\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 5.340754508972168 | KNN Loss: 4.331557273864746 | BCE Loss: 1.0091969966888428\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 5.3391218185424805 | KNN Loss: 4.3182549476623535 | BCE Loss: 1.020866870880127\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 5.399399280548096 | KNN Loss: 4.367160797119141 | BCE Loss: 1.032238483428955\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 5.384285926818848 | KNN Loss: 4.365355968475342 | BCE Loss: 1.0189298391342163\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 5.344930648803711 | KNN Loss: 4.3361053466796875 | BCE Loss: 1.0088253021240234\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 5.373416423797607 | KNN Loss: 4.338349342346191 | BCE Loss: 1.035067081451416\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 5.3842597007751465 | KNN Loss: 4.334020614624023 | BCE Loss: 1.050239086151123\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 5.358308792114258 | KNN Loss: 4.346377372741699 | BCE Loss: 1.0119316577911377\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 5.400033473968506 | KNN Loss: 4.378551006317139 | BCE Loss: 1.0214823484420776\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 5.409109115600586 | KNN Loss: 4.383501052856445 | BCE Loss: 1.0256080627441406\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 5.375248908996582 | KNN Loss: 4.367722034454346 | BCE Loss: 1.0075268745422363\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 5.365993499755859 | KNN Loss: 4.344411373138428 | BCE Loss: 1.0215823650360107\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 5.381777763366699 | KNN Loss: 4.361786365509033 | BCE Loss: 1.0199915170669556\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 5.3935418128967285 | KNN Loss: 4.35921573638916 | BCE Loss: 1.0343260765075684\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 5.3802571296691895 | KNN Loss: 4.350917339324951 | BCE Loss: 1.0293397903442383\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 5.362442493438721 | KNN Loss: 4.3329267501831055 | BCE Loss: 1.0295156240463257\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 5.39110803604126 | KNN Loss: 4.353349685668945 | BCE Loss: 1.0377583503723145\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 5.37310791015625 | KNN Loss: 4.349688529968262 | BCE Loss: 1.0234193801879883\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 5.361831188201904 | KNN Loss: 4.332555770874023 | BCE Loss: 1.0292754173278809\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 5.39681339263916 | KNN Loss: 4.35931921005249 | BCE Loss: 1.03749418258667\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 5.337206840515137 | KNN Loss: 4.338945388793945 | BCE Loss: 0.9982616305351257\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 5.3601813316345215 | KNN Loss: 4.319378852844238 | BCE Loss: 1.0408023595809937\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 5.348602771759033 | KNN Loss: 4.343796730041504 | BCE Loss: 1.0048059225082397\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 5.369548797607422 | KNN Loss: 4.3463568687438965 | BCE Loss: 1.0231916904449463\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 5.4244914054870605 | KNN Loss: 4.367412567138672 | BCE Loss: 1.0570788383483887\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 5.3330078125 | KNN Loss: 4.347066402435303 | BCE Loss: 0.985941469669342\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 5.344273567199707 | KNN Loss: 4.318044662475586 | BCE Loss: 1.0262291431427002\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 5.390287399291992 | KNN Loss: 4.362997055053711 | BCE Loss: 1.0272905826568604\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 5.420142650604248 | KNN Loss: 4.410161972045898 | BCE Loss: 1.0099806785583496\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 5.40364408493042 | KNN Loss: 4.3796067237854 | BCE Loss: 1.0240373611450195\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 5.366245269775391 | KNN Loss: 4.340523719787598 | BCE Loss: 1.025721549987793\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 5.383305072784424 | KNN Loss: 4.355091571807861 | BCE Loss: 1.0282135009765625\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 5.362447738647461 | KNN Loss: 4.357873439788818 | BCE Loss: 1.0045740604400635\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 5.396295547485352 | KNN Loss: 4.333767890930176 | BCE Loss: 1.0625274181365967\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 5.371121406555176 | KNN Loss: 4.329885005950928 | BCE Loss: 1.0412366390228271\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 5.397709369659424 | KNN Loss: 4.345092296600342 | BCE Loss: 1.0526171922683716\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 5.3589372634887695 | KNN Loss: 4.328799724578857 | BCE Loss: 1.0301377773284912\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 5.379796028137207 | KNN Loss: 4.366878509521484 | BCE Loss: 1.012917399406433\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 5.401545524597168 | KNN Loss: 4.330892086029053 | BCE Loss: 1.0706532001495361\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 5.357061386108398 | KNN Loss: 4.3404388427734375 | BCE Loss: 1.016622543334961\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 5.369465351104736 | KNN Loss: 4.356847763061523 | BCE Loss: 1.012617588043213\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 5.448678970336914 | KNN Loss: 4.362360000610352 | BCE Loss: 1.086318850517273\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 5.3964128494262695 | KNN Loss: 4.379882335662842 | BCE Loss: 1.0165306329727173\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 5.382485389709473 | KNN Loss: 4.336369514465332 | BCE Loss: 1.0461159944534302\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 5.442605018615723 | KNN Loss: 4.373452663421631 | BCE Loss: 1.069152593612671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 5.389151573181152 | KNN Loss: 4.366763591766357 | BCE Loss: 1.022388219833374\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 5.376889228820801 | KNN Loss: 4.335150718688965 | BCE Loss: 1.041738510131836\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 5.356049537658691 | KNN Loss: 4.334020614624023 | BCE Loss: 1.0220286846160889\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 5.369775772094727 | KNN Loss: 4.345573902130127 | BCE Loss: 1.0242018699645996\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 5.367750644683838 | KNN Loss: 4.340217113494873 | BCE Loss: 1.0275336503982544\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 5.376115322113037 | KNN Loss: 4.358524322509766 | BCE Loss: 1.0175909996032715\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 5.387426376342773 | KNN Loss: 4.331012725830078 | BCE Loss: 1.0564136505126953\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 5.341602325439453 | KNN Loss: 4.36120080947876 | BCE Loss: 0.9804016351699829\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 5.362005710601807 | KNN Loss: 4.357234477996826 | BCE Loss: 1.0047712326049805\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 5.364925384521484 | KNN Loss: 4.338114261627197 | BCE Loss: 1.0268113613128662\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 5.384139060974121 | KNN Loss: 4.355051517486572 | BCE Loss: 1.0290875434875488\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 5.417324066162109 | KNN Loss: 4.383786201477051 | BCE Loss: 1.0335376262664795\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 5.3962249755859375 | KNN Loss: 4.387539863586426 | BCE Loss: 1.0086853504180908\n",
      "Epoch   206: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 5.448663234710693 | KNN Loss: 4.3918256759643555 | BCE Loss: 1.0568374395370483\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 5.3653130531311035 | KNN Loss: 4.3529052734375 | BCE Loss: 1.0124077796936035\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 5.354854106903076 | KNN Loss: 4.329442501068115 | BCE Loss: 1.025411605834961\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 5.4214653968811035 | KNN Loss: 4.379922866821289 | BCE Loss: 1.0415425300598145\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 5.374372482299805 | KNN Loss: 4.33738374710083 | BCE Loss: 1.0369887351989746\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 5.364203929901123 | KNN Loss: 4.320791244506836 | BCE Loss: 1.043412685394287\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 5.372785568237305 | KNN Loss: 4.351499080657959 | BCE Loss: 1.0212866067886353\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 5.338422775268555 | KNN Loss: 4.336544513702393 | BCE Loss: 1.0018783807754517\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 5.340305328369141 | KNN Loss: 4.327252388000488 | BCE Loss: 1.0130531787872314\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 5.390246391296387 | KNN Loss: 4.355679035186768 | BCE Loss: 1.03456711769104\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 5.370630264282227 | KNN Loss: 4.3485918045043945 | BCE Loss: 1.0220385789871216\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 5.376694202423096 | KNN Loss: 4.350190162658691 | BCE Loss: 1.0265040397644043\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 5.368349552154541 | KNN Loss: 4.345359802246094 | BCE Loss: 1.0229897499084473\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 5.380679607391357 | KNN Loss: 4.333799362182617 | BCE Loss: 1.0468802452087402\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 5.398270130157471 | KNN Loss: 4.3576507568359375 | BCE Loss: 1.0406193733215332\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 5.373903274536133 | KNN Loss: 4.355655193328857 | BCE Loss: 1.0182480812072754\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 5.383423805236816 | KNN Loss: 4.362061977386475 | BCE Loss: 1.0213618278503418\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 5.361322402954102 | KNN Loss: 4.342193126678467 | BCE Loss: 1.0191293954849243\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 5.413828372955322 | KNN Loss: 4.361477851867676 | BCE Loss: 1.052350401878357\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 5.366607666015625 | KNN Loss: 4.360235691070557 | BCE Loss: 1.0063719749450684\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 5.355652809143066 | KNN Loss: 4.338336944580078 | BCE Loss: 1.0173161029815674\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 5.346794128417969 | KNN Loss: 4.336913585662842 | BCE Loss: 1.0098804235458374\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 5.350075721740723 | KNN Loss: 4.318228244781494 | BCE Loss: 1.0318477153778076\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 5.358670234680176 | KNN Loss: 4.347458362579346 | BCE Loss: 1.01121187210083\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 5.3777313232421875 | KNN Loss: 4.364992141723633 | BCE Loss: 1.0127390623092651\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 5.366241931915283 | KNN Loss: 4.3287882804870605 | BCE Loss: 1.037453532218933\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 5.402823448181152 | KNN Loss: 4.359655857086182 | BCE Loss: 1.0431673526763916\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 5.394482135772705 | KNN Loss: 4.366665840148926 | BCE Loss: 1.0278161764144897\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 5.364880561828613 | KNN Loss: 4.329443454742432 | BCE Loss: 1.0354368686676025\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 5.3533124923706055 | KNN Loss: 4.329971790313721 | BCE Loss: 1.0233408212661743\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 5.384337902069092 | KNN Loss: 4.341102600097656 | BCE Loss: 1.043235182762146\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 5.359722137451172 | KNN Loss: 4.323895454406738 | BCE Loss: 1.0358266830444336\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 5.3937249183654785 | KNN Loss: 4.339398384094238 | BCE Loss: 1.0543265342712402\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 5.391890048980713 | KNN Loss: 4.373495101928711 | BCE Loss: 1.018394947052002\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 5.358235836029053 | KNN Loss: 4.340717792510986 | BCE Loss: 1.0175179243087769\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 5.377750873565674 | KNN Loss: 4.338248252868652 | BCE Loss: 1.0395026206970215\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 5.403456687927246 | KNN Loss: 4.337217330932617 | BCE Loss: 1.066239595413208\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 5.404463768005371 | KNN Loss: 4.332999229431152 | BCE Loss: 1.0714647769927979\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 5.355762004852295 | KNN Loss: 4.328749179840088 | BCE Loss: 1.0270127058029175\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 5.333018779754639 | KNN Loss: 4.333179950714111 | BCE Loss: 0.9998389482498169\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 5.3548102378845215 | KNN Loss: 4.340759754180908 | BCE Loss: 1.0140504837036133\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 5.378808975219727 | KNN Loss: 4.354211807250977 | BCE Loss: 1.0245972871780396\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 5.415459632873535 | KNN Loss: 4.374840259552002 | BCE Loss: 1.0406193733215332\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 5.4169392585754395 | KNN Loss: 4.397397994995117 | BCE Loss: 1.0195411443710327\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 5.386679649353027 | KNN Loss: 4.370203018188477 | BCE Loss: 1.0164763927459717\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 5.350335121154785 | KNN Loss: 4.3546905517578125 | BCE Loss: 0.9956448078155518\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 5.367093086242676 | KNN Loss: 4.346344470977783 | BCE Loss: 1.0207486152648926\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 5.3675432205200195 | KNN Loss: 4.343985557556152 | BCE Loss: 1.0235575437545776\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 5.359924793243408 | KNN Loss: 4.36250114440918 | BCE Loss: 0.9974234700202942\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 5.3656487464904785 | KNN Loss: 4.355945587158203 | BCE Loss: 1.0097030401229858\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 5.375678062438965 | KNN Loss: 4.341466426849365 | BCE Loss: 1.03421151638031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 5.399343967437744 | KNN Loss: 4.348114490509033 | BCE Loss: 1.0512295961380005\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 5.373453140258789 | KNN Loss: 4.3209147453308105 | BCE Loss: 1.0525381565093994\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 5.390584468841553 | KNN Loss: 4.36714506149292 | BCE Loss: 1.0234395265579224\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 5.399969577789307 | KNN Loss: 4.360569477081299 | BCE Loss: 1.0394001007080078\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 5.3771891593933105 | KNN Loss: 4.330700874328613 | BCE Loss: 1.0464882850646973\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 5.351072311401367 | KNN Loss: 4.34326171875 | BCE Loss: 1.0078104734420776\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 5.37966775894165 | KNN Loss: 4.346429347991943 | BCE Loss: 1.0332385301589966\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 5.352939128875732 | KNN Loss: 4.347033500671387 | BCE Loss: 1.0059056282043457\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 5.374629020690918 | KNN Loss: 4.3452887535095215 | BCE Loss: 1.029340147972107\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 5.364311695098877 | KNN Loss: 4.3407769203186035 | BCE Loss: 1.0235347747802734\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 5.37934684753418 | KNN Loss: 4.345091342926025 | BCE Loss: 1.0342557430267334\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 5.379244804382324 | KNN Loss: 4.348098278045654 | BCE Loss: 1.031146764755249\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 5.379918098449707 | KNN Loss: 4.347731113433838 | BCE Loss: 1.0321869850158691\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 5.384859561920166 | KNN Loss: 4.379458427429199 | BCE Loss: 1.0054011344909668\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 5.370923042297363 | KNN Loss: 4.338866233825684 | BCE Loss: 1.0320565700531006\n",
      "Epoch   217: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 5.344608306884766 | KNN Loss: 4.333250045776367 | BCE Loss: 1.011358380317688\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 5.381798267364502 | KNN Loss: 4.366708755493164 | BCE Loss: 1.0150896310806274\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 5.388289928436279 | KNN Loss: 4.3503546714782715 | BCE Loss: 1.0379353761672974\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 5.379631042480469 | KNN Loss: 4.3499298095703125 | BCE Loss: 1.0297009944915771\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 5.386902809143066 | KNN Loss: 4.370206832885742 | BCE Loss: 1.0166962146759033\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 5.407229423522949 | KNN Loss: 4.358039855957031 | BCE Loss: 1.049189567565918\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 5.400871276855469 | KNN Loss: 4.371248722076416 | BCE Loss: 1.0296223163604736\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 5.365492343902588 | KNN Loss: 4.32980489730835 | BCE Loss: 1.0356873273849487\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 5.373170852661133 | KNN Loss: 4.338003158569336 | BCE Loss: 1.0351678133010864\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 5.379462242126465 | KNN Loss: 4.330039978027344 | BCE Loss: 1.0494225025177002\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 5.3451714515686035 | KNN Loss: 4.336081504821777 | BCE Loss: 1.0090900659561157\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 5.374484539031982 | KNN Loss: 4.33323335647583 | BCE Loss: 1.0412511825561523\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 5.360454559326172 | KNN Loss: 4.352701663970947 | BCE Loss: 1.007752776145935\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 5.3577189445495605 | KNN Loss: 4.338564395904541 | BCE Loss: 1.0191545486450195\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 5.410492420196533 | KNN Loss: 4.384561538696289 | BCE Loss: 1.0259308815002441\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 5.364490985870361 | KNN Loss: 4.327328205108643 | BCE Loss: 1.0371627807617188\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 5.380117893218994 | KNN Loss: 4.3374528884887695 | BCE Loss: 1.0426651239395142\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 5.363833904266357 | KNN Loss: 4.348420143127441 | BCE Loss: 1.015413761138916\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 5.35536003112793 | KNN Loss: 4.336136817932129 | BCE Loss: 1.0192234516143799\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 5.428043365478516 | KNN Loss: 4.371269702911377 | BCE Loss: 1.0567736625671387\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 5.440032958984375 | KNN Loss: 4.400117874145508 | BCE Loss: 1.0399149656295776\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 5.365309715270996 | KNN Loss: 4.336390495300293 | BCE Loss: 1.0289194583892822\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 5.358134746551514 | KNN Loss: 4.345026969909668 | BCE Loss: 1.0131077766418457\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 5.383354663848877 | KNN Loss: 4.348385334014893 | BCE Loss: 1.0349692106246948\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 5.35887336730957 | KNN Loss: 4.34072732925415 | BCE Loss: 1.018146276473999\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 5.381586074829102 | KNN Loss: 4.343207836151123 | BCE Loss: 1.0383782386779785\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 5.363077640533447 | KNN Loss: 4.336780548095703 | BCE Loss: 1.0262972116470337\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 5.357229709625244 | KNN Loss: 4.322762489318848 | BCE Loss: 1.034467339515686\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 5.399774551391602 | KNN Loss: 4.371401309967041 | BCE Loss: 1.0283732414245605\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 5.353001594543457 | KNN Loss: 4.358139514923096 | BCE Loss: 0.9948623180389404\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 5.363147735595703 | KNN Loss: 4.338081359863281 | BCE Loss: 1.0250661373138428\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 5.408297538757324 | KNN Loss: 4.360617160797119 | BCE Loss: 1.0476806163787842\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 5.35194206237793 | KNN Loss: 4.326425075531006 | BCE Loss: 1.025517225265503\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 5.353890419006348 | KNN Loss: 4.332413673400879 | BCE Loss: 1.0214767456054688\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 5.359694957733154 | KNN Loss: 4.351518630981445 | BCE Loss: 1.008176326751709\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 5.411970615386963 | KNN Loss: 4.365039825439453 | BCE Loss: 1.0469309091567993\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 5.354276180267334 | KNN Loss: 4.331964015960693 | BCE Loss: 1.0223121643066406\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 5.365278244018555 | KNN Loss: 4.3547797203063965 | BCE Loss: 1.010498285293579\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 5.380836486816406 | KNN Loss: 4.346863269805908 | BCE Loss: 1.033973217010498\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 5.380704879760742 | KNN Loss: 4.346961975097656 | BCE Loss: 1.033743143081665\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 5.379879474639893 | KNN Loss: 4.360287189483643 | BCE Loss: 1.0195924043655396\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 5.3422346115112305 | KNN Loss: 4.327042102813721 | BCE Loss: 1.0151925086975098\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 5.4122772216796875 | KNN Loss: 4.3566508293151855 | BCE Loss: 1.055626392364502\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 5.386096954345703 | KNN Loss: 4.350506782531738 | BCE Loss: 1.0355899333953857\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 5.383294105529785 | KNN Loss: 4.391345500946045 | BCE Loss: 0.9919483661651611\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 5.347171783447266 | KNN Loss: 4.346842288970947 | BCE Loss: 1.0003293752670288\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 5.382286548614502 | KNN Loss: 4.355325222015381 | BCE Loss: 1.0269612073898315\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 5.376743793487549 | KNN Loss: 4.3740034103393555 | BCE Loss: 1.0027403831481934\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 5.391961574554443 | KNN Loss: 4.3575029373168945 | BCE Loss: 1.0344585180282593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 5.384949207305908 | KNN Loss: 4.346677303314209 | BCE Loss: 1.0382719039916992\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 5.3904829025268555 | KNN Loss: 4.347760200500488 | BCE Loss: 1.0427225828170776\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 5.367539405822754 | KNN Loss: 4.363221645355225 | BCE Loss: 1.0043175220489502\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 5.375859260559082 | KNN Loss: 4.355532646179199 | BCE Loss: 1.020326852798462\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 5.396213054656982 | KNN Loss: 4.337543964385986 | BCE Loss: 1.0586692094802856\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 5.390443801879883 | KNN Loss: 4.35444450378418 | BCE Loss: 1.0359992980957031\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 5.39876651763916 | KNN Loss: 4.360323905944824 | BCE Loss: 1.0384423732757568\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 5.390728950500488 | KNN Loss: 4.369758605957031 | BCE Loss: 1.020970344543457\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 5.401368141174316 | KNN Loss: 4.36255407333374 | BCE Loss: 1.0388143062591553\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 5.379578590393066 | KNN Loss: 4.358194828033447 | BCE Loss: 1.0213837623596191\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 5.374724388122559 | KNN Loss: 4.346522331237793 | BCE Loss: 1.0282018184661865\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 5.389406681060791 | KNN Loss: 4.332822322845459 | BCE Loss: 1.0565844774246216\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 5.3881354331970215 | KNN Loss: 4.3371968269348145 | BCE Loss: 1.050938606262207\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 5.376534938812256 | KNN Loss: 4.362902641296387 | BCE Loss: 1.0136322975158691\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 5.391890048980713 | KNN Loss: 4.353630542755127 | BCE Loss: 1.0382596254348755\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 5.369388580322266 | KNN Loss: 4.327692985534668 | BCE Loss: 1.0416955947875977\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 5.361541748046875 | KNN Loss: 4.341472625732422 | BCE Loss: 1.0200691223144531\n",
      "Epoch   228: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 5.379360198974609 | KNN Loss: 4.344044208526611 | BCE Loss: 1.0353162288665771\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 5.421265602111816 | KNN Loss: 4.359938621520996 | BCE Loss: 1.0613269805908203\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 5.377695083618164 | KNN Loss: 4.36598539352417 | BCE Loss: 1.0117099285125732\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 5.4106926918029785 | KNN Loss: 4.395688533782959 | BCE Loss: 1.015004277229309\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 5.403852462768555 | KNN Loss: 4.365225791931152 | BCE Loss: 1.0386265516281128\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 5.394523620605469 | KNN Loss: 4.374213695526123 | BCE Loss: 1.0203099250793457\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 5.354458808898926 | KNN Loss: 4.338943958282471 | BCE Loss: 1.015514850616455\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 5.405725479125977 | KNN Loss: 4.345958232879639 | BCE Loss: 1.059767246246338\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 5.33426570892334 | KNN Loss: 4.321710586547852 | BCE Loss: 1.0125553607940674\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 5.349941253662109 | KNN Loss: 4.3275556564331055 | BCE Loss: 1.0223854780197144\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 5.403138160705566 | KNN Loss: 4.3626203536987305 | BCE Loss: 1.040517807006836\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 5.438156604766846 | KNN Loss: 4.391161918640137 | BCE Loss: 1.046994686126709\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 5.35678243637085 | KNN Loss: 4.347640037536621 | BCE Loss: 1.0091423988342285\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 5.388066291809082 | KNN Loss: 4.362964630126953 | BCE Loss: 1.025101900100708\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 5.407071113586426 | KNN Loss: 4.367310523986816 | BCE Loss: 1.0397605895996094\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 5.361635208129883 | KNN Loss: 4.350343704223633 | BCE Loss: 1.0112913846969604\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 5.3577470779418945 | KNN Loss: 4.337250709533691 | BCE Loss: 1.0204966068267822\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 5.3804707527160645 | KNN Loss: 4.369988918304443 | BCE Loss: 1.0104819536209106\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 5.3408732414245605 | KNN Loss: 4.333567142486572 | BCE Loss: 1.0073062181472778\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 5.3559489250183105 | KNN Loss: 4.346180438995361 | BCE Loss: 1.0097684860229492\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 5.381054878234863 | KNN Loss: 4.3437933921813965 | BCE Loss: 1.0372612476348877\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 5.386176109313965 | KNN Loss: 4.363424301147461 | BCE Loss: 1.0227515697479248\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 5.340189456939697 | KNN Loss: 4.327244281768799 | BCE Loss: 1.0129450559616089\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 5.337634086608887 | KNN Loss: 4.322689533233643 | BCE Loss: 1.0149444341659546\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 5.343383312225342 | KNN Loss: 4.322466850280762 | BCE Loss: 1.0209163427352905\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 5.368782043457031 | KNN Loss: 4.336126327514648 | BCE Loss: 1.0326555967330933\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 5.381091117858887 | KNN Loss: 4.337526798248291 | BCE Loss: 1.0435643196105957\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 5.357177734375 | KNN Loss: 4.341266632080078 | BCE Loss: 1.0159108638763428\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 5.381927013397217 | KNN Loss: 4.338693618774414 | BCE Loss: 1.0432332754135132\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 5.351901054382324 | KNN Loss: 4.326218128204346 | BCE Loss: 1.0256829261779785\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 5.3877668380737305 | KNN Loss: 4.368106365203857 | BCE Loss: 1.0196603536605835\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 5.398664474487305 | KNN Loss: 4.3487067222595215 | BCE Loss: 1.0499577522277832\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 5.390522003173828 | KNN Loss: 4.339506149291992 | BCE Loss: 1.0510156154632568\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 5.382266998291016 | KNN Loss: 4.3414435386657715 | BCE Loss: 1.040823221206665\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 5.351587295532227 | KNN Loss: 4.3358235359191895 | BCE Loss: 1.015763521194458\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 5.432299613952637 | KNN Loss: 4.391258716583252 | BCE Loss: 1.0410406589508057\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 5.37161111831665 | KNN Loss: 4.36580228805542 | BCE Loss: 1.0058088302612305\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 5.381357192993164 | KNN Loss: 4.3571295738220215 | BCE Loss: 1.0242278575897217\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 5.379795551300049 | KNN Loss: 4.351866245269775 | BCE Loss: 1.0279291868209839\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 5.4033732414245605 | KNN Loss: 4.355608940124512 | BCE Loss: 1.0477643013000488\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 5.339102745056152 | KNN Loss: 4.327033996582031 | BCE Loss: 1.012068510055542\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 5.419955253601074 | KNN Loss: 4.373562335968018 | BCE Loss: 1.046392798423767\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 5.344189167022705 | KNN Loss: 4.354729652404785 | BCE Loss: 0.9894595742225647\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 5.382138252258301 | KNN Loss: 4.342441558837891 | BCE Loss: 1.0396966934204102\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 5.387408256530762 | KNN Loss: 4.362766265869141 | BCE Loss: 1.024641990661621\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 5.343478202819824 | KNN Loss: 4.339481353759766 | BCE Loss: 1.003996729850769\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 5.403098106384277 | KNN Loss: 4.369311809539795 | BCE Loss: 1.0337860584259033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 5.412865161895752 | KNN Loss: 4.3706841468811035 | BCE Loss: 1.0421810150146484\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 5.358555793762207 | KNN Loss: 4.346323490142822 | BCE Loss: 1.0122325420379639\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 5.370770454406738 | KNN Loss: 4.334403038024902 | BCE Loss: 1.0363671779632568\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 5.366395473480225 | KNN Loss: 4.34962797164917 | BCE Loss: 1.0167675018310547\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 5.336621284484863 | KNN Loss: 4.347736835479736 | BCE Loss: 0.9888846278190613\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 5.32765007019043 | KNN Loss: 4.332512378692627 | BCE Loss: 0.995137631893158\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 5.452523231506348 | KNN Loss: 4.397891998291016 | BCE Loss: 1.054631233215332\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 5.383674621582031 | KNN Loss: 4.338926315307617 | BCE Loss: 1.044748306274414\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 5.361353874206543 | KNN Loss: 4.351804256439209 | BCE Loss: 1.009549856185913\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 5.366458415985107 | KNN Loss: 4.363364219665527 | BCE Loss: 1.00309419631958\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 5.398655891418457 | KNN Loss: 4.354203224182129 | BCE Loss: 1.0444529056549072\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 5.409468173980713 | KNN Loss: 4.386882781982422 | BCE Loss: 1.022585391998291\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 5.385349273681641 | KNN Loss: 4.345828533172607 | BCE Loss: 1.0395206212997437\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 5.421527862548828 | KNN Loss: 4.341760635375977 | BCE Loss: 1.0797672271728516\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 5.3851470947265625 | KNN Loss: 4.3601861000061035 | BCE Loss: 1.024961233139038\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 5.362086296081543 | KNN Loss: 4.332210063934326 | BCE Loss: 1.0298761129379272\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 5.398333549499512 | KNN Loss: 4.368436336517334 | BCE Loss: 1.0298970937728882\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 5.345254421234131 | KNN Loss: 4.341907501220703 | BCE Loss: 1.0033469200134277\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 5.34316349029541 | KNN Loss: 4.328085422515869 | BCE Loss: 1.015077829360962\n",
      "Epoch   239: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 5.351401329040527 | KNN Loss: 4.337812900543213 | BCE Loss: 1.0135886669158936\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 5.373937129974365 | KNN Loss: 4.351550579071045 | BCE Loss: 1.0223866701126099\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 5.355712890625 | KNN Loss: 4.350968837738037 | BCE Loss: 1.0047441720962524\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 5.387783050537109 | KNN Loss: 4.350117206573486 | BCE Loss: 1.037665843963623\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 5.3605732917785645 | KNN Loss: 4.341844081878662 | BCE Loss: 1.0187292098999023\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 5.357188701629639 | KNN Loss: 4.3259406089782715 | BCE Loss: 1.0312480926513672\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 5.429826736450195 | KNN Loss: 4.4042067527771 | BCE Loss: 1.0256201028823853\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 5.421719551086426 | KNN Loss: 4.378978252410889 | BCE Loss: 1.042741060256958\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 5.38819694519043 | KNN Loss: 4.364438533782959 | BCE Loss: 1.0237586498260498\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 5.355058670043945 | KNN Loss: 4.331445217132568 | BCE Loss: 1.023613452911377\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 5.390071868896484 | KNN Loss: 4.348452091217041 | BCE Loss: 1.0416195392608643\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 5.364480972290039 | KNN Loss: 4.35188627243042 | BCE Loss: 1.0125949382781982\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 5.384589195251465 | KNN Loss: 4.332418441772461 | BCE Loss: 1.052170753479004\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 5.3604936599731445 | KNN Loss: 4.334594249725342 | BCE Loss: 1.0258994102478027\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 5.348104953765869 | KNN Loss: 4.337238788604736 | BCE Loss: 1.0108662843704224\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 5.40431022644043 | KNN Loss: 4.361496448516846 | BCE Loss: 1.0428135395050049\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 5.4195661544799805 | KNN Loss: 4.380348205566406 | BCE Loss: 1.0392177104949951\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 5.429460525512695 | KNN Loss: 4.3969526290893555 | BCE Loss: 1.0325078964233398\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 5.369933128356934 | KNN Loss: 4.344304084777832 | BCE Loss: 1.025628924369812\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 5.360562324523926 | KNN Loss: 4.339047431945801 | BCE Loss: 1.021514654159546\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 5.374757766723633 | KNN Loss: 4.353518009185791 | BCE Loss: 1.0212395191192627\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 5.350790023803711 | KNN Loss: 4.33660888671875 | BCE Loss: 1.0141808986663818\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 5.396109580993652 | KNN Loss: 4.334156513214111 | BCE Loss: 1.061952829360962\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 5.320899963378906 | KNN Loss: 4.324001789093018 | BCE Loss: 0.9968979358673096\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 5.356507301330566 | KNN Loss: 4.354035377502441 | BCE Loss: 1.002471685409546\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 5.400766372680664 | KNN Loss: 4.33872652053833 | BCE Loss: 1.062039852142334\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 5.405339241027832 | KNN Loss: 4.365321159362793 | BCE Loss: 1.04001784324646\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 5.3928375244140625 | KNN Loss: 4.351729869842529 | BCE Loss: 1.0411078929901123\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 5.376349925994873 | KNN Loss: 4.354311943054199 | BCE Loss: 1.0220378637313843\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 5.334415435791016 | KNN Loss: 4.340322494506836 | BCE Loss: 0.9940927624702454\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 5.373344898223877 | KNN Loss: 4.354217052459717 | BCE Loss: 1.0191278457641602\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 5.364418029785156 | KNN Loss: 4.335947513580322 | BCE Loss: 1.028470754623413\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 5.352118492126465 | KNN Loss: 4.318881034851074 | BCE Loss: 1.0332376956939697\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 5.364930629730225 | KNN Loss: 4.340860843658447 | BCE Loss: 1.0240697860717773\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 5.375771522521973 | KNN Loss: 4.36998176574707 | BCE Loss: 1.0057899951934814\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 5.355310916900635 | KNN Loss: 4.336036682128906 | BCE Loss: 1.019274115562439\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 5.365049362182617 | KNN Loss: 4.351343631744385 | BCE Loss: 1.0137054920196533\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 5.388603687286377 | KNN Loss: 4.374947547912598 | BCE Loss: 1.0136561393737793\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 5.38666296005249 | KNN Loss: 4.348974704742432 | BCE Loss: 1.0376883745193481\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 5.373417854309082 | KNN Loss: 4.355230331420898 | BCE Loss: 1.0181877613067627\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 5.3763041496276855 | KNN Loss: 4.37654972076416 | BCE Loss: 0.9997544884681702\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 5.395991325378418 | KNN Loss: 4.3510026931762695 | BCE Loss: 1.0449883937835693\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 5.4607319831848145 | KNN Loss: 4.435767650604248 | BCE Loss: 1.024964451789856\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 5.3602495193481445 | KNN Loss: 4.330844402313232 | BCE Loss: 1.029404878616333\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 5.334840774536133 | KNN Loss: 4.335932731628418 | BCE Loss: 0.9989081621170044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 5.354574680328369 | KNN Loss: 4.335957050323486 | BCE Loss: 1.0186176300048828\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 5.380064964294434 | KNN Loss: 4.35535192489624 | BCE Loss: 1.0247132778167725\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 5.384084701538086 | KNN Loss: 4.3588972091674805 | BCE Loss: 1.0251874923706055\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 5.325982093811035 | KNN Loss: 4.323070049285889 | BCE Loss: 1.0029120445251465\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 5.384626388549805 | KNN Loss: 4.367661476135254 | BCE Loss: 1.0169649124145508\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 5.344942569732666 | KNN Loss: 4.336441516876221 | BCE Loss: 1.0085011720657349\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 5.351956367492676 | KNN Loss: 4.338168621063232 | BCE Loss: 1.0137879848480225\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 5.340036392211914 | KNN Loss: 4.320937633514404 | BCE Loss: 1.0190985202789307\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 5.389871120452881 | KNN Loss: 4.358115196228027 | BCE Loss: 1.031756043434143\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 5.376910209655762 | KNN Loss: 4.329874515533447 | BCE Loss: 1.0470354557037354\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 5.373561859130859 | KNN Loss: 4.340413570404053 | BCE Loss: 1.0331484079360962\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 5.375363349914551 | KNN Loss: 4.351389408111572 | BCE Loss: 1.0239737033843994\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 5.345914840698242 | KNN Loss: 4.336980819702148 | BCE Loss: 1.0089340209960938\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 5.409974575042725 | KNN Loss: 4.385188102722168 | BCE Loss: 1.0247864723205566\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 5.386547088623047 | KNN Loss: 4.341263294219971 | BCE Loss: 1.0452840328216553\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 5.395638465881348 | KNN Loss: 4.342896461486816 | BCE Loss: 1.0527420043945312\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 5.394346237182617 | KNN Loss: 4.349488258361816 | BCE Loss: 1.0448582172393799\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 5.412744998931885 | KNN Loss: 4.392975807189941 | BCE Loss: 1.0197691917419434\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 5.364890098571777 | KNN Loss: 4.343532085418701 | BCE Loss: 1.0213582515716553\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 5.358611106872559 | KNN Loss: 4.321843147277832 | BCE Loss: 1.0367681980133057\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 5.367730617523193 | KNN Loss: 4.350712299346924 | BCE Loss: 1.017018437385559\n",
      "Epoch   250: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 5.388457298278809 | KNN Loss: 4.380395889282227 | BCE Loss: 1.008061170578003\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 5.392667770385742 | KNN Loss: 4.348438739776611 | BCE Loss: 1.0442287921905518\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 5.38041353225708 | KNN Loss: 4.348653316497803 | BCE Loss: 1.0317602157592773\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 5.372398853302002 | KNN Loss: 4.337794303894043 | BCE Loss: 1.0346044301986694\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 5.376996040344238 | KNN Loss: 4.336854934692383 | BCE Loss: 1.0401413440704346\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 5.434575080871582 | KNN Loss: 4.418970108032227 | BCE Loss: 1.0156049728393555\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 5.351886749267578 | KNN Loss: 4.351232528686523 | BCE Loss: 1.0006541013717651\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 5.359495639801025 | KNN Loss: 4.359952926635742 | BCE Loss: 0.9995425939559937\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 5.40864896774292 | KNN Loss: 4.364893436431885 | BCE Loss: 1.0437555313110352\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 5.3989763259887695 | KNN Loss: 4.373489856719971 | BCE Loss: 1.0254862308502197\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 5.357174873352051 | KNN Loss: 4.353636264801025 | BCE Loss: 1.0035383701324463\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 5.352470397949219 | KNN Loss: 4.346179008483887 | BCE Loss: 1.006291151046753\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 5.372194290161133 | KNN Loss: 4.363995552062988 | BCE Loss: 1.0081984996795654\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 5.413471221923828 | KNN Loss: 4.363476753234863 | BCE Loss: 1.049994707107544\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 5.3777174949646 | KNN Loss: 4.33003044128418 | BCE Loss: 1.04768705368042\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 5.383246421813965 | KNN Loss: 4.357877254486084 | BCE Loss: 1.0253689289093018\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 5.350635528564453 | KNN Loss: 4.3387579917907715 | BCE Loss: 1.0118775367736816\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 5.3918986320495605 | KNN Loss: 4.359010696411133 | BCE Loss: 1.0328879356384277\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 5.388668060302734 | KNN Loss: 4.358249664306641 | BCE Loss: 1.0304186344146729\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 5.382668972015381 | KNN Loss: 4.329535961151123 | BCE Loss: 1.0531328916549683\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 5.408023834228516 | KNN Loss: 4.36583948135376 | BCE Loss: 1.0421843528747559\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 5.438729763031006 | KNN Loss: 4.398448467254639 | BCE Loss: 1.0402812957763672\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 5.395004749298096 | KNN Loss: 4.365999221801758 | BCE Loss: 1.029005527496338\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 5.341660499572754 | KNN Loss: 4.337821006774902 | BCE Loss: 1.0038397312164307\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 5.372091293334961 | KNN Loss: 4.325851917266846 | BCE Loss: 1.0462391376495361\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 5.360034942626953 | KNN Loss: 4.340987205505371 | BCE Loss: 1.019047737121582\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 5.405807018280029 | KNN Loss: 4.354415416717529 | BCE Loss: 1.0513916015625\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 5.407176494598389 | KNN Loss: 4.35326623916626 | BCE Loss: 1.0539103746414185\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 5.427801132202148 | KNN Loss: 4.402699947357178 | BCE Loss: 1.0251014232635498\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 5.354283809661865 | KNN Loss: 4.322116374969482 | BCE Loss: 1.0321673154830933\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 5.384258270263672 | KNN Loss: 4.342652797698975 | BCE Loss: 1.0416057109832764\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 5.403390884399414 | KNN Loss: 4.382700443267822 | BCE Loss: 1.0206904411315918\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 5.392567157745361 | KNN Loss: 4.34926176071167 | BCE Loss: 1.043305516242981\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 5.372579097747803 | KNN Loss: 4.335196495056152 | BCE Loss: 1.03738272190094\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 5.40437126159668 | KNN Loss: 4.3496527671813965 | BCE Loss: 1.054718255996704\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 5.344501495361328 | KNN Loss: 4.322174072265625 | BCE Loss: 1.0223276615142822\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 5.371421813964844 | KNN Loss: 4.3375983238220215 | BCE Loss: 1.0338232517242432\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 5.436304092407227 | KNN Loss: 4.380160331726074 | BCE Loss: 1.0561435222625732\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 5.367660999298096 | KNN Loss: 4.347444534301758 | BCE Loss: 1.020216464996338\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 5.384842395782471 | KNN Loss: 4.381791591644287 | BCE Loss: 1.0030508041381836\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 5.39415979385376 | KNN Loss: 4.348160743713379 | BCE Loss: 1.0459991693496704\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 5.414932727813721 | KNN Loss: 4.376523494720459 | BCE Loss: 1.0384091138839722\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 5.376034736633301 | KNN Loss: 4.333982467651367 | BCE Loss: 1.0420520305633545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 5.336450099945068 | KNN Loss: 4.33142614364624 | BCE Loss: 1.0050240755081177\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 5.3653950691223145 | KNN Loss: 4.321305274963379 | BCE Loss: 1.0440897941589355\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 5.353693008422852 | KNN Loss: 4.351749897003174 | BCE Loss: 1.0019428730010986\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 5.33466100692749 | KNN Loss: 4.326825141906738 | BCE Loss: 1.007835865020752\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 5.346556663513184 | KNN Loss: 4.342149257659912 | BCE Loss: 1.0044071674346924\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 5.420558452606201 | KNN Loss: 4.380152702331543 | BCE Loss: 1.0404057502746582\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 5.421347618103027 | KNN Loss: 4.365427494049072 | BCE Loss: 1.055919885635376\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 5.353852272033691 | KNN Loss: 4.347198009490967 | BCE Loss: 1.0066542625427246\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 5.397388935089111 | KNN Loss: 4.347323894500732 | BCE Loss: 1.050065040588379\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 5.3640265464782715 | KNN Loss: 4.336954593658447 | BCE Loss: 1.0270718336105347\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 5.342825889587402 | KNN Loss: 4.33432149887085 | BCE Loss: 1.0085043907165527\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 5.340450286865234 | KNN Loss: 4.346156120300293 | BCE Loss: 0.9942941069602966\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 5.420260429382324 | KNN Loss: 4.373485088348389 | BCE Loss: 1.0467751026153564\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 5.36843729019165 | KNN Loss: 4.358658313751221 | BCE Loss: 1.0097789764404297\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 5.366552352905273 | KNN Loss: 4.335986137390137 | BCE Loss: 1.0305660963058472\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 5.380528450012207 | KNN Loss: 4.377425193786621 | BCE Loss: 1.003103494644165\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 5.382617950439453 | KNN Loss: 4.325880527496338 | BCE Loss: 1.0567371845245361\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 5.376482009887695 | KNN Loss: 4.368416786193848 | BCE Loss: 1.0080649852752686\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 5.367707252502441 | KNN Loss: 4.371628761291504 | BCE Loss: 0.9960784912109375\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 5.359077453613281 | KNN Loss: 4.329676628112793 | BCE Loss: 1.0294008255004883\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 5.34519100189209 | KNN Loss: 4.33960485458374 | BCE Loss: 1.0055859088897705\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 5.410956859588623 | KNN Loss: 4.365288734436035 | BCE Loss: 1.045668125152588\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 5.385366439819336 | KNN Loss: 4.367864608764648 | BCE Loss: 1.017501711845398\n",
      "Epoch   261: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 5.360667705535889 | KNN Loss: 4.323700904846191 | BCE Loss: 1.0369668006896973\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 5.346221923828125 | KNN Loss: 4.332536220550537 | BCE Loss: 1.013685703277588\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 5.381706714630127 | KNN Loss: 4.355262279510498 | BCE Loss: 1.026444435119629\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 5.390356063842773 | KNN Loss: 4.355148792266846 | BCE Loss: 1.0352075099945068\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 5.385512828826904 | KNN Loss: 4.3359222412109375 | BCE Loss: 1.0495904684066772\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 5.3729753494262695 | KNN Loss: 4.360514163970947 | BCE Loss: 1.0124609470367432\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 5.362777233123779 | KNN Loss: 4.345219612121582 | BCE Loss: 1.0175577402114868\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 5.390131950378418 | KNN Loss: 4.357522010803223 | BCE Loss: 1.0326100587844849\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 5.369585037231445 | KNN Loss: 4.356812953948975 | BCE Loss: 1.0127723217010498\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 5.407491683959961 | KNN Loss: 4.391022682189941 | BCE Loss: 1.0164690017700195\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 5.391096115112305 | KNN Loss: 4.339065074920654 | BCE Loss: 1.0520312786102295\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 5.379412651062012 | KNN Loss: 4.320863723754883 | BCE Loss: 1.058548927307129\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 5.413043975830078 | KNN Loss: 4.394337177276611 | BCE Loss: 1.0187065601348877\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 5.372989654541016 | KNN Loss: 4.341806411743164 | BCE Loss: 1.0311832427978516\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 5.392609596252441 | KNN Loss: 4.350244998931885 | BCE Loss: 1.0423645973205566\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 5.3895697593688965 | KNN Loss: 4.368339538574219 | BCE Loss: 1.0212303400039673\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 5.379774570465088 | KNN Loss: 4.354373931884766 | BCE Loss: 1.0254006385803223\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 5.352426528930664 | KNN Loss: 4.332039833068848 | BCE Loss: 1.0203865766525269\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 5.381745338439941 | KNN Loss: 4.335109710693359 | BCE Loss: 1.046635627746582\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 5.359951496124268 | KNN Loss: 4.337413787841797 | BCE Loss: 1.0225377082824707\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 5.378098487854004 | KNN Loss: 4.341127872467041 | BCE Loss: 1.036970853805542\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 5.379685401916504 | KNN Loss: 4.347041606903076 | BCE Loss: 1.0326436758041382\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 5.384586334228516 | KNN Loss: 4.332018852233887 | BCE Loss: 1.0525672435760498\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 5.358538627624512 | KNN Loss: 4.320362567901611 | BCE Loss: 1.0381758213043213\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 5.446743965148926 | KNN Loss: 4.364884376525879 | BCE Loss: 1.0818594694137573\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 5.329648971557617 | KNN Loss: 4.326398849487305 | BCE Loss: 1.0032498836517334\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 5.33663272857666 | KNN Loss: 4.3465070724487305 | BCE Loss: 0.9901254177093506\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 5.347312927246094 | KNN Loss: 4.325089454650879 | BCE Loss: 1.0222235918045044\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 5.393913745880127 | KNN Loss: 4.3552422523498535 | BCE Loss: 1.038671612739563\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 5.3691911697387695 | KNN Loss: 4.356516361236572 | BCE Loss: 1.0126748085021973\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 5.3842692375183105 | KNN Loss: 4.353452205657959 | BCE Loss: 1.0308170318603516\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 5.394530773162842 | KNN Loss: 4.349802494049072 | BCE Loss: 1.04472815990448\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 5.370720863342285 | KNN Loss: 4.331573963165283 | BCE Loss: 1.0391466617584229\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 5.415286064147949 | KNN Loss: 4.369811534881592 | BCE Loss: 1.0454742908477783\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 5.400716304779053 | KNN Loss: 4.34616231918335 | BCE Loss: 1.0545541048049927\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 5.383275508880615 | KNN Loss: 4.337636470794678 | BCE Loss: 1.045638918876648\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 5.3702592849731445 | KNN Loss: 4.345073223114014 | BCE Loss: 1.0251858234405518\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 5.380049705505371 | KNN Loss: 4.348268032073975 | BCE Loss: 1.0317819118499756\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 5.379295349121094 | KNN Loss: 4.333625793457031 | BCE Loss: 1.0456695556640625\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 5.369515895843506 | KNN Loss: 4.352837085723877 | BCE Loss: 1.0166789293289185\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 5.3672590255737305 | KNN Loss: 4.374927043914795 | BCE Loss: 0.9923321604728699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 5.376496315002441 | KNN Loss: 4.369040489196777 | BCE Loss: 1.007455825805664\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 5.377857685089111 | KNN Loss: 4.3503804206848145 | BCE Loss: 1.0274772644042969\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 5.363445281982422 | KNN Loss: 4.351357936859131 | BCE Loss: 1.0120872259140015\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 5.406569957733154 | KNN Loss: 4.3738226890563965 | BCE Loss: 1.0327472686767578\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 5.416513442993164 | KNN Loss: 4.411596775054932 | BCE Loss: 1.0049166679382324\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 5.370360851287842 | KNN Loss: 4.336607933044434 | BCE Loss: 1.0337527990341187\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 5.353951454162598 | KNN Loss: 4.338425159454346 | BCE Loss: 1.015526294708252\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 5.432212829589844 | KNN Loss: 4.389011383056641 | BCE Loss: 1.0432015657424927\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 5.43960428237915 | KNN Loss: 4.360849857330322 | BCE Loss: 1.0787544250488281\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 5.380345821380615 | KNN Loss: 4.357646465301514 | BCE Loss: 1.0226993560791016\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 5.3620781898498535 | KNN Loss: 4.362267971038818 | BCE Loss: 0.9998103976249695\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 5.3642096519470215 | KNN Loss: 4.343607425689697 | BCE Loss: 1.0206022262573242\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 5.340731620788574 | KNN Loss: 4.331159591674805 | BCE Loss: 1.00957190990448\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 5.369819641113281 | KNN Loss: 4.341612339019775 | BCE Loss: 1.028207540512085\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 5.35478401184082 | KNN Loss: 4.331161975860596 | BCE Loss: 1.0236217975616455\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 5.380673408508301 | KNN Loss: 4.368394374847412 | BCE Loss: 1.0122792720794678\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 5.385202407836914 | KNN Loss: 4.3595476150512695 | BCE Loss: 1.025654911994934\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 5.384654998779297 | KNN Loss: 4.367202281951904 | BCE Loss: 1.0174527168273926\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 5.411436080932617 | KNN Loss: 4.3419904708862305 | BCE Loss: 1.0694454908370972\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 5.358312606811523 | KNN Loss: 4.338162422180176 | BCE Loss: 1.0201501846313477\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 5.364686012268066 | KNN Loss: 4.3421630859375 | BCE Loss: 1.0225231647491455\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 5.413000106811523 | KNN Loss: 4.392982006072998 | BCE Loss: 1.0200181007385254\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 5.3460540771484375 | KNN Loss: 4.3527374267578125 | BCE Loss: 0.9933167695999146\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 5.396074295043945 | KNN Loss: 4.351163864135742 | BCE Loss: 1.0449106693267822\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 5.378617763519287 | KNN Loss: 4.352396011352539 | BCE Loss: 1.026221752166748\n",
      "Epoch   272: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 5.369636535644531 | KNN Loss: 4.363326072692871 | BCE Loss: 1.0063104629516602\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 5.370350360870361 | KNN Loss: 4.328771591186523 | BCE Loss: 1.0415788888931274\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 5.399250030517578 | KNN Loss: 4.4002580642700195 | BCE Loss: 0.998991847038269\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 5.366734027862549 | KNN Loss: 4.336724281311035 | BCE Loss: 1.0300097465515137\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 5.372087478637695 | KNN Loss: 4.354812145233154 | BCE Loss: 1.0172755718231201\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 5.374558448791504 | KNN Loss: 4.333035469055176 | BCE Loss: 1.0415229797363281\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 5.375374794006348 | KNN Loss: 4.331821441650391 | BCE Loss: 1.043553113937378\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 5.401473045349121 | KNN Loss: 4.3474040031433105 | BCE Loss: 1.0540690422058105\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 5.3842973709106445 | KNN Loss: 4.334105968475342 | BCE Loss: 1.0501916408538818\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 5.390953063964844 | KNN Loss: 4.386235237121582 | BCE Loss: 1.0047180652618408\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 5.354508399963379 | KNN Loss: 4.35786771774292 | BCE Loss: 0.9966408014297485\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 5.352268218994141 | KNN Loss: 4.3375244140625 | BCE Loss: 1.0147435665130615\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 5.363924026489258 | KNN Loss: 4.318082809448242 | BCE Loss: 1.0458409786224365\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 5.382574081420898 | KNN Loss: 4.364969730377197 | BCE Loss: 1.0176042318344116\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 5.4132232666015625 | KNN Loss: 4.377132415771484 | BCE Loss: 1.0360910892486572\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 5.392831325531006 | KNN Loss: 4.349966526031494 | BCE Loss: 1.0428647994995117\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 5.399195194244385 | KNN Loss: 4.365661144256592 | BCE Loss: 1.033534049987793\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 5.345402717590332 | KNN Loss: 4.340439319610596 | BCE Loss: 1.0049636363983154\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 5.394124984741211 | KNN Loss: 4.3509931564331055 | BCE Loss: 1.0431318283081055\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 5.380587100982666 | KNN Loss: 4.349954128265381 | BCE Loss: 1.0306329727172852\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 5.391717910766602 | KNN Loss: 4.345526218414307 | BCE Loss: 1.046191930770874\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 5.373131275177002 | KNN Loss: 4.361514091491699 | BCE Loss: 1.0116171836853027\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 5.353313446044922 | KNN Loss: 4.342016220092773 | BCE Loss: 1.0112972259521484\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 5.414155006408691 | KNN Loss: 4.388629913330078 | BCE Loss: 1.0255253314971924\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 5.392010688781738 | KNN Loss: 4.371560573577881 | BCE Loss: 1.0204499959945679\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 5.362133026123047 | KNN Loss: 4.321434497833252 | BCE Loss: 1.040698766708374\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 5.350992679595947 | KNN Loss: 4.316996097564697 | BCE Loss: 1.0339964628219604\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 5.361018657684326 | KNN Loss: 4.341175556182861 | BCE Loss: 1.0198431015014648\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 5.379941940307617 | KNN Loss: 4.345150947570801 | BCE Loss: 1.034791111946106\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 5.403542995452881 | KNN Loss: 4.334117889404297 | BCE Loss: 1.0694249868392944\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 5.370438575744629 | KNN Loss: 4.345952987670898 | BCE Loss: 1.0244855880737305\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 5.36752986907959 | KNN Loss: 4.339266777038574 | BCE Loss: 1.0282630920410156\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 5.36884880065918 | KNN Loss: 4.350225448608398 | BCE Loss: 1.0186233520507812\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 5.3836140632629395 | KNN Loss: 4.36854362487793 | BCE Loss: 1.0150705575942993\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 5.402588844299316 | KNN Loss: 4.37351131439209 | BCE Loss: 1.0290776491165161\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 5.355772972106934 | KNN Loss: 4.324366092681885 | BCE Loss: 1.0314068794250488\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 5.362783432006836 | KNN Loss: 4.341817378997803 | BCE Loss: 1.020965814590454\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 5.361514091491699 | KNN Loss: 4.3344950675964355 | BCE Loss: 1.0270187854766846\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 5.435682773590088 | KNN Loss: 4.390714645385742 | BCE Loss: 1.0449682474136353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 5.352625370025635 | KNN Loss: 4.32868766784668 | BCE Loss: 1.023937702178955\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 5.3901896476745605 | KNN Loss: 4.339118480682373 | BCE Loss: 1.0510711669921875\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 5.378846168518066 | KNN Loss: 4.340297698974609 | BCE Loss: 1.0385485887527466\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 5.337159633636475 | KNN Loss: 4.340985298156738 | BCE Loss: 0.9961743354797363\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 5.3386077880859375 | KNN Loss: 4.332905292510986 | BCE Loss: 1.0057027339935303\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 5.363412380218506 | KNN Loss: 4.346210479736328 | BCE Loss: 1.0172020196914673\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 5.414684295654297 | KNN Loss: 4.385612964630127 | BCE Loss: 1.0290714502334595\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 5.3822712898254395 | KNN Loss: 4.362052917480469 | BCE Loss: 1.0202183723449707\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 5.321832656860352 | KNN Loss: 4.323422431945801 | BCE Loss: 0.9984102249145508\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 5.398698806762695 | KNN Loss: 4.356632232666016 | BCE Loss: 1.0420664548873901\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 5.394766807556152 | KNN Loss: 4.352810382843018 | BCE Loss: 1.0419561862945557\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 5.355317115783691 | KNN Loss: 4.333248138427734 | BCE Loss: 1.0220692157745361\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 5.356553077697754 | KNN Loss: 4.328533172607422 | BCE Loss: 1.028019905090332\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 5.352292060852051 | KNN Loss: 4.339921474456787 | BCE Loss: 1.0123708248138428\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 5.357895374298096 | KNN Loss: 4.342987060546875 | BCE Loss: 1.0149084329605103\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 5.406774520874023 | KNN Loss: 4.36484432220459 | BCE Loss: 1.0419299602508545\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 5.384960174560547 | KNN Loss: 4.349233150482178 | BCE Loss: 1.03572678565979\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 5.364676475524902 | KNN Loss: 4.341804027557373 | BCE Loss: 1.0228726863861084\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 5.341124534606934 | KNN Loss: 4.328031539916992 | BCE Loss: 1.013093113899231\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 5.383529186248779 | KNN Loss: 4.388499736785889 | BCE Loss: 0.9950294494628906\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 5.380457878112793 | KNN Loss: 4.341484546661377 | BCE Loss: 1.0389734506607056\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 5.418848991394043 | KNN Loss: 4.3396382331848145 | BCE Loss: 1.0792105197906494\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 5.390865325927734 | KNN Loss: 4.353135585784912 | BCE Loss: 1.0377296209335327\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 5.357080459594727 | KNN Loss: 4.343303203582764 | BCE Loss: 1.0137770175933838\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 5.376324653625488 | KNN Loss: 4.361202716827393 | BCE Loss: 1.0151219367980957\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 5.367855072021484 | KNN Loss: 4.345143795013428 | BCE Loss: 1.0227115154266357\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 5.4111714363098145 | KNN Loss: 4.3832106590271 | BCE Loss: 1.0279607772827148\n",
      "Epoch   283: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 5.359370708465576 | KNN Loss: 4.366232395172119 | BCE Loss: 0.9931381344795227\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 5.391495704650879 | KNN Loss: 4.368415832519531 | BCE Loss: 1.0230801105499268\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 5.412652015686035 | KNN Loss: 4.356208801269531 | BCE Loss: 1.056443452835083\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 5.366339683532715 | KNN Loss: 4.354046821594238 | BCE Loss: 1.0122928619384766\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 5.390707492828369 | KNN Loss: 4.354550361633301 | BCE Loss: 1.0361570119857788\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 5.37860107421875 | KNN Loss: 4.333615779876709 | BCE Loss: 1.044985294342041\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 5.363402843475342 | KNN Loss: 4.337841987609863 | BCE Loss: 1.0255608558654785\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 5.394916534423828 | KNN Loss: 4.3445024490356445 | BCE Loss: 1.0504140853881836\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 5.381799221038818 | KNN Loss: 4.3634138107299805 | BCE Loss: 1.018385410308838\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 5.408243179321289 | KNN Loss: 4.361663341522217 | BCE Loss: 1.0465795993804932\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 5.355648994445801 | KNN Loss: 4.337169170379639 | BCE Loss: 1.018479824066162\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 5.376723289489746 | KNN Loss: 4.352470397949219 | BCE Loss: 1.0242526531219482\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 5.369355201721191 | KNN Loss: 4.353464126586914 | BCE Loss: 1.0158913135528564\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 5.35797643661499 | KNN Loss: 4.337428092956543 | BCE Loss: 1.0205484628677368\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 5.380460262298584 | KNN Loss: 4.344581127166748 | BCE Loss: 1.035879135131836\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 5.320227146148682 | KNN Loss: 4.320381164550781 | BCE Loss: 0.9998458623886108\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 5.389915943145752 | KNN Loss: 4.348682880401611 | BCE Loss: 1.0412331819534302\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 5.371342658996582 | KNN Loss: 4.349481582641602 | BCE Loss: 1.02186119556427\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 5.31223201751709 | KNN Loss: 4.331275463104248 | BCE Loss: 0.9809564352035522\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 5.360268592834473 | KNN Loss: 4.363038539886475 | BCE Loss: 0.997230052947998\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 5.398891925811768 | KNN Loss: 4.370472431182861 | BCE Loss: 1.0284193754196167\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 5.395865440368652 | KNN Loss: 4.377034664154053 | BCE Loss: 1.01883065700531\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 5.357463836669922 | KNN Loss: 4.347449779510498 | BCE Loss: 1.010014295578003\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 5.38808536529541 | KNN Loss: 4.3430562019348145 | BCE Loss: 1.0450291633605957\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 5.385793685913086 | KNN Loss: 4.332662105560303 | BCE Loss: 1.053131341934204\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 5.394066333770752 | KNN Loss: 4.38324499130249 | BCE Loss: 1.0108213424682617\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 5.368405342102051 | KNN Loss: 4.33349609375 | BCE Loss: 1.0349094867706299\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 5.361668586730957 | KNN Loss: 4.3379716873168945 | BCE Loss: 1.0236968994140625\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 5.399547576904297 | KNN Loss: 4.36911678314209 | BCE Loss: 1.030430793762207\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 5.366142272949219 | KNN Loss: 4.336545467376709 | BCE Loss: 1.0295968055725098\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.3662309646606445 | KNN Loss: 4.328845024108887 | BCE Loss: 1.0373857021331787\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 5.4214935302734375 | KNN Loss: 4.36419677734375 | BCE Loss: 1.0572965145111084\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 5.360324859619141 | KNN Loss: 4.338881492614746 | BCE Loss: 1.021443247795105\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 5.3638691902160645 | KNN Loss: 4.345190525054932 | BCE Loss: 1.0186786651611328\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 5.383160591125488 | KNN Loss: 4.353865146636963 | BCE Loss: 1.0292953252792358\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 5.373688220977783 | KNN Loss: 4.355837821960449 | BCE Loss: 1.017850399017334\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 5.384017467498779 | KNN Loss: 4.332457065582275 | BCE Loss: 1.051560401916504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 5.3787384033203125 | KNN Loss: 4.349741458892822 | BCE Loss: 1.0289967060089111\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 5.391236305236816 | KNN Loss: 4.364835739135742 | BCE Loss: 1.0264006853103638\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 5.386870861053467 | KNN Loss: 4.3490309715271 | BCE Loss: 1.0378398895263672\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 5.380796432495117 | KNN Loss: 4.343257904052734 | BCE Loss: 1.0375384092330933\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 5.366450786590576 | KNN Loss: 4.345076084136963 | BCE Loss: 1.0213747024536133\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 5.3672027587890625 | KNN Loss: 4.343896865844727 | BCE Loss: 1.023305892944336\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 5.381892681121826 | KNN Loss: 4.345914363861084 | BCE Loss: 1.0359783172607422\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 5.389949798583984 | KNN Loss: 4.359908103942871 | BCE Loss: 1.0300416946411133\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 5.378985404968262 | KNN Loss: 4.335997581481934 | BCE Loss: 1.0429878234863281\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 5.377490997314453 | KNN Loss: 4.338879585266113 | BCE Loss: 1.0386111736297607\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 5.4290571212768555 | KNN Loss: 4.418288230895996 | BCE Loss: 1.0107686519622803\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 5.381433963775635 | KNN Loss: 4.3355584144592285 | BCE Loss: 1.0458755493164062\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 5.357171535491943 | KNN Loss: 4.329794883728027 | BCE Loss: 1.0273767709732056\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 5.392009735107422 | KNN Loss: 4.363876819610596 | BCE Loss: 1.0281331539154053\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 5.371313095092773 | KNN Loss: 4.3521928787231445 | BCE Loss: 1.019120454788208\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 5.345697402954102 | KNN Loss: 4.336699485778809 | BCE Loss: 1.008997917175293\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 5.374529838562012 | KNN Loss: 4.352232456207275 | BCE Loss: 1.0222973823547363\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 5.343715190887451 | KNN Loss: 4.339261531829834 | BCE Loss: 1.0044536590576172\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 5.347167491912842 | KNN Loss: 4.331164360046387 | BCE Loss: 1.016003131866455\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 5.387770652770996 | KNN Loss: 4.354983329772949 | BCE Loss: 1.0327870845794678\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 5.379787445068359 | KNN Loss: 4.328630447387695 | BCE Loss: 1.051156997680664\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 5.4039764404296875 | KNN Loss: 4.358569622039795 | BCE Loss: 1.0454069375991821\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 5.369739532470703 | KNN Loss: 4.349722862243652 | BCE Loss: 1.0200164318084717\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 5.376987457275391 | KNN Loss: 4.35145378112793 | BCE Loss: 1.0255337953567505\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 5.369784355163574 | KNN Loss: 4.3315887451171875 | BCE Loss: 1.0381956100463867\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 5.376954078674316 | KNN Loss: 4.3544416427612305 | BCE Loss: 1.022512674331665\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 5.414241790771484 | KNN Loss: 4.372023105621338 | BCE Loss: 1.0422189235687256\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 5.39026403427124 | KNN Loss: 4.344035625457764 | BCE Loss: 1.046228289604187\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 5.348342418670654 | KNN Loss: 4.326940536499023 | BCE Loss: 1.0214017629623413\n",
      "Epoch   294: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 5.361627578735352 | KNN Loss: 4.337763786315918 | BCE Loss: 1.0238640308380127\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 5.397934913635254 | KNN Loss: 4.3721160888671875 | BCE Loss: 1.0258188247680664\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 5.371826171875 | KNN Loss: 4.3536057472229 | BCE Loss: 1.01822030544281\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 5.375041484832764 | KNN Loss: 4.344779968261719 | BCE Loss: 1.030261516571045\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 5.348115921020508 | KNN Loss: 4.315359115600586 | BCE Loss: 1.0327568054199219\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 5.356790065765381 | KNN Loss: 4.331833362579346 | BCE Loss: 1.0249567031860352\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 5.417121887207031 | KNN Loss: 4.376423358917236 | BCE Loss: 1.0406986474990845\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 5.355317115783691 | KNN Loss: 4.343231201171875 | BCE Loss: 1.0120856761932373\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 5.39086389541626 | KNN Loss: 4.3696184158325195 | BCE Loss: 1.0212455987930298\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 5.396856784820557 | KNN Loss: 4.347758769989014 | BCE Loss: 1.049098014831543\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 5.375543117523193 | KNN Loss: 4.347045421600342 | BCE Loss: 1.0284976959228516\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 5.369935989379883 | KNN Loss: 4.3472466468811035 | BCE Loss: 1.0226891040802002\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 5.379193305969238 | KNN Loss: 4.343113422393799 | BCE Loss: 1.0360798835754395\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 5.36820650100708 | KNN Loss: 4.370382308959961 | BCE Loss: 0.9978243112564087\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 5.373297691345215 | KNN Loss: 4.34499454498291 | BCE Loss: 1.0283033847808838\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 5.3596510887146 | KNN Loss: 4.347990036010742 | BCE Loss: 1.0116609334945679\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 5.330879211425781 | KNN Loss: 4.330890655517578 | BCE Loss: 0.9999885559082031\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 5.392495155334473 | KNN Loss: 4.369770526885986 | BCE Loss: 1.0227248668670654\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 5.3741302490234375 | KNN Loss: 4.339906215667725 | BCE Loss: 1.0342239141464233\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 5.409100532531738 | KNN Loss: 4.377115726470947 | BCE Loss: 1.0319846868515015\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 5.372775554656982 | KNN Loss: 4.351423263549805 | BCE Loss: 1.0213524103164673\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 5.355607032775879 | KNN Loss: 4.343113899230957 | BCE Loss: 1.0124931335449219\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 5.362410545349121 | KNN Loss: 4.34297513961792 | BCE Loss: 1.0194356441497803\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 5.376126766204834 | KNN Loss: 4.339791774749756 | BCE Loss: 1.0363349914550781\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 5.3797736167907715 | KNN Loss: 4.356889724731445 | BCE Loss: 1.0228838920593262\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 5.345098495483398 | KNN Loss: 4.337109088897705 | BCE Loss: 1.0079892873764038\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 5.34403133392334 | KNN Loss: 4.326322078704834 | BCE Loss: 1.017709493637085\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 5.431130409240723 | KNN Loss: 4.392348766326904 | BCE Loss: 1.0387815237045288\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 5.355542182922363 | KNN Loss: 4.3377604484558105 | BCE Loss: 1.0177816152572632\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 5.364294528961182 | KNN Loss: 4.355879783630371 | BCE Loss: 1.0084147453308105\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 5.374990463256836 | KNN Loss: 4.349307060241699 | BCE Loss: 1.0256836414337158\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 5.3342108726501465 | KNN Loss: 4.335391044616699 | BCE Loss: 0.9988197088241577\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 5.367157459259033 | KNN Loss: 4.346137523651123 | BCE Loss: 1.0210199356079102\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 5.3583831787109375 | KNN Loss: 4.326077938079834 | BCE Loss: 1.0323050022125244\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 5.385288715362549 | KNN Loss: 4.364060878753662 | BCE Loss: 1.0212279558181763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 5.387683868408203 | KNN Loss: 4.339895248413086 | BCE Loss: 1.0477887392044067\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 5.371133327484131 | KNN Loss: 4.343096733093262 | BCE Loss: 1.0280365943908691\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 5.410622596740723 | KNN Loss: 4.3871684074401855 | BCE Loss: 1.023453950881958\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 5.35328483581543 | KNN Loss: 4.3300275802612305 | BCE Loss: 1.0232571363449097\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 5.420668601989746 | KNN Loss: 4.387984275817871 | BCE Loss: 1.032684564590454\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 5.409950256347656 | KNN Loss: 4.366508483886719 | BCE Loss: 1.0434417724609375\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 5.461366176605225 | KNN Loss: 4.411252975463867 | BCE Loss: 1.050113320350647\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 5.426380157470703 | KNN Loss: 4.364851951599121 | BCE Loss: 1.061528205871582\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 5.357494831085205 | KNN Loss: 4.349223613739014 | BCE Loss: 1.0082712173461914\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 5.40739107131958 | KNN Loss: 4.368238925933838 | BCE Loss: 1.0391520261764526\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 5.341745853424072 | KNN Loss: 4.345405101776123 | BCE Loss: 0.9963408708572388\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 5.350748538970947 | KNN Loss: 4.332525730133057 | BCE Loss: 1.0182228088378906\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 5.383946418762207 | KNN Loss: 4.349356174468994 | BCE Loss: 1.0345903635025024\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 5.377839088439941 | KNN Loss: 4.344860553741455 | BCE Loss: 1.0329782962799072\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 5.395561695098877 | KNN Loss: 4.346599578857422 | BCE Loss: 1.0489622354507446\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 5.3927083015441895 | KNN Loss: 4.351696968078613 | BCE Loss: 1.0410113334655762\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 5.368833065032959 | KNN Loss: 4.3377685546875 | BCE Loss: 1.0310643911361694\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 5.376753807067871 | KNN Loss: 4.3535966873168945 | BCE Loss: 1.0231571197509766\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 5.350433349609375 | KNN Loss: 4.340356826782227 | BCE Loss: 1.0100767612457275\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 5.373963832855225 | KNN Loss: 4.353311538696289 | BCE Loss: 1.0206522941589355\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 5.376607418060303 | KNN Loss: 4.34186315536499 | BCE Loss: 1.034744143486023\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 5.4170823097229 | KNN Loss: 4.350456714630127 | BCE Loss: 1.0666254758834839\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 5.383804798126221 | KNN Loss: 4.34495210647583 | BCE Loss: 1.0388526916503906\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 5.364073753356934 | KNN Loss: 4.339473724365234 | BCE Loss: 1.0245997905731201\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 5.375285625457764 | KNN Loss: 4.334038257598877 | BCE Loss: 1.0412474870681763\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 5.4015583992004395 | KNN Loss: 4.352578639984131 | BCE Loss: 1.0489797592163086\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 5.415053844451904 | KNN Loss: 4.373206615447998 | BCE Loss: 1.0418473482131958\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 5.356721878051758 | KNN Loss: 4.32397985458374 | BCE Loss: 1.0327420234680176\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 5.359467506408691 | KNN Loss: 4.349119663238525 | BCE Loss: 1.010347843170166\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 5.3629302978515625 | KNN Loss: 4.350098609924316 | BCE Loss: 1.0128319263458252\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 5.410295486450195 | KNN Loss: 4.375207424163818 | BCE Loss: 1.0350878238677979\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 5.373443126678467 | KNN Loss: 4.346410751342773 | BCE Loss: 1.0270323753356934\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 5.40626335144043 | KNN Loss: 4.375534534454346 | BCE Loss: 1.030729055404663\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 5.3348188400268555 | KNN Loss: 4.317544937133789 | BCE Loss: 1.0172736644744873\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 5.3932108879089355 | KNN Loss: 4.35895299911499 | BCE Loss: 1.0342580080032349\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 5.389151573181152 | KNN Loss: 4.352212905883789 | BCE Loss: 1.0369387865066528\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 5.358588218688965 | KNN Loss: 4.342855453491211 | BCE Loss: 1.0157325267791748\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 5.3303351402282715 | KNN Loss: 4.324236869812012 | BCE Loss: 1.0060982704162598\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 5.3653130531311035 | KNN Loss: 4.343964576721191 | BCE Loss: 1.0213483572006226\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 5.3728485107421875 | KNN Loss: 4.335786819458008 | BCE Loss: 1.0370615720748901\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 5.340908527374268 | KNN Loss: 4.336863994598389 | BCE Loss: 1.0040444135665894\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 5.408883094787598 | KNN Loss: 4.388931751251221 | BCE Loss: 1.0199511051177979\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 5.363649368286133 | KNN Loss: 4.33882474899292 | BCE Loss: 1.024824857711792\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 5.375588417053223 | KNN Loss: 4.333251953125 | BCE Loss: 1.0423364639282227\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 5.3983941078186035 | KNN Loss: 4.351175785064697 | BCE Loss: 1.0472183227539062\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 5.373301982879639 | KNN Loss: 4.355825901031494 | BCE Loss: 1.017475962638855\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 5.347979545593262 | KNN Loss: 4.324580669403076 | BCE Loss: 1.0233991146087646\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 5.413468360900879 | KNN Loss: 4.3798699378967285 | BCE Loss: 1.0335981845855713\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 5.377123832702637 | KNN Loss: 4.3765740394592285 | BCE Loss: 1.0005499124526978\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 5.327467441558838 | KNN Loss: 4.331820964813232 | BCE Loss: 0.9956464767456055\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 5.3660783767700195 | KNN Loss: 4.328517913818359 | BCE Loss: 1.0375604629516602\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 5.396848201751709 | KNN Loss: 4.389841079711914 | BCE Loss: 1.0070072412490845\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 5.4162092208862305 | KNN Loss: 4.380459308624268 | BCE Loss: 1.0357496738433838\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 5.378014087677002 | KNN Loss: 4.382594585418701 | BCE Loss: 0.9954195618629456\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 5.386896133422852 | KNN Loss: 4.360973834991455 | BCE Loss: 1.0259225368499756\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 5.379353046417236 | KNN Loss: 4.337437629699707 | BCE Loss: 1.0419154167175293\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 5.358518123626709 | KNN Loss: 4.33751916885376 | BCE Loss: 1.0209989547729492\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 5.371284484863281 | KNN Loss: 4.359968185424805 | BCE Loss: 1.0113165378570557\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 5.358704566955566 | KNN Loss: 4.338836193084717 | BCE Loss: 1.0198686122894287\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 5.351010322570801 | KNN Loss: 4.336583614349365 | BCE Loss: 1.014426827430725\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 5.397654056549072 | KNN Loss: 4.373100757598877 | BCE Loss: 1.0245532989501953\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 5.345060348510742 | KNN Loss: 4.339571475982666 | BCE Loss: 1.0054891109466553\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 5.390220642089844 | KNN Loss: 4.357403755187988 | BCE Loss: 1.0328171253204346\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 5.3809099197387695 | KNN Loss: 4.354571342468262 | BCE Loss: 1.0263386964797974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 5.36876106262207 | KNN Loss: 4.344345569610596 | BCE Loss: 1.0244154930114746\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 5.328984260559082 | KNN Loss: 4.330155372619629 | BCE Loss: 0.9988290071487427\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 5.389225959777832 | KNN Loss: 4.3387274742126465 | BCE Loss: 1.0504987239837646\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 5.375657081604004 | KNN Loss: 4.353789806365967 | BCE Loss: 1.0218675136566162\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 5.385025978088379 | KNN Loss: 4.366623401641846 | BCE Loss: 1.0184025764465332\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 5.340858459472656 | KNN Loss: 4.337552547454834 | BCE Loss: 1.0033060312271118\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 5.411264896392822 | KNN Loss: 4.367744445800781 | BCE Loss: 1.0435205698013306\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 5.374178409576416 | KNN Loss: 4.335440158843994 | BCE Loss: 1.0387382507324219\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 5.410045623779297 | KNN Loss: 4.352206707000732 | BCE Loss: 1.0578389167785645\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 5.394218444824219 | KNN Loss: 4.358890056610107 | BCE Loss: 1.0353283882141113\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 5.351922512054443 | KNN Loss: 4.346620559692383 | BCE Loss: 1.0053019523620605\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 5.369110584259033 | KNN Loss: 4.354551315307617 | BCE Loss: 1.014559268951416\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 5.364068031311035 | KNN Loss: 4.337972164154053 | BCE Loss: 1.0260961055755615\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 5.393923759460449 | KNN Loss: 4.366359233856201 | BCE Loss: 1.027564525604248\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 5.374059200286865 | KNN Loss: 4.364304542541504 | BCE Loss: 1.0097546577453613\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 5.381874084472656 | KNN Loss: 4.341351509094238 | BCE Loss: 1.040522575378418\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 5.377675533294678 | KNN Loss: 4.3615946769714355 | BCE Loss: 1.0160809755325317\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 5.388117790222168 | KNN Loss: 4.3496503829956055 | BCE Loss: 1.0384674072265625\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 5.37607479095459 | KNN Loss: 4.326872825622559 | BCE Loss: 1.0492022037506104\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 5.355742931365967 | KNN Loss: 4.340699672698975 | BCE Loss: 1.0150432586669922\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 5.425741195678711 | KNN Loss: 4.34190034866333 | BCE Loss: 1.08384108543396\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 5.399773597717285 | KNN Loss: 4.363323211669922 | BCE Loss: 1.0364503860473633\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 5.363471508026123 | KNN Loss: 4.351943492889404 | BCE Loss: 1.0115278959274292\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 5.390673637390137 | KNN Loss: 4.345346450805664 | BCE Loss: 1.0453271865844727\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 5.369032859802246 | KNN Loss: 4.325984001159668 | BCE Loss: 1.043048620223999\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 5.405228137969971 | KNN Loss: 4.337466716766357 | BCE Loss: 1.0677613019943237\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 5.382847785949707 | KNN Loss: 4.358832836151123 | BCE Loss: 1.0240147113800049\n",
      "Epoch   315: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 5.374996185302734 | KNN Loss: 4.332448482513428 | BCE Loss: 1.0425474643707275\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 5.356663227081299 | KNN Loss: 4.346771240234375 | BCE Loss: 1.0098919868469238\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 5.368892669677734 | KNN Loss: 4.327457427978516 | BCE Loss: 1.0414353609085083\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 5.413205146789551 | KNN Loss: 4.373300552368164 | BCE Loss: 1.0399045944213867\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 5.397199630737305 | KNN Loss: 4.363410472869873 | BCE Loss: 1.0337892770767212\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 5.336272239685059 | KNN Loss: 4.319714069366455 | BCE Loss: 1.0165584087371826\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 5.410954475402832 | KNN Loss: 4.375547885894775 | BCE Loss: 1.0354067087173462\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 5.363367080688477 | KNN Loss: 4.320556163787842 | BCE Loss: 1.0428111553192139\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 5.379571914672852 | KNN Loss: 4.3393473625183105 | BCE Loss: 1.040224552154541\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 5.348123550415039 | KNN Loss: 4.340610504150391 | BCE Loss: 1.0075132846832275\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 5.405373573303223 | KNN Loss: 4.367876052856445 | BCE Loss: 1.0374972820281982\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 5.424075126647949 | KNN Loss: 4.390959739685059 | BCE Loss: 1.0331153869628906\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 5.351001739501953 | KNN Loss: 4.337441921234131 | BCE Loss: 1.0135600566864014\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 5.366761207580566 | KNN Loss: 4.359116077423096 | BCE Loss: 1.0076448917388916\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 5.396389961242676 | KNN Loss: 4.376457214355469 | BCE Loss: 1.019932746887207\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 5.3307271003723145 | KNN Loss: 4.338788032531738 | BCE Loss: 0.991939127445221\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 5.3846235275268555 | KNN Loss: 4.348065376281738 | BCE Loss: 1.036557912826538\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 5.411487579345703 | KNN Loss: 4.3569722175598145 | BCE Loss: 1.0545153617858887\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 5.415043830871582 | KNN Loss: 4.363198757171631 | BCE Loss: 1.0518453121185303\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 5.384392261505127 | KNN Loss: 4.361573219299316 | BCE Loss: 1.0228190422058105\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 5.38340950012207 | KNN Loss: 4.348970890045166 | BCE Loss: 1.0344384908676147\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 5.429734706878662 | KNN Loss: 4.370665073394775 | BCE Loss: 1.0590695142745972\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 5.38551139831543 | KNN Loss: 4.37485408782959 | BCE Loss: 1.0106574296951294\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 5.349652290344238 | KNN Loss: 4.329947471618652 | BCE Loss: 1.019705057144165\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 5.3898162841796875 | KNN Loss: 4.356107711791992 | BCE Loss: 1.0337083339691162\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 5.39530086517334 | KNN Loss: 4.371600151062012 | BCE Loss: 1.0237007141113281\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 5.374562740325928 | KNN Loss: 4.348097801208496 | BCE Loss: 1.0264649391174316\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 5.36648416519165 | KNN Loss: 4.366483688354492 | BCE Loss: 1.0000003576278687\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 5.345697402954102 | KNN Loss: 4.336703777313232 | BCE Loss: 1.0089936256408691\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 5.36842679977417 | KNN Loss: 4.349627494812012 | BCE Loss: 1.0187991857528687\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 5.361123085021973 | KNN Loss: 4.3245463371276855 | BCE Loss: 1.036576747894287\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 5.381592750549316 | KNN Loss: 4.360447883605957 | BCE Loss: 1.0211451053619385\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 5.409758567810059 | KNN Loss: 4.386539459228516 | BCE Loss: 1.023219347000122\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 5.363541603088379 | KNN Loss: 4.340449333190918 | BCE Loss: 1.02309250831604\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 5.385978698730469 | KNN Loss: 4.356574535369873 | BCE Loss: 1.0294044017791748\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 5.363758563995361 | KNN Loss: 4.365657806396484 | BCE Loss: 0.9981006383895874\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 5.344974994659424 | KNN Loss: 4.336781024932861 | BCE Loss: 1.0081939697265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 5.345055103302002 | KNN Loss: 4.333880424499512 | BCE Loss: 1.0111747980117798\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 5.370384216308594 | KNN Loss: 4.35182523727417 | BCE Loss: 1.018559217453003\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 5.418431282043457 | KNN Loss: 4.357544898986816 | BCE Loss: 1.0608866214752197\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 5.364306449890137 | KNN Loss: 4.331836700439453 | BCE Loss: 1.0324697494506836\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 5.391447067260742 | KNN Loss: 4.365975379943848 | BCE Loss: 1.0254719257354736\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 5.417884826660156 | KNN Loss: 4.382458686828613 | BCE Loss: 1.035426139831543\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 5.395211219787598 | KNN Loss: 4.339386463165283 | BCE Loss: 1.0558247566223145\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 5.442824363708496 | KNN Loss: 4.388823509216309 | BCE Loss: 1.054000973701477\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 5.382234573364258 | KNN Loss: 4.351518154144287 | BCE Loss: 1.0307164192199707\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.381564617156982 | KNN Loss: 4.347172737121582 | BCE Loss: 1.03439199924469\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 5.415408134460449 | KNN Loss: 4.3609724044799805 | BCE Loss: 1.0544359683990479\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 5.400424003601074 | KNN Loss: 4.344531059265137 | BCE Loss: 1.055893063545227\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 5.440456390380859 | KNN Loss: 4.407599449157715 | BCE Loss: 1.0328567028045654\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 5.351535797119141 | KNN Loss: 4.321932792663574 | BCE Loss: 1.0296030044555664\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 5.3925604820251465 | KNN Loss: 4.3595194816589355 | BCE Loss: 1.033041000366211\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 5.369702339172363 | KNN Loss: 4.344457149505615 | BCE Loss: 1.0252454280853271\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 5.364017009735107 | KNN Loss: 4.369113445281982 | BCE Loss: 0.9949036240577698\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 5.335641860961914 | KNN Loss: 4.32493257522583 | BCE Loss: 1.010709524154663\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 5.388552665710449 | KNN Loss: 4.361872673034668 | BCE Loss: 1.0266799926757812\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 5.37584114074707 | KNN Loss: 4.3391642570495605 | BCE Loss: 1.0366766452789307\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 5.385097026824951 | KNN Loss: 4.35484504699707 | BCE Loss: 1.0302518606185913\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 5.4283294677734375 | KNN Loss: 4.3883771896362305 | BCE Loss: 1.039952278137207\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 5.380455017089844 | KNN Loss: 4.348430633544922 | BCE Loss: 1.0320245027542114\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 5.3883771896362305 | KNN Loss: 4.360826015472412 | BCE Loss: 1.0275510549545288\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 5.3670196533203125 | KNN Loss: 4.320400238037109 | BCE Loss: 1.0466192960739136\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 5.392706871032715 | KNN Loss: 4.375145435333252 | BCE Loss: 1.0175611972808838\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 5.375337600708008 | KNN Loss: 4.33627462387085 | BCE Loss: 1.039062738418579\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 5.3246750831604 | KNN Loss: 4.329395294189453 | BCE Loss: 0.9952796697616577\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 5.353671073913574 | KNN Loss: 4.336198806762695 | BCE Loss: 1.0174720287322998\n",
      "Epoch   326: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 5.395942211151123 | KNN Loss: 4.352697372436523 | BCE Loss: 1.0432448387145996\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 5.376410484313965 | KNN Loss: 4.341696262359619 | BCE Loss: 1.0347142219543457\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 5.362860679626465 | KNN Loss: 4.349681377410889 | BCE Loss: 1.0131793022155762\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 5.378756046295166 | KNN Loss: 4.342476844787598 | BCE Loss: 1.0362792015075684\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 5.352326393127441 | KNN Loss: 4.332002639770508 | BCE Loss: 1.0203239917755127\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 5.376874923706055 | KNN Loss: 4.328494548797607 | BCE Loss: 1.0483803749084473\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 5.398931503295898 | KNN Loss: 4.37765645980835 | BCE Loss: 1.0212748050689697\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 5.391188621520996 | KNN Loss: 4.351706027984619 | BCE Loss: 1.0394827127456665\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 5.342352867126465 | KNN Loss: 4.351781368255615 | BCE Loss: 0.9905714392662048\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 5.369017601013184 | KNN Loss: 4.34189510345459 | BCE Loss: 1.0271222591400146\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 5.364404678344727 | KNN Loss: 4.345432281494141 | BCE Loss: 1.0189725160598755\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 5.378166198730469 | KNN Loss: 4.346476078033447 | BCE Loss: 1.0316903591156006\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 5.389041900634766 | KNN Loss: 4.361130714416504 | BCE Loss: 1.0279111862182617\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 5.377415657043457 | KNN Loss: 4.341189861297607 | BCE Loss: 1.0362259149551392\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 5.341975212097168 | KNN Loss: 4.344899654388428 | BCE Loss: 0.9970757365226746\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 5.361384868621826 | KNN Loss: 4.348557472229004 | BCE Loss: 1.0128275156021118\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 5.354664325714111 | KNN Loss: 4.327898979187012 | BCE Loss: 1.0267653465270996\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 5.4396257400512695 | KNN Loss: 4.360624313354492 | BCE Loss: 1.0790011882781982\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 5.355678558349609 | KNN Loss: 4.346489906311035 | BCE Loss: 1.0091887712478638\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 5.366955757141113 | KNN Loss: 4.331084251403809 | BCE Loss: 1.0358715057373047\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 5.4063615798950195 | KNN Loss: 4.376582622528076 | BCE Loss: 1.0297787189483643\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 5.3510422706604 | KNN Loss: 4.345302581787109 | BCE Loss: 1.005739688873291\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 5.34749698638916 | KNN Loss: 4.347114086151123 | BCE Loss: 1.0003827810287476\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 5.370743751525879 | KNN Loss: 4.330202102661133 | BCE Loss: 1.0405418872833252\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 5.401312828063965 | KNN Loss: 4.348419666290283 | BCE Loss: 1.0528929233551025\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 5.398355484008789 | KNN Loss: 4.352128028869629 | BCE Loss: 1.0462275743484497\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 5.392728805541992 | KNN Loss: 4.371633052825928 | BCE Loss: 1.0210955142974854\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 5.391364097595215 | KNN Loss: 4.371564865112305 | BCE Loss: 1.0197994709014893\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 5.379266738891602 | KNN Loss: 4.346363544464111 | BCE Loss: 1.0329034328460693\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 5.403160095214844 | KNN Loss: 4.358151912689209 | BCE Loss: 1.0450081825256348\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 5.364502906799316 | KNN Loss: 4.339527130126953 | BCE Loss: 1.0249760150909424\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 5.371336936950684 | KNN Loss: 4.326321125030518 | BCE Loss: 1.0450159311294556\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 5.380146026611328 | KNN Loss: 4.344105243682861 | BCE Loss: 1.036041021347046\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 5.37748908996582 | KNN Loss: 4.327845573425293 | BCE Loss: 1.0496437549591064\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 5.387707233428955 | KNN Loss: 4.341394424438477 | BCE Loss: 1.0463128089904785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 5.360718727111816 | KNN Loss: 4.329967498779297 | BCE Loss: 1.0307509899139404\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 5.404683589935303 | KNN Loss: 4.355605602264404 | BCE Loss: 1.049078106880188\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 5.344674110412598 | KNN Loss: 4.338256359100342 | BCE Loss: 1.0064175128936768\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 5.335905075073242 | KNN Loss: 4.3275837898254395 | BCE Loss: 1.0083215236663818\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 5.357287406921387 | KNN Loss: 4.341869354248047 | BCE Loss: 1.0154178142547607\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 5.359344482421875 | KNN Loss: 4.31995964050293 | BCE Loss: 1.0393846035003662\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 5.398453712463379 | KNN Loss: 4.354249954223633 | BCE Loss: 1.044203519821167\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 5.413182735443115 | KNN Loss: 4.349185466766357 | BCE Loss: 1.0639972686767578\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 5.39022970199585 | KNN Loss: 4.344326972961426 | BCE Loss: 1.0459026098251343\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 5.367072582244873 | KNN Loss: 4.339000225067139 | BCE Loss: 1.028072476387024\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 5.375572204589844 | KNN Loss: 4.344588279724121 | BCE Loss: 1.0309836864471436\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 5.366694450378418 | KNN Loss: 4.359974384307861 | BCE Loss: 1.006719946861267\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 5.414048671722412 | KNN Loss: 4.396657943725586 | BCE Loss: 1.0173908472061157\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 5.358463287353516 | KNN Loss: 4.344832420349121 | BCE Loss: 1.0136311054229736\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 5.343448638916016 | KNN Loss: 4.333958148956299 | BCE Loss: 1.0094906091690063\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 5.3770036697387695 | KNN Loss: 4.3440752029418945 | BCE Loss: 1.032928228378296\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 5.373089790344238 | KNN Loss: 4.333041667938232 | BCE Loss: 1.0400478839874268\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 5.375007152557373 | KNN Loss: 4.357196807861328 | BCE Loss: 1.017810344696045\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 5.375782012939453 | KNN Loss: 4.341212749481201 | BCE Loss: 1.034569263458252\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 5.408994674682617 | KNN Loss: 4.366444110870361 | BCE Loss: 1.042550802230835\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 5.361762523651123 | KNN Loss: 4.3219218254089355 | BCE Loss: 1.0398406982421875\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 5.4030046463012695 | KNN Loss: 4.372872352600098 | BCE Loss: 1.0301320552825928\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 5.36724328994751 | KNN Loss: 4.342834949493408 | BCE Loss: 1.0244084596633911\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 5.388591289520264 | KNN Loss: 4.366745948791504 | BCE Loss: 1.0218453407287598\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 5.387350082397461 | KNN Loss: 4.3556108474731445 | BCE Loss: 1.0317394733428955\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 5.414236068725586 | KNN Loss: 4.377946376800537 | BCE Loss: 1.0362894535064697\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 5.346153736114502 | KNN Loss: 4.354150295257568 | BCE Loss: 0.9920035600662231\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 5.384878635406494 | KNN Loss: 4.356424808502197 | BCE Loss: 1.0284538269042969\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 5.347734451293945 | KNN Loss: 4.328584671020508 | BCE Loss: 1.0191500186920166\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 5.355281829833984 | KNN Loss: 4.339097499847412 | BCE Loss: 1.0161845684051514\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 5.397177696228027 | KNN Loss: 4.383864879608154 | BCE Loss: 1.0133129358291626\n",
      "Epoch   337: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 5.402751445770264 | KNN Loss: 4.382022857666016 | BCE Loss: 1.0207284688949585\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 5.345089912414551 | KNN Loss: 4.330410480499268 | BCE Loss: 1.014679193496704\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 5.365379333496094 | KNN Loss: 4.334133625030518 | BCE Loss: 1.031245470046997\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 5.367891311645508 | KNN Loss: 4.332962512969971 | BCE Loss: 1.034928560256958\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 5.339048385620117 | KNN Loss: 4.3386969566345215 | BCE Loss: 1.0003514289855957\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 5.39103889465332 | KNN Loss: 4.372605800628662 | BCE Loss: 1.018432855606079\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 5.365117073059082 | KNN Loss: 4.370185852050781 | BCE Loss: 0.9949309825897217\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 5.3951263427734375 | KNN Loss: 4.338657855987549 | BCE Loss: 1.0564683675765991\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 5.368464469909668 | KNN Loss: 4.3461713790893555 | BCE Loss: 1.022293210029602\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 5.367675304412842 | KNN Loss: 4.3354034423828125 | BCE Loss: 1.0322718620300293\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 5.3803277015686035 | KNN Loss: 4.354916572570801 | BCE Loss: 1.0254110097885132\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 5.317002296447754 | KNN Loss: 4.316378116607666 | BCE Loss: 1.0006239414215088\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 5.34106969833374 | KNN Loss: 4.313658237457275 | BCE Loss: 1.0274115800857544\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 5.410289764404297 | KNN Loss: 4.369843006134033 | BCE Loss: 1.0404468774795532\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 5.383153915405273 | KNN Loss: 4.362382411956787 | BCE Loss: 1.0207717418670654\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 5.368278980255127 | KNN Loss: 4.356608867645264 | BCE Loss: 1.0116699934005737\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 5.358975410461426 | KNN Loss: 4.347540855407715 | BCE Loss: 1.011434555053711\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 5.390832424163818 | KNN Loss: 4.351934909820557 | BCE Loss: 1.0388975143432617\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 5.359614372253418 | KNN Loss: 4.331175327301025 | BCE Loss: 1.028438925743103\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 5.348731994628906 | KNN Loss: 4.318809509277344 | BCE Loss: 1.0299222469329834\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 5.352632522583008 | KNN Loss: 4.3230180740356445 | BCE Loss: 1.0296144485473633\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 5.392826557159424 | KNN Loss: 4.3383469581604 | BCE Loss: 1.054479718208313\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 5.353307723999023 | KNN Loss: 4.318225860595703 | BCE Loss: 1.0350821018218994\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 5.342507362365723 | KNN Loss: 4.318722724914551 | BCE Loss: 1.023784875869751\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 5.4066596031188965 | KNN Loss: 4.363611221313477 | BCE Loss: 1.0430485010147095\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 5.398974895477295 | KNN Loss: 4.3545145988464355 | BCE Loss: 1.0444601774215698\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 5.365630149841309 | KNN Loss: 4.34160041809082 | BCE Loss: 1.0240299701690674\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 5.376411437988281 | KNN Loss: 4.335768222808838 | BCE Loss: 1.040643334388733\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 5.425743103027344 | KNN Loss: 4.399930477142334 | BCE Loss: 1.0258127450942993\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 5.353003978729248 | KNN Loss: 4.333159923553467 | BCE Loss: 1.0198439359664917\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 5.367440223693848 | KNN Loss: 4.343334674835205 | BCE Loss: 1.0241055488586426\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 5.343660354614258 | KNN Loss: 4.3372721672058105 | BCE Loss: 1.0063880681991577\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 5.35533332824707 | KNN Loss: 4.326172351837158 | BCE Loss: 1.0291612148284912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 5.46096658706665 | KNN Loss: 4.421188831329346 | BCE Loss: 1.0397777557373047\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 5.417422771453857 | KNN Loss: 4.410400867462158 | BCE Loss: 1.0070219039916992\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 5.393093109130859 | KNN Loss: 4.359714508056641 | BCE Loss: 1.0333783626556396\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 5.368679046630859 | KNN Loss: 4.329094886779785 | BCE Loss: 1.0395841598510742\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 5.367444038391113 | KNN Loss: 4.35665225982666 | BCE Loss: 1.0107920169830322\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 5.357845783233643 | KNN Loss: 4.334264278411865 | BCE Loss: 1.0235815048217773\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 5.416504859924316 | KNN Loss: 4.39518404006958 | BCE Loss: 1.0213205814361572\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 5.343430995941162 | KNN Loss: 4.336134910583496 | BCE Loss: 1.007296085357666\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 5.348801136016846 | KNN Loss: 4.3423285484313965 | BCE Loss: 1.0064727067947388\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 5.3519287109375 | KNN Loss: 4.333760738372803 | BCE Loss: 1.0181679725646973\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 5.359935760498047 | KNN Loss: 4.33654260635376 | BCE Loss: 1.023393154144287\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 5.36435079574585 | KNN Loss: 4.340153694152832 | BCE Loss: 1.0241971015930176\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 5.367084503173828 | KNN Loss: 4.341066360473633 | BCE Loss: 1.0260183811187744\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 5.364116191864014 | KNN Loss: 4.327700138092041 | BCE Loss: 1.036415934562683\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 5.391292572021484 | KNN Loss: 4.361008167266846 | BCE Loss: 1.0302841663360596\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 5.4131951332092285 | KNN Loss: 4.384989261627197 | BCE Loss: 1.0282058715820312\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 5.410321235656738 | KNN Loss: 4.377862930297852 | BCE Loss: 1.0324585437774658\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 5.361663341522217 | KNN Loss: 4.3321638107299805 | BCE Loss: 1.0294995307922363\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 5.359934329986572 | KNN Loss: 4.338109970092773 | BCE Loss: 1.0218243598937988\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 5.413931846618652 | KNN Loss: 4.35226583480835 | BCE Loss: 1.0616660118103027\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 5.373340129852295 | KNN Loss: 4.357854843139648 | BCE Loss: 1.0154852867126465\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 5.378440856933594 | KNN Loss: 4.341660022735596 | BCE Loss: 1.036780834197998\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 5.349583625793457 | KNN Loss: 4.322758197784424 | BCE Loss: 1.0268254280090332\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 5.392482280731201 | KNN Loss: 4.350777626037598 | BCE Loss: 1.041704535484314\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 5.301852226257324 | KNN Loss: 4.322903156280518 | BCE Loss: 0.9789489507675171\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 5.333241939544678 | KNN Loss: 4.322268486022949 | BCE Loss: 1.0109734535217285\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 5.387697219848633 | KNN Loss: 4.350639820098877 | BCE Loss: 1.0370571613311768\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 5.40070915222168 | KNN Loss: 4.355025768280029 | BCE Loss: 1.0456836223602295\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 5.416956901550293 | KNN Loss: 4.375 | BCE Loss: 1.0419566631317139\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 5.406489849090576 | KNN Loss: 4.359676361083984 | BCE Loss: 1.0468133687973022\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 5.375864028930664 | KNN Loss: 4.332791805267334 | BCE Loss: 1.0430723428726196\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 5.374982833862305 | KNN Loss: 4.354273319244385 | BCE Loss: 1.0207093954086304\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 5.417872428894043 | KNN Loss: 4.373205661773682 | BCE Loss: 1.0446665287017822\n",
      "Epoch   348: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 5.3475341796875 | KNN Loss: 4.349453926086426 | BCE Loss: 0.9980802536010742\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 5.404892444610596 | KNN Loss: 4.395707130432129 | BCE Loss: 1.0091853141784668\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 5.361346244812012 | KNN Loss: 4.346826553344727 | BCE Loss: 1.0145199298858643\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 5.363704681396484 | KNN Loss: 4.336236953735352 | BCE Loss: 1.0274678468704224\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 5.371888160705566 | KNN Loss: 4.356445789337158 | BCE Loss: 1.0154423713684082\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 5.373610496520996 | KNN Loss: 4.324072360992432 | BCE Loss: 1.049538016319275\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 5.387084484100342 | KNN Loss: 4.341585159301758 | BCE Loss: 1.0454994440078735\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 5.374549865722656 | KNN Loss: 4.36482048034668 | BCE Loss: 1.0097291469573975\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 5.360864639282227 | KNN Loss: 4.337707042694092 | BCE Loss: 1.0231578350067139\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 5.37046480178833 | KNN Loss: 4.333776950836182 | BCE Loss: 1.0366878509521484\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 5.373045921325684 | KNN Loss: 4.335814952850342 | BCE Loss: 1.0372310876846313\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 5.345849990844727 | KNN Loss: 4.32589864730835 | BCE Loss: 1.019951343536377\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 5.3443121910095215 | KNN Loss: 4.321556091308594 | BCE Loss: 1.0227562189102173\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 5.355257987976074 | KNN Loss: 4.328616619110107 | BCE Loss: 1.0266413688659668\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 5.352602958679199 | KNN Loss: 4.325473785400391 | BCE Loss: 1.0271292924880981\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 5.363461017608643 | KNN Loss: 4.346895217895508 | BCE Loss: 1.0165656805038452\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 5.376928806304932 | KNN Loss: 4.384336948394775 | BCE Loss: 0.9925916790962219\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 5.423091411590576 | KNN Loss: 4.359489917755127 | BCE Loss: 1.0636016130447388\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 5.469478607177734 | KNN Loss: 4.421144485473633 | BCE Loss: 1.0483343601226807\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 5.374319553375244 | KNN Loss: 4.3413166999816895 | BCE Loss: 1.0330028533935547\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 5.369897842407227 | KNN Loss: 4.34064245223999 | BCE Loss: 1.0292551517486572\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 5.390473365783691 | KNN Loss: 4.370686054229736 | BCE Loss: 1.019787311553955\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 5.3914642333984375 | KNN Loss: 4.345509052276611 | BCE Loss: 1.0459551811218262\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 5.393144130706787 | KNN Loss: 4.348155498504639 | BCE Loss: 1.0449885129928589\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 5.3580474853515625 | KNN Loss: 4.34752082824707 | BCE Loss: 1.0105266571044922\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 5.403566360473633 | KNN Loss: 4.340067386627197 | BCE Loss: 1.0634987354278564\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 5.3701019287109375 | KNN Loss: 4.343213081359863 | BCE Loss: 1.0268886089324951\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 5.3934407234191895 | KNN Loss: 4.377628803253174 | BCE Loss: 1.0158119201660156\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 5.40592098236084 | KNN Loss: 4.395845890045166 | BCE Loss: 1.0100748538970947\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 5.3810625076293945 | KNN Loss: 4.357466220855713 | BCE Loss: 1.0235960483551025\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 5.341131210327148 | KNN Loss: 4.338066577911377 | BCE Loss: 1.0030648708343506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 5.343161582946777 | KNN Loss: 4.318607330322266 | BCE Loss: 1.0245540142059326\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 5.372434616088867 | KNN Loss: 4.356947422027588 | BCE Loss: 1.0154869556427002\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 5.413322448730469 | KNN Loss: 4.338711738586426 | BCE Loss: 1.074610710144043\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 5.3520588874816895 | KNN Loss: 4.337334632873535 | BCE Loss: 1.0147241353988647\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 5.358988285064697 | KNN Loss: 4.341034412384033 | BCE Loss: 1.017953872680664\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 5.364492416381836 | KNN Loss: 4.342912197113037 | BCE Loss: 1.0215799808502197\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 5.353063106536865 | KNN Loss: 4.3678131103515625 | BCE Loss: 0.9852499961853027\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 5.353179454803467 | KNN Loss: 4.3340911865234375 | BCE Loss: 1.0190882682800293\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 5.367982864379883 | KNN Loss: 4.3532328605651855 | BCE Loss: 1.0147502422332764\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 5.342106819152832 | KNN Loss: 4.32103157043457 | BCE Loss: 1.0210754871368408\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 5.381846904754639 | KNN Loss: 4.3545002937316895 | BCE Loss: 1.0273466110229492\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 5.408216953277588 | KNN Loss: 4.375596046447754 | BCE Loss: 1.0326210260391235\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 5.332798957824707 | KNN Loss: 4.327406883239746 | BCE Loss: 1.005392074584961\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 5.351191520690918 | KNN Loss: 4.329893589019775 | BCE Loss: 1.0212981700897217\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 5.352543354034424 | KNN Loss: 4.343157768249512 | BCE Loss: 1.0093857049942017\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 5.37891960144043 | KNN Loss: 4.339931011199951 | BCE Loss: 1.0389885902404785\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 5.345972061157227 | KNN Loss: 4.328649044036865 | BCE Loss: 1.0173230171203613\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 5.359772682189941 | KNN Loss: 4.332695484161377 | BCE Loss: 1.0270771980285645\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 5.371951103210449 | KNN Loss: 4.3321146965026855 | BCE Loss: 1.0398364067077637\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 5.402229309082031 | KNN Loss: 4.33928108215332 | BCE Loss: 1.0629479885101318\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 5.363783359527588 | KNN Loss: 4.359305381774902 | BCE Loss: 1.0044779777526855\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 5.34254789352417 | KNN Loss: 4.336986541748047 | BCE Loss: 1.005561351776123\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 5.394466876983643 | KNN Loss: 4.352843761444092 | BCE Loss: 1.0416231155395508\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 5.354589462280273 | KNN Loss: 4.325440883636475 | BCE Loss: 1.0291485786437988\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 5.382600784301758 | KNN Loss: 4.36412239074707 | BCE Loss: 1.0184783935546875\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 5.3584675788879395 | KNN Loss: 4.330923080444336 | BCE Loss: 1.0275444984436035\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 5.384621620178223 | KNN Loss: 4.354447364807129 | BCE Loss: 1.0301742553710938\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 5.36101770401001 | KNN Loss: 4.334016799926758 | BCE Loss: 1.0270010232925415\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 5.36039924621582 | KNN Loss: 4.338472366333008 | BCE Loss: 1.0219266414642334\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 5.356573104858398 | KNN Loss: 4.334129810333252 | BCE Loss: 1.0224435329437256\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 5.366944789886475 | KNN Loss: 4.33445930480957 | BCE Loss: 1.0324856042861938\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 5.385215759277344 | KNN Loss: 4.399320125579834 | BCE Loss: 0.9858958721160889\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 5.38889217376709 | KNN Loss: 4.334822654724121 | BCE Loss: 1.0540697574615479\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 5.389418601989746 | KNN Loss: 4.3511176109313965 | BCE Loss: 1.0383009910583496\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 5.363201141357422 | KNN Loss: 4.348361015319824 | BCE Loss: 1.0148398876190186\n",
      "Epoch   359: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 5.38361930847168 | KNN Loss: 4.359216690063477 | BCE Loss: 1.024402379989624\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 5.372278690338135 | KNN Loss: 4.355541229248047 | BCE Loss: 1.0167375802993774\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 5.365717887878418 | KNN Loss: 4.342472553253174 | BCE Loss: 1.0232455730438232\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 5.392223834991455 | KNN Loss: 4.370476722717285 | BCE Loss: 1.0217472314834595\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 5.410274505615234 | KNN Loss: 4.349091529846191 | BCE Loss: 1.0611827373504639\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 5.394822120666504 | KNN Loss: 4.375071048736572 | BCE Loss: 1.0197513103485107\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 5.377143859863281 | KNN Loss: 4.349118232727051 | BCE Loss: 1.0280256271362305\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 5.426539421081543 | KNN Loss: 4.39084005355835 | BCE Loss: 1.0356991291046143\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 5.393540859222412 | KNN Loss: 4.368809700012207 | BCE Loss: 1.024731159210205\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 5.417568206787109 | KNN Loss: 4.3645219802856445 | BCE Loss: 1.0530462265014648\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 5.3704633712768555 | KNN Loss: 4.333828926086426 | BCE Loss: 1.0366346836090088\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 5.399409294128418 | KNN Loss: 4.348469257354736 | BCE Loss: 1.050939917564392\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 5.365945339202881 | KNN Loss: 4.358866214752197 | BCE Loss: 1.0070791244506836\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 5.417647361755371 | KNN Loss: 4.377217769622803 | BCE Loss: 1.0404298305511475\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 5.3966064453125 | KNN Loss: 4.3568339347839355 | BCE Loss: 1.0397727489471436\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 5.342831611633301 | KNN Loss: 4.336065292358398 | BCE Loss: 1.0067663192749023\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 5.392211437225342 | KNN Loss: 4.379452705383301 | BCE Loss: 1.0127588510513306\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 5.407175540924072 | KNN Loss: 4.3624114990234375 | BCE Loss: 1.0447640419006348\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 5.385162830352783 | KNN Loss: 4.357919692993164 | BCE Loss: 1.0272432565689087\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 5.372166633605957 | KNN Loss: 4.3305253982543945 | BCE Loss: 1.0416409969329834\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 5.3862690925598145 | KNN Loss: 4.353756427764893 | BCE Loss: 1.0325127840042114\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 5.408417701721191 | KNN Loss: 4.356975078582764 | BCE Loss: 1.0514428615570068\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 5.370999336242676 | KNN Loss: 4.344823837280273 | BCE Loss: 1.0261752605438232\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 5.407716751098633 | KNN Loss: 4.345957279205322 | BCE Loss: 1.0617594718933105\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 5.388383388519287 | KNN Loss: 4.341638088226318 | BCE Loss: 1.0467453002929688\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 5.352915287017822 | KNN Loss: 4.339103698730469 | BCE Loss: 1.0138115882873535\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 5.353240966796875 | KNN Loss: 4.333045959472656 | BCE Loss: 1.0201947689056396\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 5.443111419677734 | KNN Loss: 4.387578964233398 | BCE Loss: 1.055532693862915\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 5.379287242889404 | KNN Loss: 4.3456244468688965 | BCE Loss: 1.0336626768112183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 5.363353729248047 | KNN Loss: 4.346924304962158 | BCE Loss: 1.0164294242858887\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 5.385161876678467 | KNN Loss: 4.328492164611816 | BCE Loss: 1.05666983127594\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 5.342675685882568 | KNN Loss: 4.341615676879883 | BCE Loss: 1.0010600090026855\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 5.385725498199463 | KNN Loss: 4.344820976257324 | BCE Loss: 1.0409046411514282\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 5.357307434082031 | KNN Loss: 4.336922645568848 | BCE Loss: 1.0203845500946045\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 5.41559362411499 | KNN Loss: 4.374716281890869 | BCE Loss: 1.0408774614334106\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 5.398985862731934 | KNN Loss: 4.345499515533447 | BCE Loss: 1.0534861087799072\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 5.34314489364624 | KNN Loss: 4.338852405548096 | BCE Loss: 1.0042924880981445\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 5.377790451049805 | KNN Loss: 4.3563551902771 | BCE Loss: 1.021435260772705\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 5.360995292663574 | KNN Loss: 4.356705665588379 | BCE Loss: 1.0042893886566162\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 5.319234848022461 | KNN Loss: 4.332070350646973 | BCE Loss: 0.9871647357940674\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 5.406450271606445 | KNN Loss: 4.38789176940918 | BCE Loss: 1.0185587406158447\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 5.358252048492432 | KNN Loss: 4.350284099578857 | BCE Loss: 1.0079680681228638\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 5.36578369140625 | KNN Loss: 4.327788352966309 | BCE Loss: 1.0379951000213623\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 5.371872901916504 | KNN Loss: 4.369612693786621 | BCE Loss: 1.0022600889205933\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 5.381265640258789 | KNN Loss: 4.361025810241699 | BCE Loss: 1.0202398300170898\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 5.349078178405762 | KNN Loss: 4.3279709815979 | BCE Loss: 1.0211074352264404\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 5.4146270751953125 | KNN Loss: 4.365887641906738 | BCE Loss: 1.0487391948699951\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 5.361932754516602 | KNN Loss: 4.354531764984131 | BCE Loss: 1.0074007511138916\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 5.388261318206787 | KNN Loss: 4.347882270812988 | BCE Loss: 1.0403790473937988\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 5.339110374450684 | KNN Loss: 4.333344459533691 | BCE Loss: 1.0057661533355713\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 5.3686065673828125 | KNN Loss: 4.329156398773193 | BCE Loss: 1.0394500494003296\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 5.406050682067871 | KNN Loss: 4.372161865234375 | BCE Loss: 1.0338890552520752\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 5.397801876068115 | KNN Loss: 4.387232303619385 | BCE Loss: 1.0105695724487305\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 5.376070976257324 | KNN Loss: 4.35415506362915 | BCE Loss: 1.0219160318374634\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 5.351385116577148 | KNN Loss: 4.345445156097412 | BCE Loss: 1.0059401988983154\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 5.393769264221191 | KNN Loss: 4.368250846862793 | BCE Loss: 1.0255181789398193\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 5.382874488830566 | KNN Loss: 4.3485283851623535 | BCE Loss: 1.0343458652496338\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 5.358093738555908 | KNN Loss: 4.345286846160889 | BCE Loss: 1.0128068923950195\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 5.398756504058838 | KNN Loss: 4.359485626220703 | BCE Loss: 1.0392708778381348\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 5.405725002288818 | KNN Loss: 4.354855537414551 | BCE Loss: 1.0508694648742676\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 5.398383140563965 | KNN Loss: 4.345351219177246 | BCE Loss: 1.0530319213867188\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 5.3622965812683105 | KNN Loss: 4.348571300506592 | BCE Loss: 1.0137252807617188\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 5.349214553833008 | KNN Loss: 4.341091632843018 | BCE Loss: 1.0081226825714111\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 5.364008903503418 | KNN Loss: 4.332620143890381 | BCE Loss: 1.0313889980316162\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 5.349377632141113 | KNN Loss: 4.343600273132324 | BCE Loss: 1.005777359008789\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 5.365821361541748 | KNN Loss: 4.355142116546631 | BCE Loss: 1.0106791257858276\n",
      "Epoch   370: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 5.388993263244629 | KNN Loss: 4.359634876251221 | BCE Loss: 1.0293586254119873\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 5.353620529174805 | KNN Loss: 4.326272010803223 | BCE Loss: 1.0273487567901611\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 5.326449871063232 | KNN Loss: 4.319368362426758 | BCE Loss: 1.0070815086364746\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 5.34414005279541 | KNN Loss: 4.322795391082764 | BCE Loss: 1.0213444232940674\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 5.375463962554932 | KNN Loss: 4.3340044021606445 | BCE Loss: 1.0414596796035767\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 5.442504405975342 | KNN Loss: 4.39230489730835 | BCE Loss: 1.0501995086669922\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 5.353344917297363 | KNN Loss: 4.335721492767334 | BCE Loss: 1.0176236629486084\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 5.385545253753662 | KNN Loss: 4.344071865081787 | BCE Loss: 1.0414732694625854\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 5.405337810516357 | KNN Loss: 4.373967170715332 | BCE Loss: 1.0313706398010254\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 5.385076522827148 | KNN Loss: 4.353283882141113 | BCE Loss: 1.0317928791046143\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 5.432005882263184 | KNN Loss: 4.370114326477051 | BCE Loss: 1.0618915557861328\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 5.386197090148926 | KNN Loss: 4.358214855194092 | BCE Loss: 1.027982234954834\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 5.387102127075195 | KNN Loss: 4.355898857116699 | BCE Loss: 1.031203031539917\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 5.368891716003418 | KNN Loss: 4.344595909118652 | BCE Loss: 1.0242960453033447\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 5.374696731567383 | KNN Loss: 4.355957984924316 | BCE Loss: 1.018738865852356\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 5.3619585037231445 | KNN Loss: 4.331506729125977 | BCE Loss: 1.030451774597168\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 5.365396022796631 | KNN Loss: 4.344398021697998 | BCE Loss: 1.0209978818893433\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 5.39388370513916 | KNN Loss: 4.3714985847473145 | BCE Loss: 1.0223851203918457\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 5.338987827301025 | KNN Loss: 4.331393718719482 | BCE Loss: 1.007594108581543\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 5.395639896392822 | KNN Loss: 4.3399505615234375 | BCE Loss: 1.0556892156600952\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 5.448329448699951 | KNN Loss: 4.411314487457275 | BCE Loss: 1.0370149612426758\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 5.384198188781738 | KNN Loss: 4.341709613800049 | BCE Loss: 1.0424888134002686\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 5.376586437225342 | KNN Loss: 4.3455810546875 | BCE Loss: 1.0310053825378418\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 5.346059799194336 | KNN Loss: 4.328491687774658 | BCE Loss: 1.0175682306289673\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 5.338449001312256 | KNN Loss: 4.315155506134033 | BCE Loss: 1.0232936143875122\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 5.362791061401367 | KNN Loss: 4.339521884918213 | BCE Loss: 1.0232692956924438\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 5.358725547790527 | KNN Loss: 4.3470306396484375 | BCE Loss: 1.0116946697235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 5.39036226272583 | KNN Loss: 4.346982955932617 | BCE Loss: 1.0433794260025024\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 5.384981632232666 | KNN Loss: 4.382969379425049 | BCE Loss: 1.0020123720169067\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 5.378666877746582 | KNN Loss: 4.36087703704834 | BCE Loss: 1.0177898406982422\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 5.414875030517578 | KNN Loss: 4.375783920288086 | BCE Loss: 1.0390911102294922\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 5.398048400878906 | KNN Loss: 4.36453104019165 | BCE Loss: 1.033517599105835\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 5.4039387702941895 | KNN Loss: 4.356095790863037 | BCE Loss: 1.0478428602218628\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 5.358743190765381 | KNN Loss: 4.34894323348999 | BCE Loss: 1.0097999572753906\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 5.36942195892334 | KNN Loss: 4.33769416809082 | BCE Loss: 1.0317275524139404\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 5.368093490600586 | KNN Loss: 4.33016300201416 | BCE Loss: 1.0379307270050049\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 5.379369735717773 | KNN Loss: 4.357946872711182 | BCE Loss: 1.0214228630065918\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 5.383350372314453 | KNN Loss: 4.342099666595459 | BCE Loss: 1.0412505865097046\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 5.381725311279297 | KNN Loss: 4.353429794311523 | BCE Loss: 1.028295636177063\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 5.389708042144775 | KNN Loss: 4.351205348968506 | BCE Loss: 1.0385026931762695\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 5.3805341720581055 | KNN Loss: 4.351956844329834 | BCE Loss: 1.028577446937561\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 5.366737365722656 | KNN Loss: 4.356044292449951 | BCE Loss: 1.0106931924819946\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 5.361433029174805 | KNN Loss: 4.351936340332031 | BCE Loss: 1.0094964504241943\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 5.353326797485352 | KNN Loss: 4.342759132385254 | BCE Loss: 1.0105679035186768\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 5.380268096923828 | KNN Loss: 4.3269734382629395 | BCE Loss: 1.0532945394515991\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 5.385972499847412 | KNN Loss: 4.348891735076904 | BCE Loss: 1.0370807647705078\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 5.393997669219971 | KNN Loss: 4.361758232116699 | BCE Loss: 1.0322394371032715\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 5.3457465171813965 | KNN Loss: 4.335730075836182 | BCE Loss: 1.0100163221359253\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 5.3968305587768555 | KNN Loss: 4.349897384643555 | BCE Loss: 1.0469334125518799\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 5.373663902282715 | KNN Loss: 4.346561431884766 | BCE Loss: 1.0271027088165283\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 5.384711265563965 | KNN Loss: 4.340118885040283 | BCE Loss: 1.0445924997329712\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 5.3729095458984375 | KNN Loss: 4.336952209472656 | BCE Loss: 1.0359573364257812\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 5.347952842712402 | KNN Loss: 4.345025539398193 | BCE Loss: 1.002927303314209\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 5.4319000244140625 | KNN Loss: 4.39171838760376 | BCE Loss: 1.0401818752288818\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 5.412880897521973 | KNN Loss: 4.363399982452393 | BCE Loss: 1.049480676651001\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 5.386387825012207 | KNN Loss: 4.332354545593262 | BCE Loss: 1.0540335178375244\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 5.339905738830566 | KNN Loss: 4.323163032531738 | BCE Loss: 1.0167429447174072\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 5.377616882324219 | KNN Loss: 4.332263946533203 | BCE Loss: 1.0453526973724365\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 5.386796474456787 | KNN Loss: 4.353435516357422 | BCE Loss: 1.0333608388900757\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 5.382460117340088 | KNN Loss: 4.345157146453857 | BCE Loss: 1.0373029708862305\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 5.445494651794434 | KNN Loss: 4.395600318908691 | BCE Loss: 1.0498942136764526\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 5.403548240661621 | KNN Loss: 4.3436384201049805 | BCE Loss: 1.0599095821380615\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 5.362358093261719 | KNN Loss: 4.348685264587402 | BCE Loss: 1.013672947883606\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 5.370586395263672 | KNN Loss: 4.355262756347656 | BCE Loss: 1.0153236389160156\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 5.387814521789551 | KNN Loss: 4.34520149230957 | BCE Loss: 1.0426127910614014\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 5.384077072143555 | KNN Loss: 4.3619489669799805 | BCE Loss: 1.0221278667449951\n",
      "Epoch   381: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 5.355477333068848 | KNN Loss: 4.355190277099609 | BCE Loss: 1.0002872943878174\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 5.339188575744629 | KNN Loss: 4.345061779022217 | BCE Loss: 0.9941267967224121\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 5.389147758483887 | KNN Loss: 4.343905448913574 | BCE Loss: 1.0452425479888916\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 5.425493240356445 | KNN Loss: 4.3558831214904785 | BCE Loss: 1.069610357284546\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 5.418666362762451 | KNN Loss: 4.367094039916992 | BCE Loss: 1.051572322845459\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 5.371388912200928 | KNN Loss: 4.34051513671875 | BCE Loss: 1.0308737754821777\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 5.385170936584473 | KNN Loss: 4.350555419921875 | BCE Loss: 1.0346157550811768\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 5.373051166534424 | KNN Loss: 4.337333679199219 | BCE Loss: 1.035717487335205\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 5.324439525604248 | KNN Loss: 4.336851596832275 | BCE Loss: 0.9875877499580383\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 5.411787033081055 | KNN Loss: 4.372170448303223 | BCE Loss: 1.0396168231964111\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 5.364559650421143 | KNN Loss: 4.363842487335205 | BCE Loss: 1.0007171630859375\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 5.3544921875 | KNN Loss: 4.336495399475098 | BCE Loss: 1.0179967880249023\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 5.378905773162842 | KNN Loss: 4.3561482429504395 | BCE Loss: 1.0227575302124023\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 5.395625114440918 | KNN Loss: 4.350333213806152 | BCE Loss: 1.045291781425476\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 5.398005485534668 | KNN Loss: 4.35427188873291 | BCE Loss: 1.0437335968017578\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 5.365050315856934 | KNN Loss: 4.3370361328125 | BCE Loss: 1.0280143022537231\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 5.338086128234863 | KNN Loss: 4.322665691375732 | BCE Loss: 1.0154201984405518\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 5.354743480682373 | KNN Loss: 4.331850051879883 | BCE Loss: 1.0228934288024902\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 5.431010723114014 | KNN Loss: 4.393387794494629 | BCE Loss: 1.0376229286193848\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 5.387564659118652 | KNN Loss: 4.344167232513428 | BCE Loss: 1.043397307395935\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 5.413712978363037 | KNN Loss: 4.33509635925293 | BCE Loss: 1.078616738319397\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 5.376002311706543 | KNN Loss: 4.337858200073242 | BCE Loss: 1.0381443500518799\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 5.354633808135986 | KNN Loss: 4.3242716789245605 | BCE Loss: 1.0303620100021362\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 5.355548858642578 | KNN Loss: 4.329576015472412 | BCE Loss: 1.0259730815887451\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 5.364116668701172 | KNN Loss: 4.36547327041626 | BCE Loss: 0.9986432790756226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 5.3524041175842285 | KNN Loss: 4.342040061950684 | BCE Loss: 1.0103641748428345\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 5.353541851043701 | KNN Loss: 4.3537397384643555 | BCE Loss: 0.9998019933700562\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 5.38395357131958 | KNN Loss: 4.351023197174072 | BCE Loss: 1.0329303741455078\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 5.377869606018066 | KNN Loss: 4.358887672424316 | BCE Loss: 1.018982172012329\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 5.3930768966674805 | KNN Loss: 4.344080448150635 | BCE Loss: 1.0489966869354248\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 5.341936111450195 | KNN Loss: 4.316619396209717 | BCE Loss: 1.0253164768218994\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 5.371123790740967 | KNN Loss: 4.340087413787842 | BCE Loss: 1.0310362577438354\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 5.378000259399414 | KNN Loss: 4.341484069824219 | BCE Loss: 1.0365159511566162\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 5.365157127380371 | KNN Loss: 4.361501216888428 | BCE Loss: 1.0036561489105225\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 5.355718612670898 | KNN Loss: 4.341989994049072 | BCE Loss: 1.0137284994125366\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 5.388088226318359 | KNN Loss: 4.344656944274902 | BCE Loss: 1.0434315204620361\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 5.370181083679199 | KNN Loss: 4.351099967956543 | BCE Loss: 1.0190812349319458\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 5.386850833892822 | KNN Loss: 4.3583879470825195 | BCE Loss: 1.0284628868103027\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 5.3815107345581055 | KNN Loss: 4.346376895904541 | BCE Loss: 1.0351338386535645\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 5.38131856918335 | KNN Loss: 4.334869384765625 | BCE Loss: 1.0464491844177246\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 5.41368293762207 | KNN Loss: 4.368502616882324 | BCE Loss: 1.0451802015304565\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 5.377172946929932 | KNN Loss: 4.356123447418213 | BCE Loss: 1.0210493803024292\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 5.376873970031738 | KNN Loss: 4.320730686187744 | BCE Loss: 1.0561432838439941\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 5.413286209106445 | KNN Loss: 4.368682861328125 | BCE Loss: 1.0446035861968994\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 5.3615007400512695 | KNN Loss: 4.3501691818237305 | BCE Loss: 1.0113314390182495\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 5.341421604156494 | KNN Loss: 4.333770275115967 | BCE Loss: 1.0076513290405273\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 5.365017414093018 | KNN Loss: 4.341902256011963 | BCE Loss: 1.0231152772903442\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 5.394940376281738 | KNN Loss: 4.358292579650879 | BCE Loss: 1.0366475582122803\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 5.383864402770996 | KNN Loss: 4.352139472961426 | BCE Loss: 1.0317251682281494\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 5.386509895324707 | KNN Loss: 4.3697309494018555 | BCE Loss: 1.0167791843414307\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 5.358582496643066 | KNN Loss: 4.343691825866699 | BCE Loss: 1.0148906707763672\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 5.462427139282227 | KNN Loss: 4.415624141693115 | BCE Loss: 1.0468032360076904\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 5.339253902435303 | KNN Loss: 4.3323869705200195 | BCE Loss: 1.0068669319152832\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 5.361945629119873 | KNN Loss: 4.317136764526367 | BCE Loss: 1.0448088645935059\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 5.424983978271484 | KNN Loss: 4.401501655578613 | BCE Loss: 1.023482084274292\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 5.370781898498535 | KNN Loss: 4.332433700561523 | BCE Loss: 1.0383484363555908\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 5.371002197265625 | KNN Loss: 4.3485541343688965 | BCE Loss: 1.0224483013153076\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 5.4372100830078125 | KNN Loss: 4.3858513832092285 | BCE Loss: 1.0513588190078735\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 5.353581428527832 | KNN Loss: 4.341397285461426 | BCE Loss: 1.0121843814849854\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 5.339153289794922 | KNN Loss: 4.33670711517334 | BCE Loss: 1.002446174621582\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 5.390222549438477 | KNN Loss: 4.3396315574646 | BCE Loss: 1.0505911111831665\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 5.424116134643555 | KNN Loss: 4.344648361206055 | BCE Loss: 1.079468011856079\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 5.406371116638184 | KNN Loss: 4.345870018005371 | BCE Loss: 1.0605010986328125\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 5.37470817565918 | KNN Loss: 4.333130359649658 | BCE Loss: 1.041577935218811\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 5.41139030456543 | KNN Loss: 4.354715824127197 | BCE Loss: 1.056674599647522\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 5.359699726104736 | KNN Loss: 4.337584495544434 | BCE Loss: 1.0221151113510132\n",
      "Epoch   392: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 5.35981559753418 | KNN Loss: 4.365540504455566 | BCE Loss: 0.9942750334739685\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 5.375855922698975 | KNN Loss: 4.3339457511901855 | BCE Loss: 1.0419102907180786\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 5.417056560516357 | KNN Loss: 4.405179023742676 | BCE Loss: 1.0118776559829712\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 5.416494369506836 | KNN Loss: 4.369254112243652 | BCE Loss: 1.0472402572631836\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 5.340597629547119 | KNN Loss: 4.328351974487305 | BCE Loss: 1.012245535850525\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 5.337648391723633 | KNN Loss: 4.351023197174072 | BCE Loss: 0.9866249561309814\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 5.418851375579834 | KNN Loss: 4.3691020011901855 | BCE Loss: 1.0497493743896484\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 5.377310752868652 | KNN Loss: 4.3360276222229 | BCE Loss: 1.041283130645752\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 5.381719589233398 | KNN Loss: 4.3576340675354 | BCE Loss: 1.024085521697998\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 5.380136013031006 | KNN Loss: 4.332905292510986 | BCE Loss: 1.0472307205200195\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 5.365682125091553 | KNN Loss: 4.341667175292969 | BCE Loss: 1.0240148305892944\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 5.352769374847412 | KNN Loss: 4.329127788543701 | BCE Loss: 1.023641586303711\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 5.3652777671813965 | KNN Loss: 4.3253493309021 | BCE Loss: 1.0399285554885864\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 5.408121109008789 | KNN Loss: 4.4021430015563965 | BCE Loss: 1.005977988243103\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 5.392795562744141 | KNN Loss: 4.350338459014893 | BCE Loss: 1.042457103729248\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 5.384350776672363 | KNN Loss: 4.3703179359436035 | BCE Loss: 1.0140326023101807\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 5.382135391235352 | KNN Loss: 4.3306779861450195 | BCE Loss: 1.051457166671753\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 5.3525543212890625 | KNN Loss: 4.3195600509643555 | BCE Loss: 1.032994270324707\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 5.379415512084961 | KNN Loss: 4.336320400238037 | BCE Loss: 1.0430951118469238\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 5.362639904022217 | KNN Loss: 4.345684051513672 | BCE Loss: 1.0169559717178345\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 5.348484516143799 | KNN Loss: 4.33037805557251 | BCE Loss: 1.0181065797805786\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 5.373658657073975 | KNN Loss: 4.369089126586914 | BCE Loss: 1.0045695304870605\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 5.371315956115723 | KNN Loss: 4.354537010192871 | BCE Loss: 1.0167791843414307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 5.393837928771973 | KNN Loss: 4.379480361938477 | BCE Loss: 1.014357566833496\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 5.371673583984375 | KNN Loss: 4.357054233551025 | BCE Loss: 1.0146191120147705\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 5.400928497314453 | KNN Loss: 4.375899791717529 | BCE Loss: 1.025028944015503\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 5.381948471069336 | KNN Loss: 4.352011203765869 | BCE Loss: 1.0299371480941772\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 5.386092185974121 | KNN Loss: 4.353244304656982 | BCE Loss: 1.0328478813171387\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 5.378301620483398 | KNN Loss: 4.3560686111450195 | BCE Loss: 1.0222328901290894\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 5.376584529876709 | KNN Loss: 4.3347487449646 | BCE Loss: 1.0418356657028198\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 5.3806047439575195 | KNN Loss: 4.353725433349609 | BCE Loss: 1.0268793106079102\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 5.370244026184082 | KNN Loss: 4.334698677062988 | BCE Loss: 1.0355451107025146\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 5.383111953735352 | KNN Loss: 4.332148551940918 | BCE Loss: 1.0509631633758545\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 5.404770851135254 | KNN Loss: 4.356363773345947 | BCE Loss: 1.0484071969985962\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 5.370477676391602 | KNN Loss: 4.347680568695068 | BCE Loss: 1.0227969884872437\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 5.380365371704102 | KNN Loss: 4.347590923309326 | BCE Loss: 1.0327743291854858\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 5.370339393615723 | KNN Loss: 4.324921607971191 | BCE Loss: 1.0454176664352417\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 5.352410316467285 | KNN Loss: 4.327356815338135 | BCE Loss: 1.0250533819198608\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 5.370067119598389 | KNN Loss: 4.368903160095215 | BCE Loss: 1.0011638402938843\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 5.347567558288574 | KNN Loss: 4.356098175048828 | BCE Loss: 0.9914695024490356\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 5.385526657104492 | KNN Loss: 4.367452621459961 | BCE Loss: 1.0180740356445312\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 5.354645729064941 | KNN Loss: 4.32505989074707 | BCE Loss: 1.0295860767364502\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 5.367586135864258 | KNN Loss: 4.352171897888184 | BCE Loss: 1.0154142379760742\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 5.433079719543457 | KNN Loss: 4.419230937957764 | BCE Loss: 1.0138490200042725\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 5.377769947052002 | KNN Loss: 4.3458757400512695 | BCE Loss: 1.0318940877914429\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 5.400747299194336 | KNN Loss: 4.369144439697266 | BCE Loss: 1.0316028594970703\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 5.375030994415283 | KNN Loss: 4.342088222503662 | BCE Loss: 1.0329426527023315\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 5.381443977355957 | KNN Loss: 4.3541951179504395 | BCE Loss: 1.0272486209869385\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 5.349437713623047 | KNN Loss: 4.334531307220459 | BCE Loss: 1.0149061679840088\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 5.399848461151123 | KNN Loss: 4.367854118347168 | BCE Loss: 1.031994342803955\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 5.3565802574157715 | KNN Loss: 4.352173328399658 | BCE Loss: 1.0044070482254028\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 5.369228839874268 | KNN Loss: 4.3567891120910645 | BCE Loss: 1.0124396085739136\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 5.360739707946777 | KNN Loss: 4.338170051574707 | BCE Loss: 1.0225698947906494\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 5.419320106506348 | KNN Loss: 4.367189407348633 | BCE Loss: 1.052130937576294\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 5.412118911743164 | KNN Loss: 4.382991790771484 | BCE Loss: 1.0291271209716797\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 5.389967918395996 | KNN Loss: 4.339187145233154 | BCE Loss: 1.0507805347442627\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 5.416390419006348 | KNN Loss: 4.369915008544922 | BCE Loss: 1.0464755296707153\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 5.350985527038574 | KNN Loss: 4.3379340171813965 | BCE Loss: 1.0130513906478882\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 5.359570503234863 | KNN Loss: 4.372491836547852 | BCE Loss: 0.9870786666870117\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 5.444163799285889 | KNN Loss: 4.404731273651123 | BCE Loss: 1.0394325256347656\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 5.426665306091309 | KNN Loss: 4.388076305389404 | BCE Loss: 1.0385892391204834\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 5.412565231323242 | KNN Loss: 4.375327110290527 | BCE Loss: 1.037238359451294\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 5.367132186889648 | KNN Loss: 4.345551490783691 | BCE Loss: 1.0215809345245361\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 5.4097208976745605 | KNN Loss: 4.3599653244018555 | BCE Loss: 1.049755573272705\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 5.354033470153809 | KNN Loss: 4.3299994468688965 | BCE Loss: 1.0240342617034912\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 5.375064849853516 | KNN Loss: 4.334598064422607 | BCE Loss: 1.0404667854309082\n",
      "Epoch   403: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 5.432167053222656 | KNN Loss: 4.39103364944458 | BCE Loss: 1.0411332845687866\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 5.390418529510498 | KNN Loss: 4.350916862487793 | BCE Loss: 1.0395017862319946\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 5.412213325500488 | KNN Loss: 4.36333703994751 | BCE Loss: 1.0488762855529785\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 5.357155799865723 | KNN Loss: 4.3345513343811035 | BCE Loss: 1.0226043462753296\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 5.369472980499268 | KNN Loss: 4.345465660095215 | BCE Loss: 1.0240073204040527\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 5.368929386138916 | KNN Loss: 4.336455821990967 | BCE Loss: 1.0324734449386597\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 5.446977615356445 | KNN Loss: 4.395089626312256 | BCE Loss: 1.0518878698349\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 5.410730361938477 | KNN Loss: 4.350565433502197 | BCE Loss: 1.0601649284362793\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 5.367481231689453 | KNN Loss: 4.351606369018555 | BCE Loss: 1.0158751010894775\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 5.349453926086426 | KNN Loss: 4.339331150054932 | BCE Loss: 1.0101227760314941\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 5.389531135559082 | KNN Loss: 4.35259485244751 | BCE Loss: 1.0369365215301514\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 5.368447303771973 | KNN Loss: 4.330947399139404 | BCE Loss: 1.0374999046325684\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 5.403754234313965 | KNN Loss: 4.393836498260498 | BCE Loss: 1.009917974472046\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 5.376400947570801 | KNN Loss: 4.3349609375 | BCE Loss: 1.0414398908615112\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 5.338004112243652 | KNN Loss: 4.329018592834473 | BCE Loss: 1.0089852809906006\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 5.367933750152588 | KNN Loss: 4.344803810119629 | BCE Loss: 1.023129940032959\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 5.426058292388916 | KNN Loss: 4.405229568481445 | BCE Loss: 1.0208287239074707\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 5.394598960876465 | KNN Loss: 4.369757652282715 | BCE Loss: 1.02484130859375\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 5.345442771911621 | KNN Loss: 4.315162658691406 | BCE Loss: 1.0302798748016357\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 5.440603256225586 | KNN Loss: 4.417941570281982 | BCE Loss: 1.0226616859436035\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 5.4083075523376465 | KNN Loss: 4.378972053527832 | BCE Loss: 1.0293354988098145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 5.3968095779418945 | KNN Loss: 4.369535446166992 | BCE Loss: 1.0272743701934814\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 5.375361442565918 | KNN Loss: 4.363291263580322 | BCE Loss: 1.0120701789855957\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 5.391444683074951 | KNN Loss: 4.34165620803833 | BCE Loss: 1.0497885942459106\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 5.363994598388672 | KNN Loss: 4.335515975952148 | BCE Loss: 1.0284783840179443\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 5.3449296951293945 | KNN Loss: 4.374381065368652 | BCE Loss: 0.970548689365387\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 5.374638080596924 | KNN Loss: 4.353221893310547 | BCE Loss: 1.021416187286377\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 5.3875627517700195 | KNN Loss: 4.3436174392700195 | BCE Loss: 1.043945074081421\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 5.354793548583984 | KNN Loss: 4.34135103225708 | BCE Loss: 1.0134422779083252\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 5.382631778717041 | KNN Loss: 4.340567111968994 | BCE Loss: 1.0420646667480469\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 5.37869930267334 | KNN Loss: 4.3526153564453125 | BCE Loss: 1.0260837078094482\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 5.354954719543457 | KNN Loss: 4.33604097366333 | BCE Loss: 1.0189135074615479\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 5.384463787078857 | KNN Loss: 4.376251220703125 | BCE Loss: 1.0082125663757324\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 5.360337257385254 | KNN Loss: 4.326979160308838 | BCE Loss: 1.033358097076416\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 5.354345321655273 | KNN Loss: 4.318661212921143 | BCE Loss: 1.0356841087341309\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 5.384282112121582 | KNN Loss: 4.352689266204834 | BCE Loss: 1.031592607498169\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 5.402132511138916 | KNN Loss: 4.35828971862793 | BCE Loss: 1.0438426733016968\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 5.347835540771484 | KNN Loss: 4.3188700675964355 | BCE Loss: 1.028965711593628\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 5.370508670806885 | KNN Loss: 4.3666510581970215 | BCE Loss: 1.0038576126098633\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 5.368771553039551 | KNN Loss: 4.323851108551025 | BCE Loss: 1.0449202060699463\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 5.386435508728027 | KNN Loss: 4.372183322906494 | BCE Loss: 1.014251947402954\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 5.377518177032471 | KNN Loss: 4.346468448638916 | BCE Loss: 1.0310497283935547\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 5.3750200271606445 | KNN Loss: 4.340283393859863 | BCE Loss: 1.0347368717193604\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 5.400181770324707 | KNN Loss: 4.380633354187012 | BCE Loss: 1.0195482969284058\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 5.407960891723633 | KNN Loss: 4.377838611602783 | BCE Loss: 1.0301223993301392\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 5.351179599761963 | KNN Loss: 4.340603351593018 | BCE Loss: 1.0105762481689453\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 5.379168510437012 | KNN Loss: 4.337822914123535 | BCE Loss: 1.0413453578948975\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 5.384373664855957 | KNN Loss: 4.345437526702881 | BCE Loss: 1.0389361381530762\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 5.395554542541504 | KNN Loss: 4.328512668609619 | BCE Loss: 1.0670417547225952\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 5.36027193069458 | KNN Loss: 4.363635540008545 | BCE Loss: 0.9966365098953247\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 5.357612609863281 | KNN Loss: 4.36111307144165 | BCE Loss: 0.9964996576309204\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 5.378837585449219 | KNN Loss: 4.346590042114258 | BCE Loss: 1.0322473049163818\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 5.412110328674316 | KNN Loss: 4.363140106201172 | BCE Loss: 1.0489699840545654\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 5.360150337219238 | KNN Loss: 4.33678674697876 | BCE Loss: 1.0233633518218994\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 5.369848251342773 | KNN Loss: 4.3481621742248535 | BCE Loss: 1.02168607711792\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 5.348776340484619 | KNN Loss: 4.333868026733398 | BCE Loss: 1.0149084329605103\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 5.387923240661621 | KNN Loss: 4.3324713706970215 | BCE Loss: 1.05545175075531\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 5.4044294357299805 | KNN Loss: 4.367342472076416 | BCE Loss: 1.0370867252349854\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 5.357172966003418 | KNN Loss: 4.32681941986084 | BCE Loss: 1.0303537845611572\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 5.384357452392578 | KNN Loss: 4.358240127563477 | BCE Loss: 1.0261175632476807\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 5.358409881591797 | KNN Loss: 4.325040340423584 | BCE Loss: 1.0333693027496338\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 5.410134792327881 | KNN Loss: 4.352389335632324 | BCE Loss: 1.0577454566955566\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 5.42345666885376 | KNN Loss: 4.392556190490723 | BCE Loss: 1.030900478363037\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 5.355909824371338 | KNN Loss: 4.354948997497559 | BCE Loss: 1.0009608268737793\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 5.372307777404785 | KNN Loss: 4.3280158042907715 | BCE Loss: 1.0442922115325928\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 5.373631477355957 | KNN Loss: 4.3289103507995605 | BCE Loss: 1.044721245765686\n",
      "Epoch   414: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 5.375520706176758 | KNN Loss: 4.35891580581665 | BCE Loss: 1.0166046619415283\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 5.357753753662109 | KNN Loss: 4.331905364990234 | BCE Loss: 1.025848627090454\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 5.372322082519531 | KNN Loss: 4.34306001663208 | BCE Loss: 1.0292621850967407\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 5.37733268737793 | KNN Loss: 4.341590404510498 | BCE Loss: 1.0357425212860107\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 5.433625221252441 | KNN Loss: 4.383403778076172 | BCE Loss: 1.0502216815948486\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 5.315402984619141 | KNN Loss: 4.331857681274414 | BCE Loss: 0.9835450649261475\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 5.381720066070557 | KNN Loss: 4.351302623748779 | BCE Loss: 1.0304174423217773\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 5.394942760467529 | KNN Loss: 4.349013328552246 | BCE Loss: 1.0459294319152832\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 5.357239723205566 | KNN Loss: 4.326526641845703 | BCE Loss: 1.0307133197784424\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 5.350100517272949 | KNN Loss: 4.3200788497924805 | BCE Loss: 1.0300216674804688\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 5.334001064300537 | KNN Loss: 4.325577735900879 | BCE Loss: 1.0084233283996582\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 5.368033409118652 | KNN Loss: 4.348000526428223 | BCE Loss: 1.0200326442718506\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 5.387487411499023 | KNN Loss: 4.3697590827941895 | BCE Loss: 1.017728328704834\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 5.438379287719727 | KNN Loss: 4.383975028991699 | BCE Loss: 1.054404377937317\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 5.357907772064209 | KNN Loss: 4.341804027557373 | BCE Loss: 1.016103744506836\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 5.344987869262695 | KNN Loss: 4.332019805908203 | BCE Loss: 1.0129680633544922\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 5.341240882873535 | KNN Loss: 4.326172828674316 | BCE Loss: 1.0150678157806396\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 5.379022598266602 | KNN Loss: 4.3334760665893555 | BCE Loss: 1.045546531677246\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 5.393487453460693 | KNN Loss: 4.369402885437012 | BCE Loss: 1.0240845680236816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 5.3681535720825195 | KNN Loss: 4.3369598388671875 | BCE Loss: 1.0311938524246216\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 5.4168701171875 | KNN Loss: 4.372065544128418 | BCE Loss: 1.044804334640503\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 5.358777046203613 | KNN Loss: 4.340908527374268 | BCE Loss: 1.0178682804107666\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 5.35737419128418 | KNN Loss: 4.336169242858887 | BCE Loss: 1.0212047100067139\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 5.38026237487793 | KNN Loss: 4.347719669342041 | BCE Loss: 1.0325424671173096\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 5.374818801879883 | KNN Loss: 4.34874153137207 | BCE Loss: 1.0260775089263916\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 5.364240646362305 | KNN Loss: 4.340979099273682 | BCE Loss: 1.0232617855072021\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 5.40300989151001 | KNN Loss: 4.382999420166016 | BCE Loss: 1.0200105905532837\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 5.401426792144775 | KNN Loss: 4.35479211807251 | BCE Loss: 1.0466346740722656\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 5.349103927612305 | KNN Loss: 4.33359956741333 | BCE Loss: 1.015504240989685\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 5.387124061584473 | KNN Loss: 4.379991054534912 | BCE Loss: 1.0071332454681396\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 5.345078468322754 | KNN Loss: 4.351494789123535 | BCE Loss: 0.9935837388038635\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 5.357926368713379 | KNN Loss: 4.337007522583008 | BCE Loss: 1.0209190845489502\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 5.381291389465332 | KNN Loss: 4.324319362640381 | BCE Loss: 1.0569720268249512\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 5.343074321746826 | KNN Loss: 4.3476409912109375 | BCE Loss: 0.9954332113265991\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 5.377885818481445 | KNN Loss: 4.339252471923828 | BCE Loss: 1.038633108139038\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 5.386557579040527 | KNN Loss: 4.373820781707764 | BCE Loss: 1.0127370357513428\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 5.35158634185791 | KNN Loss: 4.332600116729736 | BCE Loss: 1.018986463546753\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 5.335698127746582 | KNN Loss: 4.326621055603027 | BCE Loss: 1.0090773105621338\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 5.3573102951049805 | KNN Loss: 4.334301948547363 | BCE Loss: 1.0230084657669067\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 5.364487171173096 | KNN Loss: 4.343981742858887 | BCE Loss: 1.020505428314209\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 5.325268745422363 | KNN Loss: 4.328436374664307 | BCE Loss: 0.9968324899673462\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 5.380618572235107 | KNN Loss: 4.356285095214844 | BCE Loss: 1.0243334770202637\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 5.386086940765381 | KNN Loss: 4.353240013122559 | BCE Loss: 1.0328469276428223\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 5.354445457458496 | KNN Loss: 4.327528476715088 | BCE Loss: 1.026916742324829\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 5.372258186340332 | KNN Loss: 4.342658042907715 | BCE Loss: 1.0296001434326172\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 5.445983409881592 | KNN Loss: 4.38334846496582 | BCE Loss: 1.0626349449157715\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 5.353036880493164 | KNN Loss: 4.331712245941162 | BCE Loss: 1.021324634552002\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 5.371846675872803 | KNN Loss: 4.327266216278076 | BCE Loss: 1.0445804595947266\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 5.376645088195801 | KNN Loss: 4.3516435623168945 | BCE Loss: 1.0250014066696167\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 5.356496810913086 | KNN Loss: 4.334551811218262 | BCE Loss: 1.0219452381134033\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 5.407661437988281 | KNN Loss: 4.402235507965088 | BCE Loss: 1.0054259300231934\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 5.399401664733887 | KNN Loss: 4.373816013336182 | BCE Loss: 1.025585651397705\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 5.37283992767334 | KNN Loss: 4.340357303619385 | BCE Loss: 1.0324828624725342\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 5.330263137817383 | KNN Loss: 4.329270839691162 | BCE Loss: 1.0009925365447998\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 5.347207069396973 | KNN Loss: 4.327771186828613 | BCE Loss: 1.0194361209869385\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 5.38161563873291 | KNN Loss: 4.35632848739624 | BCE Loss: 1.0252870321273804\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 5.384636402130127 | KNN Loss: 4.342525005340576 | BCE Loss: 1.0421115159988403\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 5.364836692810059 | KNN Loss: 4.346310615539551 | BCE Loss: 1.0185260772705078\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 5.443859100341797 | KNN Loss: 4.391208171844482 | BCE Loss: 1.0526506900787354\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 5.3547868728637695 | KNN Loss: 4.31759786605835 | BCE Loss: 1.0371891260147095\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 5.400552272796631 | KNN Loss: 4.350090980529785 | BCE Loss: 1.0504612922668457\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 5.350090980529785 | KNN Loss: 4.34525728225708 | BCE Loss: 1.004833698272705\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 5.368840217590332 | KNN Loss: 4.343162536621094 | BCE Loss: 1.0256775617599487\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 5.411385536193848 | KNN Loss: 4.379937648773193 | BCE Loss: 1.0314480066299438\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 5.4075164794921875 | KNN Loss: 4.367217063903809 | BCE Loss: 1.040299415588379\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 5.4028143882751465 | KNN Loss: 4.372768878936768 | BCE Loss: 1.030045509338379\n",
      "Epoch   425: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 5.355217933654785 | KNN Loss: 4.338398456573486 | BCE Loss: 1.016819715499878\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 5.340371131896973 | KNN Loss: 4.310947895050049 | BCE Loss: 1.0294231176376343\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 5.357481002807617 | KNN Loss: 4.351495742797852 | BCE Loss: 1.0059850215911865\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 5.381649971008301 | KNN Loss: 4.329880237579346 | BCE Loss: 1.0517698526382446\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 5.376471519470215 | KNN Loss: 4.347248077392578 | BCE Loss: 1.0292236804962158\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 5.374997615814209 | KNN Loss: 4.342722415924072 | BCE Loss: 1.0322751998901367\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 5.375240802764893 | KNN Loss: 4.3516716957092285 | BCE Loss: 1.023569107055664\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 5.386040687561035 | KNN Loss: 4.3627424240112305 | BCE Loss: 1.0232982635498047\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 5.374509811401367 | KNN Loss: 4.3375630378723145 | BCE Loss: 1.0369467735290527\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 5.347256660461426 | KNN Loss: 4.332762718200684 | BCE Loss: 1.0144938230514526\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 5.36257791519165 | KNN Loss: 4.327856540679932 | BCE Loss: 1.0347214937210083\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 5.3650593757629395 | KNN Loss: 4.347804069519043 | BCE Loss: 1.017255187034607\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 5.3982439041137695 | KNN Loss: 4.357203006744385 | BCE Loss: 1.0410407781600952\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 5.358818054199219 | KNN Loss: 4.349428653717041 | BCE Loss: 1.0093894004821777\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 5.3400068283081055 | KNN Loss: 4.311375141143799 | BCE Loss: 1.0286314487457275\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 5.3688578605651855 | KNN Loss: 4.331029891967773 | BCE Loss: 1.0378280878067017\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 5.377103328704834 | KNN Loss: 4.355728626251221 | BCE Loss: 1.0213747024536133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 5.339339733123779 | KNN Loss: 4.328603744506836 | BCE Loss: 1.0107359886169434\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 5.359095573425293 | KNN Loss: 4.332515239715576 | BCE Loss: 1.0265803337097168\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 5.374670505523682 | KNN Loss: 4.3401665687561035 | BCE Loss: 1.0345039367675781\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 5.368521690368652 | KNN Loss: 4.332732677459717 | BCE Loss: 1.0357887744903564\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 5.359218120574951 | KNN Loss: 4.345033645629883 | BCE Loss: 1.0141844749450684\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 5.356226921081543 | KNN Loss: 4.326531410217285 | BCE Loss: 1.0296955108642578\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 5.3976149559021 | KNN Loss: 4.345983982086182 | BCE Loss: 1.0516308546066284\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 5.468920707702637 | KNN Loss: 4.413146495819092 | BCE Loss: 1.0557739734649658\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 5.339693546295166 | KNN Loss: 4.358836650848389 | BCE Loss: 0.9808568954467773\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 5.366933822631836 | KNN Loss: 4.350581169128418 | BCE Loss: 1.016352653503418\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 5.354918003082275 | KNN Loss: 4.35782527923584 | BCE Loss: 0.9970929026603699\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 5.398203372955322 | KNN Loss: 4.383678436279297 | BCE Loss: 1.0145249366760254\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 5.351255416870117 | KNN Loss: 4.331197261810303 | BCE Loss: 1.0200581550598145\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 5.35991096496582 | KNN Loss: 4.332902431488037 | BCE Loss: 1.0270085334777832\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 5.388399600982666 | KNN Loss: 4.360674858093262 | BCE Loss: 1.0277248620986938\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 5.414982795715332 | KNN Loss: 4.354451656341553 | BCE Loss: 1.0605311393737793\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 5.3838605880737305 | KNN Loss: 4.350436210632324 | BCE Loss: 1.0334243774414062\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 5.365057945251465 | KNN Loss: 4.3387651443481445 | BCE Loss: 1.0262930393218994\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 5.349499702453613 | KNN Loss: 4.3232951164245605 | BCE Loss: 1.0262044668197632\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 5.363860130310059 | KNN Loss: 4.327374458312988 | BCE Loss: 1.0364856719970703\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 5.372925758361816 | KNN Loss: 4.351568222045898 | BCE Loss: 1.021357774734497\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 5.425269603729248 | KNN Loss: 4.373390197753906 | BCE Loss: 1.0518794059753418\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 5.457008361816406 | KNN Loss: 4.399798393249512 | BCE Loss: 1.057210087776184\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 5.3578782081604 | KNN Loss: 4.330254077911377 | BCE Loss: 1.0276240110397339\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 5.362454891204834 | KNN Loss: 4.331516265869141 | BCE Loss: 1.0309386253356934\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 5.36043643951416 | KNN Loss: 4.351935863494873 | BCE Loss: 1.0085006952285767\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 5.424644470214844 | KNN Loss: 4.379080772399902 | BCE Loss: 1.0455639362335205\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 5.393627166748047 | KNN Loss: 4.3567070960998535 | BCE Loss: 1.0369203090667725\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 5.3625288009643555 | KNN Loss: 4.351076602935791 | BCE Loss: 1.0114524364471436\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 5.37037992477417 | KNN Loss: 4.3404974937438965 | BCE Loss: 1.0298824310302734\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 5.399454593658447 | KNN Loss: 4.362408638000488 | BCE Loss: 1.037045955657959\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 5.321982383728027 | KNN Loss: 4.322967052459717 | BCE Loss: 0.9990151524543762\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 5.379245281219482 | KNN Loss: 4.354925632476807 | BCE Loss: 1.0243195295333862\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 5.347836971282959 | KNN Loss: 4.338449478149414 | BCE Loss: 1.0093876123428345\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 5.363392353057861 | KNN Loss: 4.334015369415283 | BCE Loss: 1.0293769836425781\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 5.406264781951904 | KNN Loss: 4.36747407913208 | BCE Loss: 1.0387908220291138\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 5.300085544586182 | KNN Loss: 4.320484638214111 | BCE Loss: 0.9796010255813599\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 5.3491692543029785 | KNN Loss: 4.329695224761963 | BCE Loss: 1.0194740295410156\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 5.368463516235352 | KNN Loss: 4.347857475280762 | BCE Loss: 1.0206058025360107\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 5.378823280334473 | KNN Loss: 4.338916301727295 | BCE Loss: 1.0399069786071777\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 5.411691188812256 | KNN Loss: 4.357030391693115 | BCE Loss: 1.0546607971191406\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 5.433110237121582 | KNN Loss: 4.412301063537598 | BCE Loss: 1.0208094120025635\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 5.379079818725586 | KNN Loss: 4.35415506362915 | BCE Loss: 1.0249249935150146\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 5.376220703125 | KNN Loss: 4.341343402862549 | BCE Loss: 1.0348775386810303\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 5.411998748779297 | KNN Loss: 4.351357936859131 | BCE Loss: 1.060640811920166\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 5.409889221191406 | KNN Loss: 4.3605499267578125 | BCE Loss: 1.0493390560150146\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 5.391254425048828 | KNN Loss: 4.353037357330322 | BCE Loss: 1.0382171869277954\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 5.352840900421143 | KNN Loss: 4.356931686401367 | BCE Loss: 0.9959092736244202\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 5.392080307006836 | KNN Loss: 4.370136737823486 | BCE Loss: 1.02194344997406\n",
      "Epoch   436: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 5.374642372131348 | KNN Loss: 4.351260185241699 | BCE Loss: 1.0233824253082275\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 5.417145729064941 | KNN Loss: 4.363245487213135 | BCE Loss: 1.0539002418518066\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 5.3829755783081055 | KNN Loss: 4.337983131408691 | BCE Loss: 1.0449926853179932\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 5.349493980407715 | KNN Loss: 4.330190181732178 | BCE Loss: 1.019303560256958\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 5.38578462600708 | KNN Loss: 4.350220203399658 | BCE Loss: 1.0355644226074219\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 5.377964973449707 | KNN Loss: 4.332200050354004 | BCE Loss: 1.0457650423049927\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 5.35331916809082 | KNN Loss: 4.3287200927734375 | BCE Loss: 1.0245990753173828\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 5.381854057312012 | KNN Loss: 4.347416400909424 | BCE Loss: 1.034437894821167\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 5.383991241455078 | KNN Loss: 4.335825443267822 | BCE Loss: 1.0481657981872559\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 5.406961441040039 | KNN Loss: 4.369452476501465 | BCE Loss: 1.0375087261199951\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 5.361974716186523 | KNN Loss: 4.338122367858887 | BCE Loss: 1.0238525867462158\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 5.334811210632324 | KNN Loss: 4.328366756439209 | BCE Loss: 1.0064446926116943\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 5.454225063323975 | KNN Loss: 4.416014194488525 | BCE Loss: 1.0382108688354492\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 5.385153770446777 | KNN Loss: 4.351919651031494 | BCE Loss: 1.0332341194152832\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 5.360720157623291 | KNN Loss: 4.343770980834961 | BCE Loss: 1.01694917678833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 5.383783340454102 | KNN Loss: 4.346730709075928 | BCE Loss: 1.0370525121688843\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 5.375587463378906 | KNN Loss: 4.350536346435547 | BCE Loss: 1.0250508785247803\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 5.395153522491455 | KNN Loss: 4.3513336181640625 | BCE Loss: 1.0438199043273926\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 5.381987571716309 | KNN Loss: 4.383342266082764 | BCE Loss: 0.9986453652381897\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 5.3755598068237305 | KNN Loss: 4.357601165771484 | BCE Loss: 1.017958641052246\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 5.335978031158447 | KNN Loss: 4.339078426361084 | BCE Loss: 0.9968996047973633\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 5.366513252258301 | KNN Loss: 4.342787265777588 | BCE Loss: 1.023726224899292\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 5.38269567489624 | KNN Loss: 4.350576877593994 | BCE Loss: 1.0321186780929565\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 5.399192810058594 | KNN Loss: 4.347136497497559 | BCE Loss: 1.0520565509796143\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 5.355648994445801 | KNN Loss: 4.352338790893555 | BCE Loss: 1.003309965133667\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 5.3775224685668945 | KNN Loss: 4.339166164398193 | BCE Loss: 1.0383564233779907\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 5.351456642150879 | KNN Loss: 4.329810619354248 | BCE Loss: 1.0216457843780518\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 5.380938529968262 | KNN Loss: 4.33935022354126 | BCE Loss: 1.041588306427002\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 5.385453224182129 | KNN Loss: 4.331562519073486 | BCE Loss: 1.053890585899353\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 5.356122970581055 | KNN Loss: 4.354326248168945 | BCE Loss: 1.001796841621399\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 5.337368965148926 | KNN Loss: 4.3343706130981445 | BCE Loss: 1.0029981136322021\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 5.416918754577637 | KNN Loss: 4.3598103523254395 | BCE Loss: 1.0571081638336182\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 5.3822126388549805 | KNN Loss: 4.366729736328125 | BCE Loss: 1.0154826641082764\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 5.417513847351074 | KNN Loss: 4.363661289215088 | BCE Loss: 1.0538523197174072\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 5.419482231140137 | KNN Loss: 4.36223840713501 | BCE Loss: 1.057243824005127\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 5.3692803382873535 | KNN Loss: 4.339900970458984 | BCE Loss: 1.0293792486190796\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 5.389226913452148 | KNN Loss: 4.344290256500244 | BCE Loss: 1.0449366569519043\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 5.402438163757324 | KNN Loss: 4.368658542633057 | BCE Loss: 1.0337797403335571\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 5.361152172088623 | KNN Loss: 4.3585920333862305 | BCE Loss: 1.0025601387023926\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 5.349524021148682 | KNN Loss: 4.342191219329834 | BCE Loss: 1.0073328018188477\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 5.35417366027832 | KNN Loss: 4.33266019821167 | BCE Loss: 1.0215134620666504\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 5.375113487243652 | KNN Loss: 4.346354007720947 | BCE Loss: 1.028759241104126\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 5.377251148223877 | KNN Loss: 4.3387675285339355 | BCE Loss: 1.038483738899231\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 5.387538909912109 | KNN Loss: 4.338374137878418 | BCE Loss: 1.0491647720336914\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 5.348383903503418 | KNN Loss: 4.344911098480225 | BCE Loss: 1.0034728050231934\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 5.352982997894287 | KNN Loss: 4.34468936920166 | BCE Loss: 1.0082937479019165\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 5.3622050285339355 | KNN Loss: 4.344832420349121 | BCE Loss: 1.0173726081848145\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 5.344991683959961 | KNN Loss: 4.324184894561768 | BCE Loss: 1.0208066701889038\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 5.354376792907715 | KNN Loss: 4.344590187072754 | BCE Loss: 1.0097863674163818\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 5.386043071746826 | KNN Loss: 4.346288204193115 | BCE Loss: 1.039754867553711\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 5.432437896728516 | KNN Loss: 4.399613380432129 | BCE Loss: 1.0328243970870972\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 5.481557846069336 | KNN Loss: 4.466858863830566 | BCE Loss: 1.0146987438201904\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 5.366271018981934 | KNN Loss: 4.337333679199219 | BCE Loss: 1.028937578201294\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 5.368178367614746 | KNN Loss: 4.3579840660095215 | BCE Loss: 1.0101943016052246\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 5.369167804718018 | KNN Loss: 4.331745624542236 | BCE Loss: 1.0374220609664917\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 5.403393745422363 | KNN Loss: 4.357701778411865 | BCE Loss: 1.045691728591919\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 5.378220081329346 | KNN Loss: 4.341052055358887 | BCE Loss: 1.0371679067611694\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 5.397937774658203 | KNN Loss: 4.3478546142578125 | BCE Loss: 1.0500829219818115\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 5.337103843688965 | KNN Loss: 4.324235916137695 | BCE Loss: 1.0128681659698486\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 5.36706018447876 | KNN Loss: 4.335350513458252 | BCE Loss: 1.0317096710205078\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 5.3627729415893555 | KNN Loss: 4.336181163787842 | BCE Loss: 1.0265916585922241\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 5.3481268882751465 | KNN Loss: 4.325760364532471 | BCE Loss: 1.0223666429519653\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 5.366921424865723 | KNN Loss: 4.373962879180908 | BCE Loss: 0.9929586052894592\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 5.362593650817871 | KNN Loss: 4.341017246246338 | BCE Loss: 1.021576166152954\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 5.384281158447266 | KNN Loss: 4.334282875061035 | BCE Loss: 1.04999840259552\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 5.390746116638184 | KNN Loss: 4.350362777709961 | BCE Loss: 1.0403831005096436\n",
      "Epoch   447: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 5.376897811889648 | KNN Loss: 4.333001613616943 | BCE Loss: 1.0438963174819946\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 5.383501052856445 | KNN Loss: 4.387800216674805 | BCE Loss: 0.9957008361816406\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 5.366687774658203 | KNN Loss: 4.329359531402588 | BCE Loss: 1.0373280048370361\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 5.355376243591309 | KNN Loss: 4.323237419128418 | BCE Loss: 1.0321385860443115\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 5.390379905700684 | KNN Loss: 4.367062091827393 | BCE Loss: 1.0233180522918701\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 5.350617408752441 | KNN Loss: 4.346133708953857 | BCE Loss: 1.004483938217163\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 5.365932464599609 | KNN Loss: 4.3431715965271 | BCE Loss: 1.0227607488632202\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 5.38929557800293 | KNN Loss: 4.343806266784668 | BCE Loss: 1.0454891920089722\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 5.3614349365234375 | KNN Loss: 4.341934680938721 | BCE Loss: 1.0195003747940063\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 5.411215305328369 | KNN Loss: 4.367760181427002 | BCE Loss: 1.0434551239013672\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 5.348753929138184 | KNN Loss: 4.341360569000244 | BCE Loss: 1.0073933601379395\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 5.390753269195557 | KNN Loss: 4.373346328735352 | BCE Loss: 1.0174068212509155\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 5.394260406494141 | KNN Loss: 4.345096588134766 | BCE Loss: 1.0491636991500854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 5.3634033203125 | KNN Loss: 4.333193778991699 | BCE Loss: 1.0302095413208008\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 5.404452800750732 | KNN Loss: 4.383240699768066 | BCE Loss: 1.0212122201919556\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 5.322530269622803 | KNN Loss: 4.326475143432617 | BCE Loss: 0.9960550665855408\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 5.410157203674316 | KNN Loss: 4.3770952224731445 | BCE Loss: 1.033062219619751\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 5.386936664581299 | KNN Loss: 4.351184844970703 | BCE Loss: 1.0357518196105957\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 5.3716936111450195 | KNN Loss: 4.333949089050293 | BCE Loss: 1.0377447605133057\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 5.333678722381592 | KNN Loss: 4.344855308532715 | BCE Loss: 0.9888232946395874\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 5.397200107574463 | KNN Loss: 4.3757147789001465 | BCE Loss: 1.0214853286743164\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 5.414599418640137 | KNN Loss: 4.384078502655029 | BCE Loss: 1.0305211544036865\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 5.3683271408081055 | KNN Loss: 4.345798969268799 | BCE Loss: 1.0225284099578857\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 5.377995491027832 | KNN Loss: 4.340175151824951 | BCE Loss: 1.03782057762146\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 5.403622627258301 | KNN Loss: 4.370703220367432 | BCE Loss: 1.0329195261001587\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 5.42767333984375 | KNN Loss: 4.378618240356445 | BCE Loss: 1.0490548610687256\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 5.356699466705322 | KNN Loss: 4.335837364196777 | BCE Loss: 1.0208619832992554\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 5.379640102386475 | KNN Loss: 4.360485553741455 | BCE Loss: 1.01915442943573\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 5.367774963378906 | KNN Loss: 4.341465950012207 | BCE Loss: 1.0263087749481201\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 5.346116065979004 | KNN Loss: 4.326054096221924 | BCE Loss: 1.020061731338501\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 5.379014492034912 | KNN Loss: 4.378424167633057 | BCE Loss: 1.000590443611145\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 5.389168739318848 | KNN Loss: 4.333427429199219 | BCE Loss: 1.055741548538208\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 5.370996952056885 | KNN Loss: 4.351872444152832 | BCE Loss: 1.0191246271133423\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 5.359389781951904 | KNN Loss: 4.34692907333374 | BCE Loss: 1.0124605894088745\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 5.379982948303223 | KNN Loss: 4.338076591491699 | BCE Loss: 1.0419063568115234\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 5.38065242767334 | KNN Loss: 4.3599772453308105 | BCE Loss: 1.0206754207611084\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 5.372361660003662 | KNN Loss: 4.359746932983398 | BCE Loss: 1.0126147270202637\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 5.37810754776001 | KNN Loss: 4.341642379760742 | BCE Loss: 1.0364651679992676\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 5.384100914001465 | KNN Loss: 4.334097385406494 | BCE Loss: 1.0500034093856812\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 5.386072635650635 | KNN Loss: 4.350485324859619 | BCE Loss: 1.0355873107910156\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 5.375328063964844 | KNN Loss: 4.349736213684082 | BCE Loss: 1.0255916118621826\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 5.369642734527588 | KNN Loss: 4.355191707611084 | BCE Loss: 1.014451026916504\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 5.3523712158203125 | KNN Loss: 4.333653450012207 | BCE Loss: 1.018717885017395\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 5.377960205078125 | KNN Loss: 4.342003345489502 | BCE Loss: 1.035956621170044\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 5.379910469055176 | KNN Loss: 4.3596954345703125 | BCE Loss: 1.0202152729034424\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 5.332951545715332 | KNN Loss: 4.326814651489258 | BCE Loss: 1.0061366558074951\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 5.361811637878418 | KNN Loss: 4.348416805267334 | BCE Loss: 1.0133945941925049\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 5.370805740356445 | KNN Loss: 4.338324546813965 | BCE Loss: 1.0324814319610596\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 5.382291793823242 | KNN Loss: 4.338223457336426 | BCE Loss: 1.0440680980682373\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 5.346340179443359 | KNN Loss: 4.331904411315918 | BCE Loss: 1.0144357681274414\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 5.3841681480407715 | KNN Loss: 4.325338840484619 | BCE Loss: 1.0588293075561523\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 5.360034942626953 | KNN Loss: 4.328746795654297 | BCE Loss: 1.0312879085540771\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 5.37097692489624 | KNN Loss: 4.3206562995910645 | BCE Loss: 1.0503206253051758\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 5.368708610534668 | KNN Loss: 4.362970352172852 | BCE Loss: 1.0057384967803955\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 5.415440082550049 | KNN Loss: 4.367358207702637 | BCE Loss: 1.048081874847412\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 5.3814616203308105 | KNN Loss: 4.33929967880249 | BCE Loss: 1.0421618223190308\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 5.358710289001465 | KNN Loss: 4.364292621612549 | BCE Loss: 0.9944178462028503\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 5.364934921264648 | KNN Loss: 4.356835842132568 | BCE Loss: 1.008098840713501\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 5.368535995483398 | KNN Loss: 4.340164661407471 | BCE Loss: 1.0283715724945068\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 5.352068901062012 | KNN Loss: 4.315844535827637 | BCE Loss: 1.0362242460250854\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 5.385732650756836 | KNN Loss: 4.336319923400879 | BCE Loss: 1.0494129657745361\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 5.404229640960693 | KNN Loss: 4.38555383682251 | BCE Loss: 1.018675684928894\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 5.375239372253418 | KNN Loss: 4.343326091766357 | BCE Loss: 1.0319135189056396\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 5.37550687789917 | KNN Loss: 4.343227863311768 | BCE Loss: 1.032279133796692\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 5.36972713470459 | KNN Loss: 4.360879421234131 | BCE Loss: 1.008847951889038\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 5.350032806396484 | KNN Loss: 4.331335544586182 | BCE Loss: 1.0186972618103027\n",
      "Epoch   458: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 5.3748979568481445 | KNN Loss: 4.338287353515625 | BCE Loss: 1.0366108417510986\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 5.353930950164795 | KNN Loss: 4.327966690063477 | BCE Loss: 1.025964379310608\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 5.365484237670898 | KNN Loss: 4.346495628356934 | BCE Loss: 1.0189886093139648\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 5.386602401733398 | KNN Loss: 4.340094089508057 | BCE Loss: 1.046508550643921\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 5.389975547790527 | KNN Loss: 4.360015869140625 | BCE Loss: 1.0299595594406128\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 5.374500274658203 | KNN Loss: 4.35989236831665 | BCE Loss: 1.0146081447601318\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 5.4044318199157715 | KNN Loss: 4.385924816131592 | BCE Loss: 1.0185070037841797\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 5.352265357971191 | KNN Loss: 4.3360395431518555 | BCE Loss: 1.016225814819336\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 5.378441333770752 | KNN Loss: 4.355691909790039 | BCE Loss: 1.022749423980713\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 5.346631050109863 | KNN Loss: 4.323977470397949 | BCE Loss: 1.0226538181304932\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 5.344268798828125 | KNN Loss: 4.335864067077637 | BCE Loss: 1.0084044933319092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 5.342750549316406 | KNN Loss: 4.327213287353516 | BCE Loss: 1.0155372619628906\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 5.390583038330078 | KNN Loss: 4.345832824707031 | BCE Loss: 1.0447500944137573\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 5.360966205596924 | KNN Loss: 4.330828666687012 | BCE Loss: 1.030137538909912\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 5.360551834106445 | KNN Loss: 4.328232288360596 | BCE Loss: 1.0323193073272705\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 5.377540111541748 | KNN Loss: 4.339588165283203 | BCE Loss: 1.0379518270492554\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 5.392430782318115 | KNN Loss: 4.370214462280273 | BCE Loss: 1.0222163200378418\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 5.371382713317871 | KNN Loss: 4.358486652374268 | BCE Loss: 1.0128958225250244\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 5.382922172546387 | KNN Loss: 4.359556674957275 | BCE Loss: 1.0233652591705322\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 5.359867095947266 | KNN Loss: 4.34210729598999 | BCE Loss: 1.0177600383758545\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 5.362091064453125 | KNN Loss: 4.337479114532471 | BCE Loss: 1.0246121883392334\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 5.3588666915893555 | KNN Loss: 4.3492889404296875 | BCE Loss: 1.0095775127410889\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 5.367114067077637 | KNN Loss: 4.348630905151367 | BCE Loss: 1.0184831619262695\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 5.379783630371094 | KNN Loss: 4.3685526847839355 | BCE Loss: 1.011230707168579\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 5.426443576812744 | KNN Loss: 4.3663763999938965 | BCE Loss: 1.0600672960281372\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 5.406332015991211 | KNN Loss: 4.37585973739624 | BCE Loss: 1.0304721593856812\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 5.422090530395508 | KNN Loss: 4.384721755981445 | BCE Loss: 1.0373687744140625\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 5.389715671539307 | KNN Loss: 4.36613130569458 | BCE Loss: 1.0235843658447266\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 5.348381996154785 | KNN Loss: 4.342698097229004 | BCE Loss: 1.0056840181350708\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 5.36470890045166 | KNN Loss: 4.339369297027588 | BCE Loss: 1.0253398418426514\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 5.355068206787109 | KNN Loss: 4.3253021240234375 | BCE Loss: 1.0297659635543823\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 5.365160942077637 | KNN Loss: 4.328117370605469 | BCE Loss: 1.0370434522628784\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 5.3674750328063965 | KNN Loss: 4.3568620681762695 | BCE Loss: 1.0106128454208374\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 5.4202880859375 | KNN Loss: 4.40028715133667 | BCE Loss: 1.02000093460083\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 5.413335800170898 | KNN Loss: 4.3708953857421875 | BCE Loss: 1.0424402952194214\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 5.354496002197266 | KNN Loss: 4.315463542938232 | BCE Loss: 1.0390324592590332\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 5.3475823402404785 | KNN Loss: 4.344398498535156 | BCE Loss: 1.0031838417053223\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 5.404879093170166 | KNN Loss: 4.353394985198975 | BCE Loss: 1.0514841079711914\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 5.3651123046875 | KNN Loss: 4.33751106262207 | BCE Loss: 1.0276012420654297\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 5.427094459533691 | KNN Loss: 4.360971450805664 | BCE Loss: 1.0661227703094482\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 5.384078025817871 | KNN Loss: 4.346648216247559 | BCE Loss: 1.0374300479888916\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 5.334724426269531 | KNN Loss: 4.342979907989502 | BCE Loss: 0.9917443990707397\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 5.347883224487305 | KNN Loss: 4.3360748291015625 | BCE Loss: 1.0118086338043213\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 5.380575180053711 | KNN Loss: 4.363199234008789 | BCE Loss: 1.017376184463501\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 5.355651378631592 | KNN Loss: 4.324496746063232 | BCE Loss: 1.0311545133590698\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 5.355853080749512 | KNN Loss: 4.3529133796691895 | BCE Loss: 1.0029399394989014\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 5.35791540145874 | KNN Loss: 4.342967510223389 | BCE Loss: 1.0149480104446411\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 5.346128463745117 | KNN Loss: 4.337130546569824 | BCE Loss: 1.008998155593872\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 5.396692752838135 | KNN Loss: 4.359853744506836 | BCE Loss: 1.0368391275405884\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 5.409002780914307 | KNN Loss: 4.379591941833496 | BCE Loss: 1.0294109582901\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 5.346406936645508 | KNN Loss: 4.3264007568359375 | BCE Loss: 1.0200064182281494\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 5.386585235595703 | KNN Loss: 4.363469123840332 | BCE Loss: 1.0231163501739502\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 5.424482345581055 | KNN Loss: 4.389078140258789 | BCE Loss: 1.0354043245315552\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 5.386196136474609 | KNN Loss: 4.323076248168945 | BCE Loss: 1.063119649887085\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 5.359794616699219 | KNN Loss: 4.34320068359375 | BCE Loss: 1.0165941715240479\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 5.404614448547363 | KNN Loss: 4.3597092628479 | BCE Loss: 1.044905185699463\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 5.3820719718933105 | KNN Loss: 4.366450309753418 | BCE Loss: 1.0156216621398926\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 5.42500638961792 | KNN Loss: 4.382014751434326 | BCE Loss: 1.0429915189743042\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 5.481356143951416 | KNN Loss: 4.4202656745910645 | BCE Loss: 1.0610905885696411\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 5.367607593536377 | KNN Loss: 4.328568935394287 | BCE Loss: 1.0390385389328003\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 5.378437519073486 | KNN Loss: 4.360776901245117 | BCE Loss: 1.0176604986190796\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 5.366336822509766 | KNN Loss: 4.349643230438232 | BCE Loss: 1.0166935920715332\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 5.432013034820557 | KNN Loss: 4.383589744567871 | BCE Loss: 1.0484232902526855\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 5.357048511505127 | KNN Loss: 4.347187519073486 | BCE Loss: 1.0098609924316406\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 5.3721113204956055 | KNN Loss: 4.326620101928711 | BCE Loss: 1.0454912185668945\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 5.392368316650391 | KNN Loss: 4.335341930389404 | BCE Loss: 1.0570266246795654\n",
      "Epoch   469: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 5.365248680114746 | KNN Loss: 4.355896949768066 | BCE Loss: 1.0093517303466797\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 5.37542724609375 | KNN Loss: 4.3503570556640625 | BCE Loss: 1.0250699520111084\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 5.3770856857299805 | KNN Loss: 4.343703746795654 | BCE Loss: 1.0333820581436157\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 5.370723247528076 | KNN Loss: 4.3422675132751465 | BCE Loss: 1.0284558534622192\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 5.380768775939941 | KNN Loss: 4.330707550048828 | BCE Loss: 1.0500612258911133\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 5.408974647521973 | KNN Loss: 4.35963249206543 | BCE Loss: 1.0493419170379639\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 5.374637126922607 | KNN Loss: 4.337836265563965 | BCE Loss: 1.0368008613586426\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 5.395444869995117 | KNN Loss: 4.369912624359131 | BCE Loss: 1.0255320072174072\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 5.3759565353393555 | KNN Loss: 4.344487190246582 | BCE Loss: 1.0314695835113525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 5.358429431915283 | KNN Loss: 4.32494592666626 | BCE Loss: 1.0334833860397339\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 5.368271350860596 | KNN Loss: 4.356976509094238 | BCE Loss: 1.011294960975647\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 5.3690080642700195 | KNN Loss: 4.339217662811279 | BCE Loss: 1.0297901630401611\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 5.34496545791626 | KNN Loss: 4.337339878082275 | BCE Loss: 1.0076255798339844\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 5.389900207519531 | KNN Loss: 4.372283458709717 | BCE Loss: 1.0176165103912354\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 5.404962539672852 | KNN Loss: 4.372114658355713 | BCE Loss: 1.0328476428985596\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 5.380067825317383 | KNN Loss: 4.3546648025512695 | BCE Loss: 1.0254027843475342\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 5.427299499511719 | KNN Loss: 4.386161804199219 | BCE Loss: 1.0411376953125\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 5.384666919708252 | KNN Loss: 4.369581699371338 | BCE Loss: 1.0150853395462036\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 5.352138042449951 | KNN Loss: 4.331406116485596 | BCE Loss: 1.020731806755066\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 5.3628764152526855 | KNN Loss: 4.347801208496094 | BCE Loss: 1.0150752067565918\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 5.3713531494140625 | KNN Loss: 4.363999366760254 | BCE Loss: 1.0073537826538086\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 5.383091449737549 | KNN Loss: 4.347073078155518 | BCE Loss: 1.0360184907913208\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 5.386809825897217 | KNN Loss: 4.3570404052734375 | BCE Loss: 1.0297694206237793\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 5.381011962890625 | KNN Loss: 4.354839324951172 | BCE Loss: 1.026172399520874\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 5.323997974395752 | KNN Loss: 4.325128555297852 | BCE Loss: 0.9988694787025452\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 5.376129150390625 | KNN Loss: 4.355013847351074 | BCE Loss: 1.0211153030395508\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 5.373088836669922 | KNN Loss: 4.335916519165039 | BCE Loss: 1.0371720790863037\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 5.370321750640869 | KNN Loss: 4.357050895690918 | BCE Loss: 1.0132709741592407\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 5.335768699645996 | KNN Loss: 4.335752964019775 | BCE Loss: 1.0000158548355103\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 5.3851704597473145 | KNN Loss: 4.345605373382568 | BCE Loss: 1.039565086364746\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 5.37079381942749 | KNN Loss: 4.340753078460693 | BCE Loss: 1.0300407409667969\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 5.444808006286621 | KNN Loss: 4.38104772567749 | BCE Loss: 1.0637600421905518\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 5.414121150970459 | KNN Loss: 4.381713390350342 | BCE Loss: 1.0324076414108276\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 5.3449530601501465 | KNN Loss: 4.347822189331055 | BCE Loss: 0.997130811214447\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 5.344174385070801 | KNN Loss: 4.327122211456299 | BCE Loss: 1.017052412033081\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 5.354727745056152 | KNN Loss: 4.328117370605469 | BCE Loss: 1.0266103744506836\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 5.370366096496582 | KNN Loss: 4.351734161376953 | BCE Loss: 1.018631935119629\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 5.335973739624023 | KNN Loss: 4.339266300201416 | BCE Loss: 0.9967072010040283\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 5.358447551727295 | KNN Loss: 4.342343807220459 | BCE Loss: 1.0161036252975464\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 5.377527713775635 | KNN Loss: 4.341097354888916 | BCE Loss: 1.0364303588867188\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 5.380594253540039 | KNN Loss: 4.337934494018555 | BCE Loss: 1.0426599979400635\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 5.358081817626953 | KNN Loss: 4.34834623336792 | BCE Loss: 1.0097355842590332\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 5.4055328369140625 | KNN Loss: 4.366460800170898 | BCE Loss: 1.0390722751617432\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 5.3987250328063965 | KNN Loss: 4.355834484100342 | BCE Loss: 1.0428905487060547\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 5.350323677062988 | KNN Loss: 4.338892936706543 | BCE Loss: 1.0114305019378662\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 5.404218673706055 | KNN Loss: 4.351291179656982 | BCE Loss: 1.0529277324676514\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 5.3868913650512695 | KNN Loss: 4.35880708694458 | BCE Loss: 1.0280842781066895\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 5.378754615783691 | KNN Loss: 4.35268497467041 | BCE Loss: 1.0260698795318604\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 5.34517765045166 | KNN Loss: 4.326730728149414 | BCE Loss: 1.018446922302246\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 5.4090728759765625 | KNN Loss: 4.377156734466553 | BCE Loss: 1.0319163799285889\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 5.38193941116333 | KNN Loss: 4.342011451721191 | BCE Loss: 1.0399279594421387\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 5.3653411865234375 | KNN Loss: 4.33797550201416 | BCE Loss: 1.0273654460906982\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 5.377506256103516 | KNN Loss: 4.371944904327393 | BCE Loss: 1.005561113357544\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 5.372758865356445 | KNN Loss: 4.339008331298828 | BCE Loss: 1.0337507724761963\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 5.37063455581665 | KNN Loss: 4.324648857116699 | BCE Loss: 1.0459856986999512\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 5.397334575653076 | KNN Loss: 4.368102550506592 | BCE Loss: 1.0292320251464844\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 5.373481750488281 | KNN Loss: 4.3443217277526855 | BCE Loss: 1.0291602611541748\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 5.388589859008789 | KNN Loss: 4.35623836517334 | BCE Loss: 1.0323514938354492\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 5.3875932693481445 | KNN Loss: 4.36693811416626 | BCE Loss: 1.0206553936004639\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 5.391478538513184 | KNN Loss: 4.3543701171875 | BCE Loss: 1.0371081829071045\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 5.392264366149902 | KNN Loss: 4.331036567687988 | BCE Loss: 1.0612276792526245\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 5.387078285217285 | KNN Loss: 4.3386077880859375 | BCE Loss: 1.0484702587127686\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 5.360317230224609 | KNN Loss: 4.366847515106201 | BCE Loss: 0.9934697151184082\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 5.352327346801758 | KNN Loss: 4.344761848449707 | BCE Loss: 1.0075652599334717\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 5.374794960021973 | KNN Loss: 4.338320732116699 | BCE Loss: 1.036474347114563\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 5.374754905700684 | KNN Loss: 4.366700172424316 | BCE Loss: 1.008054494857788\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 5.3848724365234375 | KNN Loss: 4.330324649810791 | BCE Loss: 1.0545475482940674\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 5.37094783782959 | KNN Loss: 4.3786139488220215 | BCE Loss: 0.9923336505889893\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 5.432595252990723 | KNN Loss: 4.3637590408325195 | BCE Loss: 1.0688364505767822\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 5.339919090270996 | KNN Loss: 4.32859992980957 | BCE Loss: 1.0113189220428467\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 5.353121280670166 | KNN Loss: 4.354940891265869 | BCE Loss: 0.9981802701950073\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 5.391376972198486 | KNN Loss: 4.344996929168701 | BCE Loss: 1.0463800430297852\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 5.374837875366211 | KNN Loss: 4.336880683898926 | BCE Loss: 1.0379571914672852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 5.369363784790039 | KNN Loss: 4.339537620544434 | BCE Loss: 1.0298261642456055\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 5.389568328857422 | KNN Loss: 4.364975929260254 | BCE Loss: 1.024592399597168\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 5.369410514831543 | KNN Loss: 4.359279155731201 | BCE Loss: 1.0101313591003418\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 5.3662590980529785 | KNN Loss: 4.34240198135376 | BCE Loss: 1.0238569974899292\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 5.434374809265137 | KNN Loss: 4.3694376945495605 | BCE Loss: 1.0649373531341553\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 5.386667728424072 | KNN Loss: 4.363371849060059 | BCE Loss: 1.0232957601547241\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 5.3647918701171875 | KNN Loss: 4.347118854522705 | BCE Loss: 1.0176732540130615\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 5.4127044677734375 | KNN Loss: 4.358959197998047 | BCE Loss: 1.0537453889846802\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 5.380040168762207 | KNN Loss: 4.372409343719482 | BCE Loss: 1.0076305866241455\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 5.39471435546875 | KNN Loss: 4.358212947845459 | BCE Loss: 1.036501169204712\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 5.398099899291992 | KNN Loss: 4.349022388458252 | BCE Loss: 1.0490777492523193\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 5.352252006530762 | KNN Loss: 4.334507942199707 | BCE Loss: 1.0177440643310547\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 5.3795623779296875 | KNN Loss: 4.348177909851074 | BCE Loss: 1.0313842296600342\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 5.416684150695801 | KNN Loss: 4.349765777587891 | BCE Loss: 1.0669184923171997\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 5.393505096435547 | KNN Loss: 4.349026679992676 | BCE Loss: 1.0444786548614502\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 5.323117256164551 | KNN Loss: 4.330066204071045 | BCE Loss: 0.9930509328842163\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 5.42431640625 | KNN Loss: 4.382195949554443 | BCE Loss: 1.0421206951141357\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 5.345530033111572 | KNN Loss: 4.316873073577881 | BCE Loss: 1.0286569595336914\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 5.422284126281738 | KNN Loss: 4.391908168792725 | BCE Loss: 1.0303759574890137\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 5.359770774841309 | KNN Loss: 4.337185859680176 | BCE Loss: 1.0225849151611328\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 5.361184120178223 | KNN Loss: 4.356295108795166 | BCE Loss: 1.0048890113830566\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 5.397586345672607 | KNN Loss: 4.351637363433838 | BCE Loss: 1.045949101448059\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 5.383670330047607 | KNN Loss: 4.366734504699707 | BCE Loss: 1.0169357061386108\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 5.366109848022461 | KNN Loss: 4.338891983032227 | BCE Loss: 1.0272178649902344\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 5.375616550445557 | KNN Loss: 4.358096122741699 | BCE Loss: 1.0175204277038574\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 5.361054420471191 | KNN Loss: 4.351436138153076 | BCE Loss: 1.0096182823181152\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 5.392469882965088 | KNN Loss: 4.361771583557129 | BCE Loss: 1.030698299407959\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 5.408707618713379 | KNN Loss: 4.336091995239258 | BCE Loss: 1.0726155042648315\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 5.322732448577881 | KNN Loss: 4.3488450050354 | BCE Loss: 0.9738874435424805\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 5.387712478637695 | KNN Loss: 4.343932628631592 | BCE Loss: 1.0437798500061035\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 5.339974403381348 | KNN Loss: 4.3416056632995605 | BCE Loss: 0.9983688592910767\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 5.395693778991699 | KNN Loss: 4.366015434265137 | BCE Loss: 1.0296785831451416\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 5.380026340484619 | KNN Loss: 4.34666633605957 | BCE Loss: 1.0333600044250488\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 5.405457496643066 | KNN Loss: 4.3571858406066895 | BCE Loss: 1.048271894454956\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 5.398262023925781 | KNN Loss: 4.374346733093262 | BCE Loss: 1.0239152908325195\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 5.362244129180908 | KNN Loss: 4.351173400878906 | BCE Loss: 1.011070728302002\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 5.343690395355225 | KNN Loss: 4.327086448669434 | BCE Loss: 1.016603946685791\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 5.366449356079102 | KNN Loss: 4.351048469543457 | BCE Loss: 1.0154008865356445\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 5.4329729080200195 | KNN Loss: 4.398716926574707 | BCE Loss: 1.034256100654602\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 5.403068542480469 | KNN Loss: 4.353221893310547 | BCE Loss: 1.0498466491699219\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 5.384521484375 | KNN Loss: 4.331711769104004 | BCE Loss: 1.0528098344802856\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 5.40767765045166 | KNN Loss: 4.362249374389648 | BCE Loss: 1.0454285144805908\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 5.39052677154541 | KNN Loss: 4.369071006774902 | BCE Loss: 1.0214558839797974\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 5.375362396240234 | KNN Loss: 4.347499370574951 | BCE Loss: 1.0278632640838623\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 5.377445697784424 | KNN Loss: 4.347114562988281 | BCE Loss: 1.0303312540054321\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 5.369319915771484 | KNN Loss: 4.341116428375244 | BCE Loss: 1.0282032489776611\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 5.403223037719727 | KNN Loss: 4.334630966186523 | BCE Loss: 1.0685923099517822\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 5.3476738929748535 | KNN Loss: 4.330671310424805 | BCE Loss: 1.0170024633407593\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 5.400412082672119 | KNN Loss: 4.369866847991943 | BCE Loss: 1.0305453538894653\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 5.343203544616699 | KNN Loss: 4.328451156616211 | BCE Loss: 1.0147521495819092\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 5.394001483917236 | KNN Loss: 4.345468997955322 | BCE Loss: 1.0485323667526245\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 5.3751912117004395 | KNN Loss: 4.3530683517456055 | BCE Loss: 1.022122859954834\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 5.358366966247559 | KNN Loss: 4.35860538482666 | BCE Loss: 0.9997617602348328\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 5.377692222595215 | KNN Loss: 4.339042663574219 | BCE Loss: 1.0386497974395752\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 5.342782974243164 | KNN Loss: 4.321528911590576 | BCE Loss: 1.0212538242340088\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 5.361367225646973 | KNN Loss: 4.334903717041016 | BCE Loss: 1.0264633893966675\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 5.372454643249512 | KNN Loss: 4.345780849456787 | BCE Loss: 1.0266740322113037\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 5.3982133865356445 | KNN Loss: 4.391434192657471 | BCE Loss: 1.006779432296753\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 5.395259857177734 | KNN Loss: 4.363498687744141 | BCE Loss: 1.0317612886428833\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 5.412156581878662 | KNN Loss: 4.374986171722412 | BCE Loss: 1.03717041015625\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 5.382407188415527 | KNN Loss: 4.332728862762451 | BCE Loss: 1.0496785640716553\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 5.347404956817627 | KNN Loss: 4.348195552825928 | BCE Loss: 0.999209463596344\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 5.37568998336792 | KNN Loss: 4.329738616943359 | BCE Loss: 1.045951247215271\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 5.37307071685791 | KNN Loss: 4.337700843811035 | BCE Loss: 1.035370111465454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 5.353229522705078 | KNN Loss: 4.346161365509033 | BCE Loss: 1.007068395614624\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 5.421252250671387 | KNN Loss: 4.371200084686279 | BCE Loss: 1.0500521659851074\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 5.3846845626831055 | KNN Loss: 4.335709571838379 | BCE Loss: 1.0489747524261475\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 5.403069496154785 | KNN Loss: 4.333566665649414 | BCE Loss: 1.0695027112960815\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 5.36777400970459 | KNN Loss: 4.340906143188477 | BCE Loss: 1.0268676280975342\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 5.383893966674805 | KNN Loss: 4.3631415367126465 | BCE Loss: 1.020752191543579\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 5.348280429840088 | KNN Loss: 4.353436470031738 | BCE Loss: 0.9948439598083496\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 5.375085353851318 | KNN Loss: 4.338935852050781 | BCE Loss: 1.036149501800537\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 5.364687442779541 | KNN Loss: 4.343027591705322 | BCE Loss: 1.0216599702835083\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 5.359705924987793 | KNN Loss: 4.340874195098877 | BCE Loss: 1.0188319683074951\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 5.3622212409973145 | KNN Loss: 4.35099458694458 | BCE Loss: 1.011226773262024\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 5.361487865447998 | KNN Loss: 4.3543829917907715 | BCE Loss: 1.0071049928665161\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 5.388065814971924 | KNN Loss: 4.345251083374023 | BCE Loss: 1.0428147315979004\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 5.368188858032227 | KNN Loss: 4.342639923095703 | BCE Loss: 1.0255486965179443\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 5.408346652984619 | KNN Loss: 4.351864337921143 | BCE Loss: 1.0564823150634766\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 5.367815971374512 | KNN Loss: 4.361756801605225 | BCE Loss: 1.006059169769287\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 5.3899078369140625 | KNN Loss: 4.3368635177612305 | BCE Loss: 1.0530441999435425\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 5.387983798980713 | KNN Loss: 4.3690595626831055 | BCE Loss: 1.0189242362976074\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 5.3936967849731445 | KNN Loss: 4.379395008087158 | BCE Loss: 1.0143020153045654\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 5.412369728088379 | KNN Loss: 4.376068592071533 | BCE Loss: 1.0363008975982666\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 5.401141166687012 | KNN Loss: 4.360576152801514 | BCE Loss: 1.0405648946762085\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 5.398774147033691 | KNN Loss: 4.373338222503662 | BCE Loss: 1.0254361629486084\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 5.363734245300293 | KNN Loss: 4.343210220336914 | BCE Loss: 1.0205237865447998\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 5.431063652038574 | KNN Loss: 4.398930072784424 | BCE Loss: 1.0321334600448608\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 5.373016834259033 | KNN Loss: 4.356868267059326 | BCE Loss: 1.016148567199707\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 5.342663288116455 | KNN Loss: 4.328751087188721 | BCE Loss: 1.0139122009277344\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 5.3649492263793945 | KNN Loss: 4.352619171142578 | BCE Loss: 1.0123299360275269\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 5.346078872680664 | KNN Loss: 4.318583965301514 | BCE Loss: 1.0274946689605713\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 5.380669116973877 | KNN Loss: 4.342626094818115 | BCE Loss: 1.0380430221557617\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 5.344632625579834 | KNN Loss: 4.3379387855529785 | BCE Loss: 1.006693720817566\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 5.384451866149902 | KNN Loss: 4.343134880065918 | BCE Loss: 1.0413167476654053\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 5.443255424499512 | KNN Loss: 4.391672611236572 | BCE Loss: 1.0515828132629395\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 5.478638172149658 | KNN Loss: 4.435548782348633 | BCE Loss: 1.0430893898010254\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 5.37515926361084 | KNN Loss: 4.346909523010254 | BCE Loss: 1.028249979019165\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 5.387774467468262 | KNN Loss: 4.349372863769531 | BCE Loss: 1.0384013652801514\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 5.371315002441406 | KNN Loss: 4.353426456451416 | BCE Loss: 1.0178887844085693\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 5.364528656005859 | KNN Loss: 4.353528022766113 | BCE Loss: 1.011000633239746\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 5.362292289733887 | KNN Loss: 4.321304798126221 | BCE Loss: 1.040987253189087\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 5.395000457763672 | KNN Loss: 4.3476643562316895 | BCE Loss: 1.0473361015319824\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 5.394001007080078 | KNN Loss: 4.340227127075195 | BCE Loss: 1.0537738800048828\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 5.362288475036621 | KNN Loss: 4.351659297943115 | BCE Loss: 1.010629415512085\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 5.383538246154785 | KNN Loss: 4.359744548797607 | BCE Loss: 1.0237939357757568\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 5.399906158447266 | KNN Loss: 4.369213581085205 | BCE Loss: 1.0306928157806396\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 5.37441349029541 | KNN Loss: 4.337710380554199 | BCE Loss: 1.036703109741211\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 5.381124019622803 | KNN Loss: 4.343753337860107 | BCE Loss: 1.0373708009719849\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 5.396519184112549 | KNN Loss: 4.364954948425293 | BCE Loss: 1.0315642356872559\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 5.362174987792969 | KNN Loss: 4.32948112487793 | BCE Loss: 1.032693862915039\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 5.374627113342285 | KNN Loss: 4.360760688781738 | BCE Loss: 1.0138664245605469\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 5.363710403442383 | KNN Loss: 4.333188533782959 | BCE Loss: 1.030522108078003\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3926,  4.0860,  2.8788,  3.9504,  2.8986,  0.4837,  2.7811,  1.9030,\n",
      "          2.1603,  2.3338,  2.5971,  2.5487,  1.0050,  1.4871,  1.5329,  1.5996,\n",
      "          2.2310,  3.5485,  2.7080,  1.6907,  2.0699,  2.6769,  2.0625,  2.9899,\n",
      "          1.9688,  2.0603,  2.1489,  1.4235,  1.8046,  0.2838, -0.4469,  0.9981,\n",
      "         -0.2694,  0.8887,  1.6710,  1.5721,  0.9988,  3.7110,  1.0052,  1.3846,\n",
      "          0.9644, -0.8202, -0.4330,  2.1562,  2.2686,  0.7811, -0.1593,  0.0440,\n",
      "          1.2229,  2.8391,  2.1248,  0.1024,  1.5054,  0.5918, -0.5685,  1.0243,\n",
      "          1.4711,  1.4002,  1.3616,  2.1332,  0.5040,  0.9411,  0.2120,  1.7772,\n",
      "          1.1621,  1.6978, -2.1811,  0.0616,  2.3492,  2.2480,  1.9991,  0.3823,\n",
      "          1.1766,  2.1853,  2.2963,  1.0241,  0.3581,  0.8016,  0.1199,  1.6242,\n",
      "          0.0295,  0.4071,  1.8388, -0.4244,  0.2974, -1.0537, -2.5488, -0.3111,\n",
      "          0.5336, -2.0137,  0.4038, -0.1622, -0.6053, -0.9296,  0.4635,  1.2992,\n",
      "         -0.6913, -0.6401,  0.3233,  1.4224,  0.7130, -1.2775,  0.9230,  1.0678,\n",
      "         -1.2324, -1.1151, -0.1525,  0.0938, -0.9601, -1.8374, -0.4065, -2.8967,\n",
      "         -0.3451,  1.8106,  1.8776, -0.2893, -0.9879,  0.1214,  1.8198, -2.6692,\n",
      "          0.1099, -0.2999,  0.3437, -0.7486, -0.0186, -0.8383, -0.9456,  1.0103,\n",
      "          0.0430, -0.6481,  0.4202, -0.9545, -1.3431, -0.3089, -0.6339,  0.8520,\n",
      "         -0.5683,  0.2229, -2.1934, -0.8819, -1.6970,  0.6219, -1.8817, -0.8898,\n",
      "         -1.0285, -0.7238, -1.7604, -1.1247, -2.5630, -0.9216, -1.6883, -0.3441,\n",
      "         -1.7413,  0.4681, -1.6281, -0.5305, -3.3961,  0.0344, -0.2835, -0.8242,\n",
      "         -2.2381, -1.7331, -1.1869, -1.3384, -2.4698, -2.7102, -3.4018]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.4018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.0860, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733bf33353014575812c0ed7337f18aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 88.88it/s] \n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84c78d306734440b3db4c9452649b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e4abec2b84c879d312c1ce848401b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5600a7e0d749e080c80afda2c80c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 029 | Total loss: 9.607 | Reg loss: 0.011 | Tree loss: 9.607 | Accuracy: 0.000000 | 1.088 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 029 | Total loss: 9.603 | Reg loss: 0.011 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.95 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 029 | Total loss: 9.598 | Reg loss: 0.010 | Tree loss: 9.598 | Accuracy: 0.000000 | 0.909 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 029 | Total loss: 9.594 | Reg loss: 0.009 | Tree loss: 9.594 | Accuracy: 0.000000 | 0.892 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 029 | Total loss: 9.588 | Reg loss: 0.009 | Tree loss: 9.588 | Accuracy: 0.000000 | 0.88 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 029 | Total loss: 9.582 | Reg loss: 0.008 | Tree loss: 9.582 | Accuracy: 0.000000 | 0.874 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 029 | Total loss: 9.577 | Reg loss: 0.008 | Tree loss: 9.577 | Accuracy: 0.000000 | 0.868 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 029 | Total loss: 9.574 | Reg loss: 0.007 | Tree loss: 9.574 | Accuracy: 0.000000 | 0.865 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 029 | Total loss: 9.569 | Reg loss: 0.007 | Tree loss: 9.569 | Accuracy: 0.000000 | 0.861 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 029 | Total loss: 9.564 | Reg loss: 0.007 | Tree loss: 9.564 | Accuracy: 0.000000 | 0.857 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 029 | Total loss: 9.557 | Reg loss: 0.007 | Tree loss: 9.557 | Accuracy: 0.000000 | 0.854 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 029 | Total loss: 9.554 | Reg loss: 0.007 | Tree loss: 9.554 | Accuracy: 0.000000 | 0.851 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 029 | Total loss: 9.551 | Reg loss: 0.007 | Tree loss: 9.551 | Accuracy: 0.001953 | 0.849 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 029 | Total loss: 9.548 | Reg loss: 0.007 | Tree loss: 9.548 | Accuracy: 0.013672 | 0.848 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 029 | Total loss: 9.542 | Reg loss: 0.007 | Tree loss: 9.542 | Accuracy: 0.005859 | 0.846 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 029 | Total loss: 9.537 | Reg loss: 0.007 | Tree loss: 9.537 | Accuracy: 0.025391 | 0.844 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 029 | Total loss: 9.534 | Reg loss: 0.007 | Tree loss: 9.534 | Accuracy: 0.025391 | 0.841 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 029 | Total loss: 9.529 | Reg loss: 0.008 | Tree loss: 9.529 | Accuracy: 0.041016 | 0.842 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 029 | Total loss: 9.525 | Reg loss: 0.008 | Tree loss: 9.525 | Accuracy: 0.062500 | 0.841 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 029 | Total loss: 9.522 | Reg loss: 0.008 | Tree loss: 9.522 | Accuracy: 0.103516 | 0.839 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 029 | Total loss: 9.514 | Reg loss: 0.008 | Tree loss: 9.514 | Accuracy: 0.156250 | 0.841 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 029 | Total loss: 9.511 | Reg loss: 0.009 | Tree loss: 9.511 | Accuracy: 0.166016 | 0.843 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 029 | Total loss: 9.508 | Reg loss: 0.009 | Tree loss: 9.508 | Accuracy: 0.164062 | 0.843 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 029 | Total loss: 9.506 | Reg loss: 0.009 | Tree loss: 9.506 | Accuracy: 0.162109 | 0.844 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 029 | Total loss: 9.500 | Reg loss: 0.009 | Tree loss: 9.500 | Accuracy: 0.193359 | 0.844 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 029 | Total loss: 9.498 | Reg loss: 0.010 | Tree loss: 9.498 | Accuracy: 0.201172 | 0.844 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 029 | Total loss: 9.494 | Reg loss: 0.010 | Tree loss: 9.494 | Accuracy: 0.193359 | 0.844 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 029 | Total loss: 9.488 | Reg loss: 0.010 | Tree loss: 9.488 | Accuracy: 0.218750 | 0.843 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 029 | Total loss: 9.490 | Reg loss: 0.010 | Tree loss: 9.490 | Accuracy: 0.189655 | 0.842 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 029 | Total loss: 9.531 | Reg loss: 0.005 | Tree loss: 9.531 | Accuracy: 0.089844 | 0.878 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 029 | Total loss: 9.528 | Reg loss: 0.005 | Tree loss: 9.528 | Accuracy: 0.111328 | 0.875 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 029 | Total loss: 9.523 | Reg loss: 0.005 | Tree loss: 9.523 | Accuracy: 0.224609 | 0.873 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 029 | Total loss: 9.519 | Reg loss: 0.005 | Tree loss: 9.519 | Accuracy: 0.222656 | 0.871 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 029 | Total loss: 9.515 | Reg loss: 0.005 | Tree loss: 9.515 | Accuracy: 0.181641 | 0.869 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 029 | Total loss: 9.511 | Reg loss: 0.005 | Tree loss: 9.511 | Accuracy: 0.195312 | 0.867 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 029 | Total loss: 9.504 | Reg loss: 0.006 | Tree loss: 9.504 | Accuracy: 0.224609 | 0.865 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 029 | Total loss: 9.505 | Reg loss: 0.006 | Tree loss: 9.505 | Accuracy: 0.181641 | 0.864 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 029 | Total loss: 9.498 | Reg loss: 0.006 | Tree loss: 9.498 | Accuracy: 0.205078 | 0.864 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 029 | Total loss: 9.492 | Reg loss: 0.007 | Tree loss: 9.492 | Accuracy: 0.201172 | 0.863 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 029 | Total loss: 9.488 | Reg loss: 0.007 | Tree loss: 9.488 | Accuracy: 0.208984 | 0.862 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 029 | Total loss: 9.483 | Reg loss: 0.007 | Tree loss: 9.483 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 029 | Total loss: 9.481 | Reg loss: 0.008 | Tree loss: 9.481 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 029 | Total loss: 9.475 | Reg loss: 0.008 | Tree loss: 9.475 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 029 | Total loss: 9.468 | Reg loss: 0.008 | Tree loss: 9.468 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 029 | Total loss: 9.471 | Reg loss: 0.009 | Tree loss: 9.471 | Accuracy: 0.166016 | 0.858 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 029 | Total loss: 9.464 | Reg loss: 0.009 | Tree loss: 9.464 | Accuracy: 0.201172 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 029 | Total loss: 9.461 | Reg loss: 0.009 | Tree loss: 9.461 | Accuracy: 0.199219 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 029 | Total loss: 9.450 | Reg loss: 0.010 | Tree loss: 9.450 | Accuracy: 0.222656 | 0.856 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 029 | Total loss: 9.449 | Reg loss: 0.010 | Tree loss: 9.449 | Accuracy: 0.214844 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 029 | Total loss: 9.441 | Reg loss: 0.010 | Tree loss: 9.441 | Accuracy: 0.226562 | 0.855 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 029 | Total loss: 9.438 | Reg loss: 0.011 | Tree loss: 9.438 | Accuracy: 0.185547 | 0.854 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 029 | Total loss: 9.434 | Reg loss: 0.011 | Tree loss: 9.434 | Accuracy: 0.216797 | 0.854 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 029 | Total loss: 9.432 | Reg loss: 0.011 | Tree loss: 9.432 | Accuracy: 0.208984 | 0.853 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 029 | Total loss: 9.425 | Reg loss: 0.012 | Tree loss: 9.425 | Accuracy: 0.240234 | 0.853 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 029 | Total loss: 9.419 | Reg loss: 0.012 | Tree loss: 9.419 | Accuracy: 0.205078 | 0.852 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 029 | Total loss: 9.417 | Reg loss: 0.012 | Tree loss: 9.417 | Accuracy: 0.185547 | 0.851 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 029 | Total loss: 9.416 | Reg loss: 0.012 | Tree loss: 9.416 | Accuracy: 0.201172 | 0.85 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 029 | Total loss: 9.425 | Reg loss: 0.013 | Tree loss: 9.425 | Accuracy: 0.189655 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 029 | Total loss: 9.470 | Reg loss: 0.007 | Tree loss: 9.470 | Accuracy: 0.181641 | 0.867 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 029 | Total loss: 9.466 | Reg loss: 0.007 | Tree loss: 9.466 | Accuracy: 0.175781 | 0.865 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 029 | Total loss: 9.462 | Reg loss: 0.007 | Tree loss: 9.462 | Accuracy: 0.185547 | 0.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 003 / 029 | Total loss: 9.457 | Reg loss: 0.008 | Tree loss: 9.457 | Accuracy: 0.218750 | 0.863 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 029 | Total loss: 9.452 | Reg loss: 0.008 | Tree loss: 9.452 | Accuracy: 0.189453 | 0.863 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 029 | Total loss: 9.444 | Reg loss: 0.008 | Tree loss: 9.444 | Accuracy: 0.220703 | 0.862 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 029 | Total loss: 9.439 | Reg loss: 0.008 | Tree loss: 9.439 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 029 | Total loss: 9.436 | Reg loss: 0.009 | Tree loss: 9.436 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 029 | Total loss: 9.427 | Reg loss: 0.009 | Tree loss: 9.427 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 029 | Total loss: 9.424 | Reg loss: 0.009 | Tree loss: 9.424 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 029 | Total loss: 9.416 | Reg loss: 0.010 | Tree loss: 9.416 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 029 | Total loss: 9.417 | Reg loss: 0.010 | Tree loss: 9.417 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 029 | Total loss: 9.407 | Reg loss: 0.010 | Tree loss: 9.407 | Accuracy: 0.244141 | 0.858 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 029 | Total loss: 9.398 | Reg loss: 0.011 | Tree loss: 9.398 | Accuracy: 0.185547 | 0.857 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 029 | Total loss: 9.398 | Reg loss: 0.011 | Tree loss: 9.398 | Accuracy: 0.173828 | 0.857 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 029 | Total loss: 9.391 | Reg loss: 0.011 | Tree loss: 9.391 | Accuracy: 0.203125 | 0.856 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 029 | Total loss: 9.383 | Reg loss: 0.012 | Tree loss: 9.383 | Accuracy: 0.205078 | 0.855 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 029 | Total loss: 9.381 | Reg loss: 0.012 | Tree loss: 9.381 | Accuracy: 0.216797 | 0.855 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 029 | Total loss: 9.376 | Reg loss: 0.013 | Tree loss: 9.376 | Accuracy: 0.193359 | 0.855 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 029 | Total loss: 9.374 | Reg loss: 0.013 | Tree loss: 9.374 | Accuracy: 0.183594 | 0.855 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 029 | Total loss: 9.362 | Reg loss: 0.014 | Tree loss: 9.362 | Accuracy: 0.216797 | 0.854 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 029 | Total loss: 9.355 | Reg loss: 0.014 | Tree loss: 9.355 | Accuracy: 0.218750 | 0.854 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 029 | Total loss: 9.347 | Reg loss: 0.014 | Tree loss: 9.347 | Accuracy: 0.185547 | 0.854 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 029 | Total loss: 9.339 | Reg loss: 0.015 | Tree loss: 9.339 | Accuracy: 0.207031 | 0.853 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 029 | Total loss: 9.336 | Reg loss: 0.015 | Tree loss: 9.336 | Accuracy: 0.207031 | 0.853 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 029 | Total loss: 9.332 | Reg loss: 0.015 | Tree loss: 9.332 | Accuracy: 0.210938 | 0.852 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 029 | Total loss: 9.326 | Reg loss: 0.016 | Tree loss: 9.326 | Accuracy: 0.210938 | 0.852 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 029 | Total loss: 9.316 | Reg loss: 0.016 | Tree loss: 9.316 | Accuracy: 0.212891 | 0.852 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 029 | Total loss: 9.294 | Reg loss: 0.017 | Tree loss: 9.294 | Accuracy: 0.155172 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 029 | Total loss: 9.399 | Reg loss: 0.010 | Tree loss: 9.399 | Accuracy: 0.222656 | 0.864 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 029 | Total loss: 9.397 | Reg loss: 0.010 | Tree loss: 9.397 | Accuracy: 0.199219 | 0.863 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 029 | Total loss: 9.384 | Reg loss: 0.010 | Tree loss: 9.384 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 029 | Total loss: 9.381 | Reg loss: 0.011 | Tree loss: 9.381 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 029 | Total loss: 9.370 | Reg loss: 0.011 | Tree loss: 9.370 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 029 | Total loss: 9.371 | Reg loss: 0.011 | Tree loss: 9.371 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 029 | Total loss: 9.362 | Reg loss: 0.011 | Tree loss: 9.362 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 029 | Total loss: 9.353 | Reg loss: 0.012 | Tree loss: 9.353 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 029 | Total loss: 9.347 | Reg loss: 0.012 | Tree loss: 9.347 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 029 | Total loss: 9.340 | Reg loss: 0.012 | Tree loss: 9.340 | Accuracy: 0.210938 | 0.858 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 029 | Total loss: 9.327 | Reg loss: 0.013 | Tree loss: 9.327 | Accuracy: 0.212891 | 0.857 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 029 | Total loss: 9.320 | Reg loss: 0.013 | Tree loss: 9.320 | Accuracy: 0.203125 | 0.857 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 029 | Total loss: 9.310 | Reg loss: 0.014 | Tree loss: 9.310 | Accuracy: 0.207031 | 0.857 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 029 | Total loss: 9.309 | Reg loss: 0.014 | Tree loss: 9.309 | Accuracy: 0.152344 | 0.857 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 029 | Total loss: 9.307 | Reg loss: 0.014 | Tree loss: 9.307 | Accuracy: 0.185547 | 0.856 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 029 | Total loss: 9.293 | Reg loss: 0.015 | Tree loss: 9.293 | Accuracy: 0.214844 | 0.856 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 029 | Total loss: 9.286 | Reg loss: 0.015 | Tree loss: 9.286 | Accuracy: 0.191406 | 0.856 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 029 | Total loss: 9.263 | Reg loss: 0.016 | Tree loss: 9.263 | Accuracy: 0.208984 | 0.856 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 029 | Total loss: 9.264 | Reg loss: 0.016 | Tree loss: 9.264 | Accuracy: 0.197266 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 029 | Total loss: 9.247 | Reg loss: 0.017 | Tree loss: 9.247 | Accuracy: 0.205078 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 029 | Total loss: 9.240 | Reg loss: 0.017 | Tree loss: 9.240 | Accuracy: 0.205078 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 029 | Total loss: 9.220 | Reg loss: 0.018 | Tree loss: 9.220 | Accuracy: 0.228516 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 029 | Total loss: 9.206 | Reg loss: 0.018 | Tree loss: 9.206 | Accuracy: 0.248047 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 029 | Total loss: 9.212 | Reg loss: 0.018 | Tree loss: 9.212 | Accuracy: 0.214844 | 0.855 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 029 | Total loss: 9.196 | Reg loss: 0.019 | Tree loss: 9.196 | Accuracy: 0.189453 | 0.854 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 029 | Total loss: 9.188 | Reg loss: 0.019 | Tree loss: 9.188 | Accuracy: 0.226562 | 0.854 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 029 | Total loss: 9.185 | Reg loss: 0.020 | Tree loss: 9.185 | Accuracy: 0.179688 | 0.854 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 029 | Total loss: 9.159 | Reg loss: 0.020 | Tree loss: 9.159 | Accuracy: 0.218750 | 0.854 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 029 | Total loss: 9.130 | Reg loss: 0.021 | Tree loss: 9.130 | Accuracy: 0.241379 | 0.853 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 029 | Total loss: 9.304 | Reg loss: 0.013 | Tree loss: 9.304 | Accuracy: 0.205078 | 0.863 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 029 | Total loss: 9.301 | Reg loss: 0.013 | Tree loss: 9.301 | Accuracy: 0.208984 | 0.863 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 029 | Total loss: 9.275 | Reg loss: 0.014 | Tree loss: 9.275 | Accuracy: 0.228516 | 0.862 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 029 | Total loss: 9.277 | Reg loss: 0.014 | Tree loss: 9.277 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 029 | Total loss: 9.271 | Reg loss: 0.014 | Tree loss: 9.271 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 029 | Total loss: 9.263 | Reg loss: 0.014 | Tree loss: 9.263 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 029 | Total loss: 9.246 | Reg loss: 0.014 | Tree loss: 9.246 | Accuracy: 0.189453 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 007 / 029 | Total loss: 9.230 | Reg loss: 0.015 | Tree loss: 9.230 | Accuracy: 0.246094 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 029 | Total loss: 9.230 | Reg loss: 0.015 | Tree loss: 9.230 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 029 | Total loss: 9.211 | Reg loss: 0.015 | Tree loss: 9.211 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 029 | Total loss: 9.202 | Reg loss: 0.016 | Tree loss: 9.202 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 029 | Total loss: 9.198 | Reg loss: 0.016 | Tree loss: 9.198 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 029 | Total loss: 9.175 | Reg loss: 0.017 | Tree loss: 9.175 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 029 | Total loss: 9.169 | Reg loss: 0.017 | Tree loss: 9.169 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 029 | Total loss: 9.154 | Reg loss: 0.018 | Tree loss: 9.154 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 029 | Total loss: 9.138 | Reg loss: 0.018 | Tree loss: 9.138 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 029 | Total loss: 9.114 | Reg loss: 0.019 | Tree loss: 9.114 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 029 | Total loss: 9.103 | Reg loss: 0.019 | Tree loss: 9.103 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 029 | Total loss: 9.101 | Reg loss: 0.020 | Tree loss: 9.101 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 029 | Total loss: 9.069 | Reg loss: 0.020 | Tree loss: 9.069 | Accuracy: 0.236328 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 029 | Total loss: 9.065 | Reg loss: 0.020 | Tree loss: 9.065 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 029 | Total loss: 9.052 | Reg loss: 0.021 | Tree loss: 9.052 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 029 | Total loss: 9.030 | Reg loss: 0.021 | Tree loss: 9.030 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 029 | Total loss: 9.032 | Reg loss: 0.022 | Tree loss: 9.032 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 029 | Total loss: 9.007 | Reg loss: 0.022 | Tree loss: 9.007 | Accuracy: 0.207031 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 029 | Total loss: 8.988 | Reg loss: 0.023 | Tree loss: 8.988 | Accuracy: 0.187500 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 029 | Total loss: 8.968 | Reg loss: 0.023 | Tree loss: 8.968 | Accuracy: 0.216797 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 029 | Total loss: 8.966 | Reg loss: 0.024 | Tree loss: 8.966 | Accuracy: 0.175781 | 0.858 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 029 | Total loss: 8.913 | Reg loss: 0.024 | Tree loss: 8.913 | Accuracy: 0.224138 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 029 | Total loss: 9.177 | Reg loss: 0.016 | Tree loss: 9.177 | Accuracy: 0.210938 | 0.864 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 029 | Total loss: 9.149 | Reg loss: 0.016 | Tree loss: 9.149 | Accuracy: 0.244141 | 0.864 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 029 | Total loss: 9.154 | Reg loss: 0.017 | Tree loss: 9.154 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 029 | Total loss: 9.142 | Reg loss: 0.017 | Tree loss: 9.142 | Accuracy: 0.166016 | 0.863 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 029 | Total loss: 9.112 | Reg loss: 0.017 | Tree loss: 9.112 | Accuracy: 0.224609 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 029 | Total loss: 9.106 | Reg loss: 0.017 | Tree loss: 9.106 | Accuracy: 0.205078 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 029 | Total loss: 9.090 | Reg loss: 0.017 | Tree loss: 9.090 | Accuracy: 0.187500 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 029 | Total loss: 9.081 | Reg loss: 0.018 | Tree loss: 9.081 | Accuracy: 0.181641 | 0.863 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 029 | Total loss: 9.070 | Reg loss: 0.018 | Tree loss: 9.070 | Accuracy: 0.189453 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 029 | Total loss: 9.038 | Reg loss: 0.018 | Tree loss: 9.038 | Accuracy: 0.244141 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 029 | Total loss: 9.023 | Reg loss: 0.019 | Tree loss: 9.023 | Accuracy: 0.220703 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 029 | Total loss: 9.006 | Reg loss: 0.019 | Tree loss: 9.006 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 029 | Total loss: 8.989 | Reg loss: 0.020 | Tree loss: 8.989 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 029 | Total loss: 8.980 | Reg loss: 0.020 | Tree loss: 8.980 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 029 | Total loss: 8.956 | Reg loss: 0.020 | Tree loss: 8.956 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 029 | Total loss: 8.951 | Reg loss: 0.021 | Tree loss: 8.951 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 029 | Total loss: 8.920 | Reg loss: 0.021 | Tree loss: 8.920 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 029 | Total loss: 8.890 | Reg loss: 0.022 | Tree loss: 8.890 | Accuracy: 0.226562 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 029 | Total loss: 8.900 | Reg loss: 0.022 | Tree loss: 8.900 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 029 | Total loss: 8.850 | Reg loss: 0.023 | Tree loss: 8.850 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 029 | Total loss: 8.852 | Reg loss: 0.023 | Tree loss: 8.852 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 029 | Total loss: 8.830 | Reg loss: 0.024 | Tree loss: 8.830 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 029 | Total loss: 8.798 | Reg loss: 0.024 | Tree loss: 8.798 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 029 | Total loss: 8.794 | Reg loss: 0.024 | Tree loss: 8.794 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 029 | Total loss: 8.767 | Reg loss: 0.025 | Tree loss: 8.767 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 029 | Total loss: 8.763 | Reg loss: 0.025 | Tree loss: 8.763 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 029 | Total loss: 8.759 | Reg loss: 0.026 | Tree loss: 8.759 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 029 | Total loss: 8.727 | Reg loss: 0.026 | Tree loss: 8.727 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 029 | Total loss: 8.648 | Reg loss: 0.027 | Tree loss: 8.648 | Accuracy: 0.241379 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 029 | Total loss: 9.000 | Reg loss: 0.019 | Tree loss: 9.000 | Accuracy: 0.193359 | 0.863 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 029 | Total loss: 8.990 | Reg loss: 0.019 | Tree loss: 8.990 | Accuracy: 0.181641 | 0.863 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 029 | Total loss: 8.969 | Reg loss: 0.019 | Tree loss: 8.969 | Accuracy: 0.208984 | 0.863 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 029 | Total loss: 8.946 | Reg loss: 0.019 | Tree loss: 8.946 | Accuracy: 0.197266 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 029 | Total loss: 8.949 | Reg loss: 0.020 | Tree loss: 8.949 | Accuracy: 0.183594 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 029 | Total loss: 8.922 | Reg loss: 0.020 | Tree loss: 8.922 | Accuracy: 0.208984 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 029 | Total loss: 8.899 | Reg loss: 0.020 | Tree loss: 8.899 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 029 | Total loss: 8.875 | Reg loss: 0.020 | Tree loss: 8.875 | Accuracy: 0.205078 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 029 | Total loss: 8.856 | Reg loss: 0.021 | Tree loss: 8.856 | Accuracy: 0.218750 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 029 | Total loss: 8.837 | Reg loss: 0.021 | Tree loss: 8.837 | Accuracy: 0.208984 | 0.862 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 029 | Total loss: 8.809 | Reg loss: 0.021 | Tree loss: 8.809 | Accuracy: 0.234375 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 011 / 029 | Total loss: 8.789 | Reg loss: 0.022 | Tree loss: 8.789 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 029 | Total loss: 8.775 | Reg loss: 0.022 | Tree loss: 8.775 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 029 | Total loss: 8.758 | Reg loss: 0.022 | Tree loss: 8.758 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 029 | Total loss: 8.708 | Reg loss: 0.023 | Tree loss: 8.708 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 029 | Total loss: 8.712 | Reg loss: 0.023 | Tree loss: 8.712 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 029 | Total loss: 8.705 | Reg loss: 0.024 | Tree loss: 8.705 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 029 | Total loss: 8.679 | Reg loss: 0.024 | Tree loss: 8.679 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 029 | Total loss: 8.624 | Reg loss: 0.024 | Tree loss: 8.624 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 029 | Total loss: 8.625 | Reg loss: 0.025 | Tree loss: 8.625 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 029 | Total loss: 8.584 | Reg loss: 0.025 | Tree loss: 8.584 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 029 | Total loss: 8.569 | Reg loss: 0.026 | Tree loss: 8.569 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 029 | Total loss: 8.570 | Reg loss: 0.026 | Tree loss: 8.570 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 029 | Total loss: 8.553 | Reg loss: 0.026 | Tree loss: 8.553 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 029 | Total loss: 8.518 | Reg loss: 0.027 | Tree loss: 8.518 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 029 | Total loss: 8.504 | Reg loss: 0.027 | Tree loss: 8.504 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 029 | Total loss: 8.484 | Reg loss: 0.028 | Tree loss: 8.484 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 029 | Total loss: 8.466 | Reg loss: 0.028 | Tree loss: 8.466 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 029 | Total loss: 8.532 | Reg loss: 0.028 | Tree loss: 8.532 | Accuracy: 0.137931 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 029 | Total loss: 8.785 | Reg loss: 0.022 | Tree loss: 8.785 | Accuracy: 0.195312 | 0.864 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 029 | Total loss: 8.763 | Reg loss: 0.022 | Tree loss: 8.763 | Accuracy: 0.236328 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 029 | Total loss: 8.770 | Reg loss: 0.022 | Tree loss: 8.770 | Accuracy: 0.203125 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 029 | Total loss: 8.717 | Reg loss: 0.022 | Tree loss: 8.717 | Accuracy: 0.203125 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 029 | Total loss: 8.701 | Reg loss: 0.022 | Tree loss: 8.701 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 029 | Total loss: 8.679 | Reg loss: 0.022 | Tree loss: 8.679 | Accuracy: 0.228516 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 029 | Total loss: 8.662 | Reg loss: 0.022 | Tree loss: 8.662 | Accuracy: 0.193359 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 029 | Total loss: 8.632 | Reg loss: 0.023 | Tree loss: 8.632 | Accuracy: 0.195312 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 029 | Total loss: 8.620 | Reg loss: 0.023 | Tree loss: 8.620 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 029 | Total loss: 8.607 | Reg loss: 0.023 | Tree loss: 8.607 | Accuracy: 0.191406 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 029 | Total loss: 8.569 | Reg loss: 0.023 | Tree loss: 8.569 | Accuracy: 0.210938 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 029 | Total loss: 8.560 | Reg loss: 0.024 | Tree loss: 8.560 | Accuracy: 0.181641 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 029 | Total loss: 8.521 | Reg loss: 0.024 | Tree loss: 8.521 | Accuracy: 0.208984 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 029 | Total loss: 8.499 | Reg loss: 0.024 | Tree loss: 8.499 | Accuracy: 0.207031 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 029 | Total loss: 8.490 | Reg loss: 0.025 | Tree loss: 8.490 | Accuracy: 0.207031 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 029 | Total loss: 8.443 | Reg loss: 0.025 | Tree loss: 8.443 | Accuracy: 0.207031 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 029 | Total loss: 8.452 | Reg loss: 0.025 | Tree loss: 8.452 | Accuracy: 0.169922 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 029 | Total loss: 8.385 | Reg loss: 0.026 | Tree loss: 8.385 | Accuracy: 0.242188 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 029 | Total loss: 8.389 | Reg loss: 0.026 | Tree loss: 8.389 | Accuracy: 0.238281 | 0.863 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 029 | Total loss: 8.369 | Reg loss: 0.026 | Tree loss: 8.369 | Accuracy: 0.189453 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 029 | Total loss: 8.345 | Reg loss: 0.027 | Tree loss: 8.345 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 029 | Total loss: 8.328 | Reg loss: 0.027 | Tree loss: 8.328 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 029 | Total loss: 8.315 | Reg loss: 0.028 | Tree loss: 8.315 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 029 | Total loss: 8.277 | Reg loss: 0.028 | Tree loss: 8.277 | Accuracy: 0.210938 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 029 | Total loss: 8.285 | Reg loss: 0.028 | Tree loss: 8.285 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 029 | Total loss: 8.234 | Reg loss: 0.029 | Tree loss: 8.234 | Accuracy: 0.222656 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 029 | Total loss: 8.231 | Reg loss: 0.029 | Tree loss: 8.231 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 029 | Total loss: 8.210 | Reg loss: 0.029 | Tree loss: 8.210 | Accuracy: 0.179688 | 0.862 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 029 | Total loss: 8.170 | Reg loss: 0.030 | Tree loss: 8.170 | Accuracy: 0.155172 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 029 | Total loss: 8.525 | Reg loss: 0.024 | Tree loss: 8.525 | Accuracy: 0.226562 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 029 | Total loss: 8.529 | Reg loss: 0.024 | Tree loss: 8.529 | Accuracy: 0.193359 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 029 | Total loss: 8.503 | Reg loss: 0.024 | Tree loss: 8.503 | Accuracy: 0.248047 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 029 | Total loss: 8.490 | Reg loss: 0.024 | Tree loss: 8.490 | Accuracy: 0.201172 | 0.865 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 029 | Total loss: 8.449 | Reg loss: 0.024 | Tree loss: 8.449 | Accuracy: 0.228516 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 029 | Total loss: 8.458 | Reg loss: 0.024 | Tree loss: 8.458 | Accuracy: 0.197266 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 029 | Total loss: 8.429 | Reg loss: 0.024 | Tree loss: 8.429 | Accuracy: 0.183594 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 029 | Total loss: 8.390 | Reg loss: 0.024 | Tree loss: 8.390 | Accuracy: 0.197266 | 0.866 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 029 | Total loss: 8.363 | Reg loss: 0.025 | Tree loss: 8.363 | Accuracy: 0.195312 | 0.865 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 029 | Total loss: 8.351 | Reg loss: 0.025 | Tree loss: 8.351 | Accuracy: 0.197266 | 0.865 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 029 | Total loss: 8.337 | Reg loss: 0.025 | Tree loss: 8.337 | Accuracy: 0.175781 | 0.865 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 029 | Total loss: 8.286 | Reg loss: 0.025 | Tree loss: 8.286 | Accuracy: 0.195312 | 0.865 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 029 | Total loss: 8.262 | Reg loss: 0.026 | Tree loss: 8.262 | Accuracy: 0.201172 | 0.864 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 029 | Total loss: 8.233 | Reg loss: 0.026 | Tree loss: 8.233 | Accuracy: 0.224609 | 0.864 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 029 | Total loss: 8.196 | Reg loss: 0.026 | Tree loss: 8.196 | Accuracy: 0.230469 | 0.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 015 / 029 | Total loss: 8.196 | Reg loss: 0.027 | Tree loss: 8.196 | Accuracy: 0.181641 | 0.864 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 029 | Total loss: 8.162 | Reg loss: 0.027 | Tree loss: 8.162 | Accuracy: 0.189453 | 0.864 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 029 | Total loss: 8.122 | Reg loss: 0.027 | Tree loss: 8.122 | Accuracy: 0.230469 | 0.864 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 029 | Total loss: 8.113 | Reg loss: 0.028 | Tree loss: 8.113 | Accuracy: 0.167969 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 029 | Total loss: 8.111 | Reg loss: 0.028 | Tree loss: 8.111 | Accuracy: 0.212891 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 029 | Total loss: 8.069 | Reg loss: 0.028 | Tree loss: 8.069 | Accuracy: 0.207031 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 029 | Total loss: 8.053 | Reg loss: 0.028 | Tree loss: 8.053 | Accuracy: 0.214844 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 029 | Total loss: 8.055 | Reg loss: 0.029 | Tree loss: 8.055 | Accuracy: 0.201172 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 029 | Total loss: 8.026 | Reg loss: 0.029 | Tree loss: 8.026 | Accuracy: 0.179688 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 029 | Total loss: 7.966 | Reg loss: 0.029 | Tree loss: 7.966 | Accuracy: 0.218750 | 0.863 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 029 | Total loss: 7.976 | Reg loss: 0.030 | Tree loss: 7.976 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 029 | Total loss: 7.952 | Reg loss: 0.030 | Tree loss: 7.952 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 029 | Total loss: 7.962 | Reg loss: 0.030 | Tree loss: 7.962 | Accuracy: 0.183594 | 0.862 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 029 | Total loss: 7.937 | Reg loss: 0.031 | Tree loss: 7.937 | Accuracy: 0.206897 | 0.862 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 029 | Total loss: 8.265 | Reg loss: 0.025 | Tree loss: 8.265 | Accuracy: 0.220703 | 0.867 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 029 | Total loss: 8.275 | Reg loss: 0.025 | Tree loss: 8.275 | Accuracy: 0.193359 | 0.867 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 029 | Total loss: 8.237 | Reg loss: 0.025 | Tree loss: 8.237 | Accuracy: 0.214844 | 0.866 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 029 | Total loss: 8.222 | Reg loss: 0.026 | Tree loss: 8.222 | Accuracy: 0.189453 | 0.866 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 029 | Total loss: 8.193 | Reg loss: 0.026 | Tree loss: 8.193 | Accuracy: 0.218750 | 0.866 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 029 | Total loss: 8.192 | Reg loss: 0.026 | Tree loss: 8.192 | Accuracy: 0.201172 | 0.866 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 029 | Total loss: 8.142 | Reg loss: 0.026 | Tree loss: 8.142 | Accuracy: 0.171875 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 029 | Total loss: 8.093 | Reg loss: 0.026 | Tree loss: 8.093 | Accuracy: 0.246094 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 029 | Total loss: 8.104 | Reg loss: 0.026 | Tree loss: 8.104 | Accuracy: 0.216797 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 029 | Total loss: 8.068 | Reg loss: 0.026 | Tree loss: 8.068 | Accuracy: 0.218750 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 029 | Total loss: 8.072 | Reg loss: 0.027 | Tree loss: 8.072 | Accuracy: 0.181641 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 029 | Total loss: 8.023 | Reg loss: 0.027 | Tree loss: 8.023 | Accuracy: 0.185547 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 029 | Total loss: 7.994 | Reg loss: 0.027 | Tree loss: 7.994 | Accuracy: 0.214844 | 0.865 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 029 | Total loss: 7.967 | Reg loss: 0.027 | Tree loss: 7.967 | Accuracy: 0.201172 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 029 | Total loss: 7.967 | Reg loss: 0.028 | Tree loss: 7.967 | Accuracy: 0.191406 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 029 | Total loss: 7.926 | Reg loss: 0.028 | Tree loss: 7.926 | Accuracy: 0.207031 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 029 | Total loss: 7.885 | Reg loss: 0.028 | Tree loss: 7.885 | Accuracy: 0.212891 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 029 | Total loss: 7.857 | Reg loss: 0.028 | Tree loss: 7.857 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 029 | Total loss: 7.860 | Reg loss: 0.029 | Tree loss: 7.860 | Accuracy: 0.187500 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 029 | Total loss: 7.865 | Reg loss: 0.029 | Tree loss: 7.865 | Accuracy: 0.173828 | 0.864 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 029 | Total loss: 7.828 | Reg loss: 0.029 | Tree loss: 7.828 | Accuracy: 0.201172 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 029 | Total loss: 7.806 | Reg loss: 0.029 | Tree loss: 7.806 | Accuracy: 0.205078 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 029 | Total loss: 7.770 | Reg loss: 0.030 | Tree loss: 7.770 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 029 | Total loss: 7.716 | Reg loss: 0.030 | Tree loss: 7.716 | Accuracy: 0.205078 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 029 | Total loss: 7.705 | Reg loss: 0.030 | Tree loss: 7.705 | Accuracy: 0.220703 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 029 | Total loss: 7.686 | Reg loss: 0.031 | Tree loss: 7.686 | Accuracy: 0.189453 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 029 | Total loss: 7.694 | Reg loss: 0.031 | Tree loss: 7.694 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 029 | Total loss: 7.680 | Reg loss: 0.031 | Tree loss: 7.680 | Accuracy: 0.189453 | 0.863 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 029 | Total loss: 7.625 | Reg loss: 0.031 | Tree loss: 7.625 | Accuracy: 0.120690 | 0.863 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 029 | Total loss: 7.999 | Reg loss: 0.027 | Tree loss: 7.999 | Accuracy: 0.199219 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 029 | Total loss: 7.988 | Reg loss: 0.027 | Tree loss: 7.988 | Accuracy: 0.197266 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 029 | Total loss: 7.977 | Reg loss: 0.027 | Tree loss: 7.977 | Accuracy: 0.193359 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 029 | Total loss: 7.963 | Reg loss: 0.027 | Tree loss: 7.963 | Accuracy: 0.187500 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 029 | Total loss: 7.906 | Reg loss: 0.027 | Tree loss: 7.906 | Accuracy: 0.201172 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 029 | Total loss: 7.902 | Reg loss: 0.027 | Tree loss: 7.902 | Accuracy: 0.203125 | 0.866 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 029 | Total loss: 7.873 | Reg loss: 0.027 | Tree loss: 7.873 | Accuracy: 0.205078 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 029 | Total loss: 7.833 | Reg loss: 0.027 | Tree loss: 7.833 | Accuracy: 0.207031 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 029 | Total loss: 7.825 | Reg loss: 0.027 | Tree loss: 7.825 | Accuracy: 0.230469 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 029 | Total loss: 7.804 | Reg loss: 0.028 | Tree loss: 7.804 | Accuracy: 0.199219 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 029 | Total loss: 7.735 | Reg loss: 0.028 | Tree loss: 7.735 | Accuracy: 0.246094 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 029 | Total loss: 7.718 | Reg loss: 0.028 | Tree loss: 7.718 | Accuracy: 0.205078 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 029 | Total loss: 7.724 | Reg loss: 0.028 | Tree loss: 7.724 | Accuracy: 0.189453 | 0.865 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 029 | Total loss: 7.704 | Reg loss: 0.028 | Tree loss: 7.704 | Accuracy: 0.201172 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 029 | Total loss: 7.686 | Reg loss: 0.029 | Tree loss: 7.686 | Accuracy: 0.185547 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 029 | Total loss: 7.623 | Reg loss: 0.029 | Tree loss: 7.623 | Accuracy: 0.195312 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 029 | Total loss: 7.628 | Reg loss: 0.029 | Tree loss: 7.628 | Accuracy: 0.197266 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 029 | Total loss: 7.617 | Reg loss: 0.029 | Tree loss: 7.617 | Accuracy: 0.167969 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 029 | Total loss: 7.605 | Reg loss: 0.029 | Tree loss: 7.605 | Accuracy: 0.191406 | 0.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 019 / 029 | Total loss: 7.578 | Reg loss: 0.030 | Tree loss: 7.578 | Accuracy: 0.179688 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 029 | Total loss: 7.536 | Reg loss: 0.030 | Tree loss: 7.536 | Accuracy: 0.193359 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 029 | Total loss: 7.527 | Reg loss: 0.030 | Tree loss: 7.527 | Accuracy: 0.173828 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 029 | Total loss: 7.512 | Reg loss: 0.030 | Tree loss: 7.512 | Accuracy: 0.205078 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 029 | Total loss: 7.516 | Reg loss: 0.030 | Tree loss: 7.516 | Accuracy: 0.177734 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 029 | Total loss: 7.449 | Reg loss: 0.031 | Tree loss: 7.449 | Accuracy: 0.210938 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 029 | Total loss: 7.457 | Reg loss: 0.031 | Tree loss: 7.457 | Accuracy: 0.212891 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 029 | Total loss: 7.424 | Reg loss: 0.031 | Tree loss: 7.424 | Accuracy: 0.205078 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 029 | Total loss: 7.412 | Reg loss: 0.031 | Tree loss: 7.412 | Accuracy: 0.240234 | 0.864 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 029 | Total loss: 7.362 | Reg loss: 0.031 | Tree loss: 7.362 | Accuracy: 0.241379 | 0.863 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 029 | Total loss: 7.748 | Reg loss: 0.028 | Tree loss: 7.748 | Accuracy: 0.207031 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 029 | Total loss: 7.681 | Reg loss: 0.028 | Tree loss: 7.681 | Accuracy: 0.210938 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 029 | Total loss: 7.655 | Reg loss: 0.028 | Tree loss: 7.655 | Accuracy: 0.224609 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 029 | Total loss: 7.657 | Reg loss: 0.028 | Tree loss: 7.657 | Accuracy: 0.195312 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 029 | Total loss: 7.598 | Reg loss: 0.028 | Tree loss: 7.598 | Accuracy: 0.195312 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 029 | Total loss: 7.621 | Reg loss: 0.028 | Tree loss: 7.621 | Accuracy: 0.179688 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 029 | Total loss: 7.605 | Reg loss: 0.028 | Tree loss: 7.605 | Accuracy: 0.199219 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 029 | Total loss: 7.575 | Reg loss: 0.028 | Tree loss: 7.575 | Accuracy: 0.183594 | 0.866 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 029 | Total loss: 7.552 | Reg loss: 0.028 | Tree loss: 7.552 | Accuracy: 0.195312 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 029 | Total loss: 7.543 | Reg loss: 0.028 | Tree loss: 7.543 | Accuracy: 0.185547 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 029 | Total loss: 7.476 | Reg loss: 0.029 | Tree loss: 7.476 | Accuracy: 0.201172 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 029 | Total loss: 7.481 | Reg loss: 0.029 | Tree loss: 7.481 | Accuracy: 0.214844 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 029 | Total loss: 7.448 | Reg loss: 0.029 | Tree loss: 7.448 | Accuracy: 0.216797 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 029 | Total loss: 7.421 | Reg loss: 0.029 | Tree loss: 7.421 | Accuracy: 0.193359 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 029 | Total loss: 7.404 | Reg loss: 0.029 | Tree loss: 7.404 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 029 | Total loss: 7.389 | Reg loss: 0.029 | Tree loss: 7.389 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 029 | Total loss: 7.351 | Reg loss: 0.029 | Tree loss: 7.351 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 029 | Total loss: 7.352 | Reg loss: 0.030 | Tree loss: 7.352 | Accuracy: 0.197266 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 029 | Total loss: 7.336 | Reg loss: 0.030 | Tree loss: 7.336 | Accuracy: 0.208984 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 029 | Total loss: 7.322 | Reg loss: 0.030 | Tree loss: 7.322 | Accuracy: 0.185547 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 029 | Total loss: 7.248 | Reg loss: 0.030 | Tree loss: 7.248 | Accuracy: 0.236328 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 029 | Total loss: 7.267 | Reg loss: 0.030 | Tree loss: 7.267 | Accuracy: 0.199219 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 029 | Total loss: 7.239 | Reg loss: 0.030 | Tree loss: 7.239 | Accuracy: 0.183594 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 029 | Total loss: 7.242 | Reg loss: 0.031 | Tree loss: 7.242 | Accuracy: 0.201172 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 029 | Total loss: 7.198 | Reg loss: 0.031 | Tree loss: 7.198 | Accuracy: 0.187500 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 029 | Total loss: 7.176 | Reg loss: 0.031 | Tree loss: 7.176 | Accuracy: 0.220703 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 029 | Total loss: 7.194 | Reg loss: 0.031 | Tree loss: 7.194 | Accuracy: 0.179688 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 029 | Total loss: 7.131 | Reg loss: 0.031 | Tree loss: 7.131 | Accuracy: 0.212891 | 0.864 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 029 | Total loss: 7.203 | Reg loss: 0.031 | Tree loss: 7.203 | Accuracy: 0.155172 | 0.864 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 029 | Total loss: 7.428 | Reg loss: 0.028 | Tree loss: 7.428 | Accuracy: 0.197266 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 029 | Total loss: 7.412 | Reg loss: 0.028 | Tree loss: 7.412 | Accuracy: 0.210938 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 029 | Total loss: 7.392 | Reg loss: 0.028 | Tree loss: 7.392 | Accuracy: 0.230469 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 029 | Total loss: 7.371 | Reg loss: 0.028 | Tree loss: 7.371 | Accuracy: 0.205078 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 029 | Total loss: 7.354 | Reg loss: 0.028 | Tree loss: 7.354 | Accuracy: 0.218750 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 029 | Total loss: 7.330 | Reg loss: 0.028 | Tree loss: 7.330 | Accuracy: 0.197266 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 029 | Total loss: 7.340 | Reg loss: 0.029 | Tree loss: 7.340 | Accuracy: 0.166016 | 0.866 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 029 | Total loss: 7.311 | Reg loss: 0.029 | Tree loss: 7.311 | Accuracy: 0.164062 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 029 | Total loss: 7.303 | Reg loss: 0.029 | Tree loss: 7.303 | Accuracy: 0.173828 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 029 | Total loss: 7.233 | Reg loss: 0.029 | Tree loss: 7.233 | Accuracy: 0.220703 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 029 | Total loss: 7.244 | Reg loss: 0.029 | Tree loss: 7.244 | Accuracy: 0.193359 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 029 | Total loss: 7.170 | Reg loss: 0.029 | Tree loss: 7.170 | Accuracy: 0.214844 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 029 | Total loss: 7.161 | Reg loss: 0.029 | Tree loss: 7.161 | Accuracy: 0.191406 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 029 | Total loss: 7.166 | Reg loss: 0.029 | Tree loss: 7.166 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 029 | Total loss: 7.134 | Reg loss: 0.029 | Tree loss: 7.134 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 029 | Total loss: 7.117 | Reg loss: 0.029 | Tree loss: 7.117 | Accuracy: 0.189453 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 029 | Total loss: 7.072 | Reg loss: 0.030 | Tree loss: 7.072 | Accuracy: 0.224609 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 029 | Total loss: 7.071 | Reg loss: 0.030 | Tree loss: 7.071 | Accuracy: 0.207031 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 029 | Total loss: 7.103 | Reg loss: 0.030 | Tree loss: 7.103 | Accuracy: 0.199219 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 029 | Total loss: 7.029 | Reg loss: 0.030 | Tree loss: 7.029 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 029 | Total loss: 7.006 | Reg loss: 0.030 | Tree loss: 7.006 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 029 | Total loss: 7.026 | Reg loss: 0.030 | Tree loss: 7.026 | Accuracy: 0.181641 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 029 | Total loss: 6.962 | Reg loss: 0.030 | Tree loss: 6.962 | Accuracy: 0.210938 | 0.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 023 / 029 | Total loss: 6.939 | Reg loss: 0.030 | Tree loss: 6.939 | Accuracy: 0.208984 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 029 | Total loss: 6.942 | Reg loss: 0.031 | Tree loss: 6.942 | Accuracy: 0.185547 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 029 | Total loss: 6.909 | Reg loss: 0.031 | Tree loss: 6.909 | Accuracy: 0.208984 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 029 | Total loss: 6.905 | Reg loss: 0.031 | Tree loss: 6.905 | Accuracy: 0.216797 | 0.864 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 029 | Total loss: 6.864 | Reg loss: 0.031 | Tree loss: 6.864 | Accuracy: 0.199219 | 0.863 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 029 | Total loss: 6.906 | Reg loss: 0.031 | Tree loss: 6.906 | Accuracy: 0.258621 | 0.863 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 029 | Total loss: 7.181 | Reg loss: 0.029 | Tree loss: 7.181 | Accuracy: 0.179688 | 0.866 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 029 | Total loss: 7.147 | Reg loss: 0.028 | Tree loss: 7.147 | Accuracy: 0.212891 | 0.866 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 029 | Total loss: 7.127 | Reg loss: 0.029 | Tree loss: 7.127 | Accuracy: 0.185547 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 029 | Total loss: 7.114 | Reg loss: 0.029 | Tree loss: 7.114 | Accuracy: 0.201172 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 029 | Total loss: 7.094 | Reg loss: 0.029 | Tree loss: 7.094 | Accuracy: 0.205078 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 029 | Total loss: 7.057 | Reg loss: 0.029 | Tree loss: 7.057 | Accuracy: 0.226562 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 029 | Total loss: 7.048 | Reg loss: 0.029 | Tree loss: 7.048 | Accuracy: 0.181641 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 029 | Total loss: 7.035 | Reg loss: 0.029 | Tree loss: 7.035 | Accuracy: 0.185547 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 029 | Total loss: 7.014 | Reg loss: 0.029 | Tree loss: 7.014 | Accuracy: 0.193359 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 029 | Total loss: 6.954 | Reg loss: 0.029 | Tree loss: 6.954 | Accuracy: 0.207031 | 0.865 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 029 | Total loss: 6.968 | Reg loss: 0.029 | Tree loss: 6.968 | Accuracy: 0.197266 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 029 | Total loss: 6.937 | Reg loss: 0.029 | Tree loss: 6.937 | Accuracy: 0.193359 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 029 | Total loss: 6.916 | Reg loss: 0.029 | Tree loss: 6.916 | Accuracy: 0.207031 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 029 | Total loss: 6.886 | Reg loss: 0.029 | Tree loss: 6.886 | Accuracy: 0.179688 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 029 | Total loss: 6.872 | Reg loss: 0.029 | Tree loss: 6.872 | Accuracy: 0.173828 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 029 | Total loss: 6.817 | Reg loss: 0.029 | Tree loss: 6.817 | Accuracy: 0.240234 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 029 | Total loss: 6.778 | Reg loss: 0.029 | Tree loss: 6.778 | Accuracy: 0.242188 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 029 | Total loss: 6.828 | Reg loss: 0.029 | Tree loss: 6.828 | Accuracy: 0.191406 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 029 | Total loss: 6.797 | Reg loss: 0.030 | Tree loss: 6.797 | Accuracy: 0.205078 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 029 | Total loss: 6.755 | Reg loss: 0.030 | Tree loss: 6.755 | Accuracy: 0.205078 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 029 | Total loss: 6.761 | Reg loss: 0.030 | Tree loss: 6.761 | Accuracy: 0.212891 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 029 | Total loss: 6.706 | Reg loss: 0.030 | Tree loss: 6.706 | Accuracy: 0.195312 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 029 | Total loss: 6.681 | Reg loss: 0.030 | Tree loss: 6.681 | Accuracy: 0.214844 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 029 | Total loss: 6.689 | Reg loss: 0.030 | Tree loss: 6.689 | Accuracy: 0.210938 | 0.864 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 029 | Total loss: 6.637 | Reg loss: 0.030 | Tree loss: 6.637 | Accuracy: 0.208984 | 0.863 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 029 | Total loss: 6.640 | Reg loss: 0.030 | Tree loss: 6.640 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 029 | Total loss: 6.595 | Reg loss: 0.030 | Tree loss: 6.595 | Accuracy: 0.222656 | 0.863 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 029 | Total loss: 6.638 | Reg loss: 0.030 | Tree loss: 6.638 | Accuracy: 0.169922 | 0.863 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 029 | Total loss: 6.585 | Reg loss: 0.031 | Tree loss: 6.585 | Accuracy: 0.241379 | 0.863 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 029 | Total loss: 6.884 | Reg loss: 0.028 | Tree loss: 6.884 | Accuracy: 0.185547 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 029 | Total loss: 6.888 | Reg loss: 0.028 | Tree loss: 6.888 | Accuracy: 0.193359 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 029 | Total loss: 6.811 | Reg loss: 0.028 | Tree loss: 6.811 | Accuracy: 0.234375 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 029 | Total loss: 6.868 | Reg loss: 0.028 | Tree loss: 6.868 | Accuracy: 0.167969 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 029 | Total loss: 6.782 | Reg loss: 0.028 | Tree loss: 6.782 | Accuracy: 0.240234 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 029 | Total loss: 6.813 | Reg loss: 0.028 | Tree loss: 6.813 | Accuracy: 0.175781 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 029 | Total loss: 6.737 | Reg loss: 0.028 | Tree loss: 6.737 | Accuracy: 0.224609 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 029 | Total loss: 6.745 | Reg loss: 0.028 | Tree loss: 6.745 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 029 | Total loss: 6.677 | Reg loss: 0.028 | Tree loss: 6.677 | Accuracy: 0.193359 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 029 | Total loss: 6.713 | Reg loss: 0.028 | Tree loss: 6.713 | Accuracy: 0.189453 | 0.865 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 029 | Total loss: 6.639 | Reg loss: 0.028 | Tree loss: 6.639 | Accuracy: 0.218750 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 029 | Total loss: 6.604 | Reg loss: 0.029 | Tree loss: 6.604 | Accuracy: 0.210938 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 029 | Total loss: 6.629 | Reg loss: 0.029 | Tree loss: 6.629 | Accuracy: 0.216797 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 029 | Total loss: 6.587 | Reg loss: 0.029 | Tree loss: 6.587 | Accuracy: 0.191406 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 029 | Total loss: 6.579 | Reg loss: 0.029 | Tree loss: 6.579 | Accuracy: 0.208984 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 029 | Total loss: 6.594 | Reg loss: 0.029 | Tree loss: 6.594 | Accuracy: 0.183594 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 029 | Total loss: 6.555 | Reg loss: 0.029 | Tree loss: 6.555 | Accuracy: 0.160156 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 029 | Total loss: 6.503 | Reg loss: 0.029 | Tree loss: 6.503 | Accuracy: 0.214844 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 029 | Total loss: 6.466 | Reg loss: 0.029 | Tree loss: 6.466 | Accuracy: 0.214844 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 029 | Total loss: 6.485 | Reg loss: 0.029 | Tree loss: 6.485 | Accuracy: 0.173828 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 029 | Total loss: 6.413 | Reg loss: 0.029 | Tree loss: 6.413 | Accuracy: 0.226562 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 029 | Total loss: 6.406 | Reg loss: 0.029 | Tree loss: 6.406 | Accuracy: 0.230469 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 029 | Total loss: 6.412 | Reg loss: 0.029 | Tree loss: 6.412 | Accuracy: 0.193359 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 029 | Total loss: 6.408 | Reg loss: 0.029 | Tree loss: 6.408 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 029 | Total loss: 6.346 | Reg loss: 0.030 | Tree loss: 6.346 | Accuracy: 0.250000 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 029 | Total loss: 6.337 | Reg loss: 0.030 | Tree loss: 6.337 | Accuracy: 0.173828 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 029 | Total loss: 6.293 | Reg loss: 0.030 | Tree loss: 6.293 | Accuracy: 0.210938 | 0.864 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 027 / 029 | Total loss: 6.294 | Reg loss: 0.030 | Tree loss: 6.294 | Accuracy: 0.220703 | 0.864 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 029 | Total loss: 6.260 | Reg loss: 0.030 | Tree loss: 6.260 | Accuracy: 0.206897 | 0.863 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 029 | Total loss: 6.631 | Reg loss: 0.028 | Tree loss: 6.631 | Accuracy: 0.195312 | 0.865 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 029 | Total loss: 6.573 | Reg loss: 0.028 | Tree loss: 6.573 | Accuracy: 0.208984 | 0.865 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 029 | Total loss: 6.555 | Reg loss: 0.028 | Tree loss: 6.555 | Accuracy: 0.169922 | 0.864 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 029 | Total loss: 6.540 | Reg loss: 0.028 | Tree loss: 6.540 | Accuracy: 0.210938 | 0.864 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 029 | Total loss: 6.509 | Reg loss: 0.028 | Tree loss: 6.509 | Accuracy: 0.193359 | 0.864 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 029 | Total loss: 6.482 | Reg loss: 0.028 | Tree loss: 6.482 | Accuracy: 0.199219 | 0.864 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 029 | Total loss: 6.454 | Reg loss: 0.028 | Tree loss: 6.454 | Accuracy: 0.203125 | 0.864 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 029 | Total loss: 6.447 | Reg loss: 0.028 | Tree loss: 6.447 | Accuracy: 0.189453 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 029 | Total loss: 6.401 | Reg loss: 0.028 | Tree loss: 6.401 | Accuracy: 0.218750 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 029 | Total loss: 6.366 | Reg loss: 0.028 | Tree loss: 6.366 | Accuracy: 0.220703 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 029 | Total loss: 6.351 | Reg loss: 0.028 | Tree loss: 6.351 | Accuracy: 0.232422 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 029 | Total loss: 6.331 | Reg loss: 0.028 | Tree loss: 6.331 | Accuracy: 0.212891 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 029 | Total loss: 6.308 | Reg loss: 0.028 | Tree loss: 6.308 | Accuracy: 0.212891 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 029 | Total loss: 6.294 | Reg loss: 0.028 | Tree loss: 6.294 | Accuracy: 0.185547 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 029 | Total loss: 6.303 | Reg loss: 0.028 | Tree loss: 6.303 | Accuracy: 0.187500 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 029 | Total loss: 6.250 | Reg loss: 0.028 | Tree loss: 6.250 | Accuracy: 0.224609 | 0.863 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 029 | Total loss: 6.232 | Reg loss: 0.028 | Tree loss: 6.232 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 029 | Total loss: 6.186 | Reg loss: 0.028 | Tree loss: 6.186 | Accuracy: 0.210938 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 029 | Total loss: 6.187 | Reg loss: 0.028 | Tree loss: 6.187 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 029 | Total loss: 6.163 | Reg loss: 0.028 | Tree loss: 6.163 | Accuracy: 0.220703 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 029 | Total loss: 6.149 | Reg loss: 0.029 | Tree loss: 6.149 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 029 | Total loss: 6.103 | Reg loss: 0.029 | Tree loss: 6.103 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 029 | Total loss: 6.093 | Reg loss: 0.029 | Tree loss: 6.093 | Accuracy: 0.164062 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 029 | Total loss: 6.048 | Reg loss: 0.029 | Tree loss: 6.048 | Accuracy: 0.224609 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 029 | Total loss: 6.038 | Reg loss: 0.029 | Tree loss: 6.038 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 029 | Total loss: 6.001 | Reg loss: 0.029 | Tree loss: 6.001 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 029 | Total loss: 5.972 | Reg loss: 0.029 | Tree loss: 5.972 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 029 | Total loss: 5.952 | Reg loss: 0.029 | Tree loss: 5.952 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 029 | Total loss: 5.897 | Reg loss: 0.029 | Tree loss: 5.897 | Accuracy: 0.327586 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 029 | Total loss: 6.267 | Reg loss: 0.027 | Tree loss: 6.267 | Accuracy: 0.226562 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 029 | Total loss: 6.275 | Reg loss: 0.027 | Tree loss: 6.275 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 029 | Total loss: 6.203 | Reg loss: 0.027 | Tree loss: 6.203 | Accuracy: 0.218750 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 029 | Total loss: 6.252 | Reg loss: 0.027 | Tree loss: 6.252 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 029 | Total loss: 6.169 | Reg loss: 0.027 | Tree loss: 6.169 | Accuracy: 0.220703 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 029 | Total loss: 6.162 | Reg loss: 0.027 | Tree loss: 6.162 | Accuracy: 0.193359 | 0.863 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 029 | Total loss: 6.123 | Reg loss: 0.027 | Tree loss: 6.123 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 029 | Total loss: 6.083 | Reg loss: 0.027 | Tree loss: 6.083 | Accuracy: 0.253906 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 029 | Total loss: 6.114 | Reg loss: 0.027 | Tree loss: 6.114 | Accuracy: 0.193359 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 029 | Total loss: 6.012 | Reg loss: 0.027 | Tree loss: 6.012 | Accuracy: 0.234375 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 029 | Total loss: 6.038 | Reg loss: 0.027 | Tree loss: 6.038 | Accuracy: 0.183594 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 029 | Total loss: 6.049 | Reg loss: 0.027 | Tree loss: 6.049 | Accuracy: 0.179688 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 029 | Total loss: 6.000 | Reg loss: 0.027 | Tree loss: 6.000 | Accuracy: 0.191406 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 029 | Total loss: 5.920 | Reg loss: 0.027 | Tree loss: 5.920 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 029 | Total loss: 5.943 | Reg loss: 0.027 | Tree loss: 5.943 | Accuracy: 0.232422 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 029 | Total loss: 5.908 | Reg loss: 0.027 | Tree loss: 5.908 | Accuracy: 0.195312 | 0.862 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 029 | Total loss: 5.877 | Reg loss: 0.028 | Tree loss: 5.877 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 029 | Total loss: 5.879 | Reg loss: 0.028 | Tree loss: 5.879 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 029 | Total loss: 5.876 | Reg loss: 0.028 | Tree loss: 5.876 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 029 | Total loss: 5.819 | Reg loss: 0.028 | Tree loss: 5.819 | Accuracy: 0.214844 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 029 | Total loss: 5.828 | Reg loss: 0.028 | Tree loss: 5.828 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 029 | Total loss: 5.761 | Reg loss: 0.028 | Tree loss: 5.761 | Accuracy: 0.214844 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 029 | Total loss: 5.726 | Reg loss: 0.028 | Tree loss: 5.726 | Accuracy: 0.236328 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 029 | Total loss: 5.712 | Reg loss: 0.029 | Tree loss: 5.712 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 029 | Total loss: 5.705 | Reg loss: 0.029 | Tree loss: 5.705 | Accuracy: 0.177734 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 029 | Total loss: 5.700 | Reg loss: 0.029 | Tree loss: 5.700 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 029 | Total loss: 5.613 | Reg loss: 0.029 | Tree loss: 5.613 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 029 | Total loss: 5.607 | Reg loss: 0.029 | Tree loss: 5.607 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 029 | Total loss: 5.495 | Reg loss: 0.029 | Tree loss: 5.495 | Accuracy: 0.155172 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 029 | Total loss: 6.003 | Reg loss: 0.026 | Tree loss: 6.003 | Accuracy: 0.187500 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 029 | Total loss: 5.938 | Reg loss: 0.026 | Tree loss: 5.938 | Accuracy: 0.224609 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 029 | Total loss: 5.944 | Reg loss: 0.026 | Tree loss: 5.944 | Accuracy: 0.183594 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 029 | Total loss: 5.876 | Reg loss: 0.026 | Tree loss: 5.876 | Accuracy: 0.197266 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 029 | Total loss: 5.831 | Reg loss: 0.026 | Tree loss: 5.831 | Accuracy: 0.195312 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 029 | Total loss: 5.814 | Reg loss: 0.026 | Tree loss: 5.814 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 029 | Total loss: 5.807 | Reg loss: 0.026 | Tree loss: 5.807 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 029 | Total loss: 5.828 | Reg loss: 0.026 | Tree loss: 5.828 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 029 | Total loss: 5.783 | Reg loss: 0.026 | Tree loss: 5.783 | Accuracy: 0.185547 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 029 | Total loss: 5.723 | Reg loss: 0.026 | Tree loss: 5.723 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 029 | Total loss: 5.688 | Reg loss: 0.026 | Tree loss: 5.688 | Accuracy: 0.208984 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 029 | Total loss: 5.675 | Reg loss: 0.026 | Tree loss: 5.675 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 029 | Total loss: 5.653 | Reg loss: 0.026 | Tree loss: 5.653 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 029 | Total loss: 5.613 | Reg loss: 0.026 | Tree loss: 5.613 | Accuracy: 0.234375 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 029 | Total loss: 5.616 | Reg loss: 0.027 | Tree loss: 5.616 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 029 | Total loss: 5.573 | Reg loss: 0.027 | Tree loss: 5.573 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 029 | Total loss: 5.534 | Reg loss: 0.027 | Tree loss: 5.534 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 029 | Total loss: 5.534 | Reg loss: 0.027 | Tree loss: 5.534 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 029 | Total loss: 5.502 | Reg loss: 0.027 | Tree loss: 5.502 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 029 | Total loss: 5.480 | Reg loss: 0.028 | Tree loss: 5.480 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 029 | Total loss: 5.434 | Reg loss: 0.028 | Tree loss: 5.434 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 029 | Total loss: 5.437 | Reg loss: 0.028 | Tree loss: 5.437 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 029 | Total loss: 5.391 | Reg loss: 0.028 | Tree loss: 5.391 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 029 | Total loss: 5.417 | Reg loss: 0.028 | Tree loss: 5.417 | Accuracy: 0.214844 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 029 | Total loss: 5.304 | Reg loss: 0.029 | Tree loss: 5.304 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 029 | Total loss: 5.352 | Reg loss: 0.029 | Tree loss: 5.352 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 029 | Total loss: 5.307 | Reg loss: 0.029 | Tree loss: 5.307 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 029 | Total loss: 5.247 | Reg loss: 0.029 | Tree loss: 5.247 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 029 | Total loss: 5.234 | Reg loss: 0.029 | Tree loss: 5.234 | Accuracy: 0.241379 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 029 | Total loss: 5.617 | Reg loss: 0.025 | Tree loss: 5.617 | Accuracy: 0.185547 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 029 | Total loss: 5.611 | Reg loss: 0.025 | Tree loss: 5.611 | Accuracy: 0.228516 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 029 | Total loss: 5.577 | Reg loss: 0.025 | Tree loss: 5.577 | Accuracy: 0.226562 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 029 | Total loss: 5.570 | Reg loss: 0.025 | Tree loss: 5.570 | Accuracy: 0.216797 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 029 | Total loss: 5.560 | Reg loss: 0.025 | Tree loss: 5.560 | Accuracy: 0.171875 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 029 | Total loss: 5.495 | Reg loss: 0.025 | Tree loss: 5.495 | Accuracy: 0.224609 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 029 | Total loss: 5.470 | Reg loss: 0.025 | Tree loss: 5.470 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 029 | Total loss: 5.493 | Reg loss: 0.025 | Tree loss: 5.493 | Accuracy: 0.187500 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 029 | Total loss: 5.408 | Reg loss: 0.025 | Tree loss: 5.408 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 029 | Total loss: 5.400 | Reg loss: 0.026 | Tree loss: 5.400 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 029 | Total loss: 5.391 | Reg loss: 0.026 | Tree loss: 5.391 | Accuracy: 0.187500 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 029 | Total loss: 5.378 | Reg loss: 0.026 | Tree loss: 5.378 | Accuracy: 0.230469 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 029 | Total loss: 5.322 | Reg loss: 0.026 | Tree loss: 5.322 | Accuracy: 0.181641 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 029 | Total loss: 5.264 | Reg loss: 0.026 | Tree loss: 5.264 | Accuracy: 0.208984 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 029 | Total loss: 5.273 | Reg loss: 0.027 | Tree loss: 5.273 | Accuracy: 0.181641 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 029 | Total loss: 5.274 | Reg loss: 0.027 | Tree loss: 5.274 | Accuracy: 0.195312 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 029 | Total loss: 5.208 | Reg loss: 0.027 | Tree loss: 5.208 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 029 | Total loss: 5.190 | Reg loss: 0.027 | Tree loss: 5.190 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 029 | Total loss: 5.135 | Reg loss: 0.027 | Tree loss: 5.135 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 029 | Total loss: 5.159 | Reg loss: 0.028 | Tree loss: 5.159 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 029 | Total loss: 5.148 | Reg loss: 0.028 | Tree loss: 5.148 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 029 | Total loss: 5.075 | Reg loss: 0.028 | Tree loss: 5.075 | Accuracy: 0.234375 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 029 | Total loss: 5.045 | Reg loss: 0.028 | Tree loss: 5.045 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 029 | Total loss: 5.082 | Reg loss: 0.029 | Tree loss: 5.082 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 029 | Total loss: 5.044 | Reg loss: 0.029 | Tree loss: 5.044 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 029 | Total loss: 5.009 | Reg loss: 0.029 | Tree loss: 5.009 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 029 | Total loss: 4.940 | Reg loss: 0.029 | Tree loss: 4.940 | Accuracy: 0.228516 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 029 | Total loss: 4.882 | Reg loss: 0.030 | Tree loss: 4.882 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 029 | Total loss: 4.984 | Reg loss: 0.030 | Tree loss: 4.984 | Accuracy: 0.120690 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 029 | Total loss: 5.287 | Reg loss: 0.025 | Tree loss: 5.287 | Accuracy: 0.210938 | 0.863 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 029 | Total loss: 5.312 | Reg loss: 0.025 | Tree loss: 5.312 | Accuracy: 0.197266 | 0.863 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 029 | Total loss: 5.301 | Reg loss: 0.025 | Tree loss: 5.301 | Accuracy: 0.197266 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 029 | Total loss: 5.242 | Reg loss: 0.025 | Tree loss: 5.242 | Accuracy: 0.187500 | 0.862 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 004 / 029 | Total loss: 5.202 | Reg loss: 0.025 | Tree loss: 5.202 | Accuracy: 0.220703 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 029 | Total loss: 5.164 | Reg loss: 0.026 | Tree loss: 5.164 | Accuracy: 0.236328 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 029 | Total loss: 5.208 | Reg loss: 0.026 | Tree loss: 5.208 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 029 | Total loss: 5.131 | Reg loss: 0.026 | Tree loss: 5.131 | Accuracy: 0.197266 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 029 | Total loss: 5.112 | Reg loss: 0.026 | Tree loss: 5.112 | Accuracy: 0.232422 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 029 | Total loss: 5.114 | Reg loss: 0.026 | Tree loss: 5.114 | Accuracy: 0.216797 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 029 | Total loss: 5.069 | Reg loss: 0.026 | Tree loss: 5.069 | Accuracy: 0.232422 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 029 | Total loss: 5.018 | Reg loss: 0.026 | Tree loss: 5.018 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 029 | Total loss: 4.991 | Reg loss: 0.027 | Tree loss: 4.991 | Accuracy: 0.220703 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 029 | Total loss: 5.020 | Reg loss: 0.027 | Tree loss: 5.020 | Accuracy: 0.216797 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 029 | Total loss: 4.986 | Reg loss: 0.027 | Tree loss: 4.986 | Accuracy: 0.185547 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 029 | Total loss: 4.969 | Reg loss: 0.027 | Tree loss: 4.969 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 029 | Total loss: 4.922 | Reg loss: 0.027 | Tree loss: 4.922 | Accuracy: 0.191406 | 0.862 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 029 | Total loss: 4.864 | Reg loss: 0.028 | Tree loss: 4.864 | Accuracy: 0.230469 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 029 | Total loss: 4.889 | Reg loss: 0.028 | Tree loss: 4.889 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 029 | Total loss: 4.822 | Reg loss: 0.028 | Tree loss: 4.822 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 029 | Total loss: 4.847 | Reg loss: 0.028 | Tree loss: 4.847 | Accuracy: 0.158203 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 029 | Total loss: 4.781 | Reg loss: 0.028 | Tree loss: 4.781 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 029 | Total loss: 4.739 | Reg loss: 0.029 | Tree loss: 4.739 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 029 | Total loss: 4.781 | Reg loss: 0.029 | Tree loss: 4.781 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 029 | Total loss: 4.705 | Reg loss: 0.029 | Tree loss: 4.705 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 029 | Total loss: 4.782 | Reg loss: 0.029 | Tree loss: 4.782 | Accuracy: 0.173828 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 029 | Total loss: 4.714 | Reg loss: 0.030 | Tree loss: 4.714 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 029 | Total loss: 4.749 | Reg loss: 0.030 | Tree loss: 4.749 | Accuracy: 0.173828 | 0.861 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 029 | Total loss: 4.753 | Reg loss: 0.030 | Tree loss: 4.753 | Accuracy: 0.103448 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 029 | Total loss: 5.032 | Reg loss: 0.026 | Tree loss: 5.032 | Accuracy: 0.203125 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 029 | Total loss: 4.949 | Reg loss: 0.026 | Tree loss: 4.949 | Accuracy: 0.236328 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 029 | Total loss: 4.969 | Reg loss: 0.026 | Tree loss: 4.969 | Accuracy: 0.210938 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 029 | Total loss: 5.005 | Reg loss: 0.026 | Tree loss: 5.005 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 029 | Total loss: 4.988 | Reg loss: 0.026 | Tree loss: 4.988 | Accuracy: 0.173828 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 029 | Total loss: 4.918 | Reg loss: 0.026 | Tree loss: 4.918 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 029 | Total loss: 4.891 | Reg loss: 0.026 | Tree loss: 4.891 | Accuracy: 0.199219 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 029 | Total loss: 4.837 | Reg loss: 0.027 | Tree loss: 4.837 | Accuracy: 0.195312 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 029 | Total loss: 4.820 | Reg loss: 0.027 | Tree loss: 4.820 | Accuracy: 0.222656 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 029 | Total loss: 4.804 | Reg loss: 0.027 | Tree loss: 4.804 | Accuracy: 0.218750 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 029 | Total loss: 4.850 | Reg loss: 0.027 | Tree loss: 4.850 | Accuracy: 0.212891 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 029 | Total loss: 4.734 | Reg loss: 0.027 | Tree loss: 4.734 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 029 | Total loss: 4.760 | Reg loss: 0.027 | Tree loss: 4.760 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 029 | Total loss: 4.704 | Reg loss: 0.027 | Tree loss: 4.704 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 029 | Total loss: 4.714 | Reg loss: 0.028 | Tree loss: 4.714 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 029 | Total loss: 4.699 | Reg loss: 0.028 | Tree loss: 4.699 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 029 | Total loss: 4.652 | Reg loss: 0.028 | Tree loss: 4.652 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 029 | Total loss: 4.620 | Reg loss: 0.028 | Tree loss: 4.620 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 029 | Total loss: 4.591 | Reg loss: 0.028 | Tree loss: 4.591 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 029 | Total loss: 4.619 | Reg loss: 0.029 | Tree loss: 4.619 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 029 | Total loss: 4.594 | Reg loss: 0.029 | Tree loss: 4.594 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 029 | Total loss: 4.542 | Reg loss: 0.029 | Tree loss: 4.542 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 029 | Total loss: 4.547 | Reg loss: 0.029 | Tree loss: 4.547 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 029 | Total loss: 4.491 | Reg loss: 0.029 | Tree loss: 4.491 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 029 | Total loss: 4.484 | Reg loss: 0.030 | Tree loss: 4.484 | Accuracy: 0.234375 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 029 | Total loss: 4.431 | Reg loss: 0.030 | Tree loss: 4.431 | Accuracy: 0.230469 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 029 | Total loss: 4.433 | Reg loss: 0.030 | Tree loss: 4.433 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 029 | Total loss: 4.416 | Reg loss: 0.030 | Tree loss: 4.416 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 029 | Total loss: 4.271 | Reg loss: 0.030 | Tree loss: 4.271 | Accuracy: 0.224138 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 029 | Total loss: 4.752 | Reg loss: 0.027 | Tree loss: 4.752 | Accuracy: 0.181641 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 029 | Total loss: 4.772 | Reg loss: 0.027 | Tree loss: 4.772 | Accuracy: 0.195312 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 029 | Total loss: 4.769 | Reg loss: 0.027 | Tree loss: 4.769 | Accuracy: 0.205078 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 029 | Total loss: 4.722 | Reg loss: 0.027 | Tree loss: 4.722 | Accuracy: 0.191406 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 029 | Total loss: 4.694 | Reg loss: 0.027 | Tree loss: 4.694 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 029 | Total loss: 4.639 | Reg loss: 0.027 | Tree loss: 4.639 | Accuracy: 0.210938 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 029 | Total loss: 4.678 | Reg loss: 0.027 | Tree loss: 4.678 | Accuracy: 0.205078 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 029 | Total loss: 4.618 | Reg loss: 0.027 | Tree loss: 4.618 | Accuracy: 0.205078 | 0.862 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Batch: 008 / 029 | Total loss: 4.577 | Reg loss: 0.028 | Tree loss: 4.577 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 029 | Total loss: 4.563 | Reg loss: 0.028 | Tree loss: 4.563 | Accuracy: 0.224609 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 029 | Total loss: 4.527 | Reg loss: 0.028 | Tree loss: 4.527 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 029 | Total loss: 4.532 | Reg loss: 0.028 | Tree loss: 4.532 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 029 | Total loss: 4.521 | Reg loss: 0.028 | Tree loss: 4.521 | Accuracy: 0.226562 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 029 | Total loss: 4.496 | Reg loss: 0.028 | Tree loss: 4.496 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 029 | Total loss: 4.429 | Reg loss: 0.028 | Tree loss: 4.429 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 029 | Total loss: 4.436 | Reg loss: 0.028 | Tree loss: 4.436 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 029 | Total loss: 4.482 | Reg loss: 0.029 | Tree loss: 4.482 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 029 | Total loss: 4.434 | Reg loss: 0.029 | Tree loss: 4.434 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 029 | Total loss: 4.324 | Reg loss: 0.029 | Tree loss: 4.324 | Accuracy: 0.214844 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 029 | Total loss: 4.332 | Reg loss: 0.029 | Tree loss: 4.332 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 029 | Total loss: 4.393 | Reg loss: 0.029 | Tree loss: 4.393 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 029 | Total loss: 4.292 | Reg loss: 0.029 | Tree loss: 4.292 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 029 | Total loss: 4.253 | Reg loss: 0.030 | Tree loss: 4.253 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 029 | Total loss: 4.179 | Reg loss: 0.030 | Tree loss: 4.179 | Accuracy: 0.226562 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 029 | Total loss: 4.193 | Reg loss: 0.030 | Tree loss: 4.193 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 029 | Total loss: 4.238 | Reg loss: 0.030 | Tree loss: 4.238 | Accuracy: 0.164062 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 029 | Total loss: 4.180 | Reg loss: 0.030 | Tree loss: 4.180 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 029 | Total loss: 4.190 | Reg loss: 0.030 | Tree loss: 4.190 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 029 | Total loss: 4.061 | Reg loss: 0.031 | Tree loss: 4.061 | Accuracy: 0.258621 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 029 | Total loss: 4.496 | Reg loss: 0.028 | Tree loss: 4.496 | Accuracy: 0.214844 | 0.862 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 029 | Total loss: 4.506 | Reg loss: 0.028 | Tree loss: 4.506 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 029 | Total loss: 4.465 | Reg loss: 0.028 | Tree loss: 4.465 | Accuracy: 0.197266 | 0.862 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 029 | Total loss: 4.437 | Reg loss: 0.028 | Tree loss: 4.437 | Accuracy: 0.201172 | 0.862 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 029 | Total loss: 4.419 | Reg loss: 0.028 | Tree loss: 4.419 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 029 | Total loss: 4.400 | Reg loss: 0.028 | Tree loss: 4.400 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 029 | Total loss: 4.424 | Reg loss: 0.028 | Tree loss: 4.424 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 029 | Total loss: 4.336 | Reg loss: 0.028 | Tree loss: 4.336 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 029 | Total loss: 4.328 | Reg loss: 0.028 | Tree loss: 4.328 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 029 | Total loss: 4.330 | Reg loss: 0.028 | Tree loss: 4.330 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 029 | Total loss: 4.301 | Reg loss: 0.028 | Tree loss: 4.301 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 029 | Total loss: 4.330 | Reg loss: 0.029 | Tree loss: 4.330 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 029 | Total loss: 4.275 | Reg loss: 0.029 | Tree loss: 4.275 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 029 | Total loss: 4.241 | Reg loss: 0.029 | Tree loss: 4.241 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 029 | Total loss: 4.268 | Reg loss: 0.029 | Tree loss: 4.268 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 029 | Total loss: 4.243 | Reg loss: 0.029 | Tree loss: 4.243 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 029 | Total loss: 4.205 | Reg loss: 0.029 | Tree loss: 4.205 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 029 | Total loss: 4.183 | Reg loss: 0.029 | Tree loss: 4.183 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 029 | Total loss: 4.136 | Reg loss: 0.029 | Tree loss: 4.136 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 029 | Total loss: 4.144 | Reg loss: 0.030 | Tree loss: 4.144 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 029 | Total loss: 4.176 | Reg loss: 0.030 | Tree loss: 4.176 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 029 | Total loss: 4.076 | Reg loss: 0.030 | Tree loss: 4.076 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 029 | Total loss: 4.073 | Reg loss: 0.030 | Tree loss: 4.073 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 029 | Total loss: 4.056 | Reg loss: 0.030 | Tree loss: 4.056 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 029 | Total loss: 4.045 | Reg loss: 0.030 | Tree loss: 4.045 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 029 | Total loss: 4.020 | Reg loss: 0.031 | Tree loss: 4.020 | Accuracy: 0.236328 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 029 | Total loss: 4.051 | Reg loss: 0.031 | Tree loss: 4.051 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 029 | Total loss: 4.021 | Reg loss: 0.031 | Tree loss: 4.021 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 029 | Total loss: 3.951 | Reg loss: 0.031 | Tree loss: 3.951 | Accuracy: 0.293103 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 029 | Total loss: 4.332 | Reg loss: 0.029 | Tree loss: 4.332 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 029 | Total loss: 4.294 | Reg loss: 0.029 | Tree loss: 4.294 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 029 | Total loss: 4.298 | Reg loss: 0.029 | Tree loss: 4.298 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 029 | Total loss: 4.206 | Reg loss: 0.029 | Tree loss: 4.206 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 029 | Total loss: 4.227 | Reg loss: 0.029 | Tree loss: 4.227 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 029 | Total loss: 4.240 | Reg loss: 0.029 | Tree loss: 4.240 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 029 | Total loss: 4.192 | Reg loss: 0.029 | Tree loss: 4.192 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 029 | Total loss: 4.121 | Reg loss: 0.029 | Tree loss: 4.121 | Accuracy: 0.240234 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 029 | Total loss: 4.144 | Reg loss: 0.029 | Tree loss: 4.144 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 029 | Total loss: 4.080 | Reg loss: 0.029 | Tree loss: 4.080 | Accuracy: 0.224609 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 029 | Total loss: 4.113 | Reg loss: 0.029 | Tree loss: 4.113 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 029 | Total loss: 4.051 | Reg loss: 0.029 | Tree loss: 4.051 | Accuracy: 0.212891 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 012 / 029 | Total loss: 4.103 | Reg loss: 0.029 | Tree loss: 4.103 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 029 | Total loss: 4.054 | Reg loss: 0.029 | Tree loss: 4.054 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 029 | Total loss: 4.078 | Reg loss: 0.029 | Tree loss: 4.078 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 029 | Total loss: 4.047 | Reg loss: 0.030 | Tree loss: 4.047 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 029 | Total loss: 4.000 | Reg loss: 0.030 | Tree loss: 4.000 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 029 | Total loss: 3.991 | Reg loss: 0.030 | Tree loss: 3.991 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 029 | Total loss: 3.982 | Reg loss: 0.030 | Tree loss: 3.982 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 029 | Total loss: 3.867 | Reg loss: 0.030 | Tree loss: 3.867 | Accuracy: 0.257812 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 029 | Total loss: 3.938 | Reg loss: 0.030 | Tree loss: 3.938 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 029 | Total loss: 3.882 | Reg loss: 0.030 | Tree loss: 3.882 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 029 | Total loss: 3.900 | Reg loss: 0.030 | Tree loss: 3.900 | Accuracy: 0.230469 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 029 | Total loss: 3.854 | Reg loss: 0.031 | Tree loss: 3.854 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 029 | Total loss: 3.856 | Reg loss: 0.031 | Tree loss: 3.856 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 029 | Total loss: 3.900 | Reg loss: 0.031 | Tree loss: 3.900 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 029 | Total loss: 3.823 | Reg loss: 0.031 | Tree loss: 3.823 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 029 | Total loss: 3.803 | Reg loss: 0.031 | Tree loss: 3.803 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 029 | Total loss: 3.702 | Reg loss: 0.031 | Tree loss: 3.702 | Accuracy: 0.103448 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 029 | Total loss: 4.088 | Reg loss: 0.029 | Tree loss: 4.088 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 029 | Total loss: 4.034 | Reg loss: 0.029 | Tree loss: 4.034 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 029 | Total loss: 4.068 | Reg loss: 0.029 | Tree loss: 4.068 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 029 | Total loss: 4.090 | Reg loss: 0.029 | Tree loss: 4.090 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 029 | Total loss: 3.984 | Reg loss: 0.029 | Tree loss: 3.984 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 029 | Total loss: 4.045 | Reg loss: 0.029 | Tree loss: 4.045 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 029 | Total loss: 4.001 | Reg loss: 0.029 | Tree loss: 4.001 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 029 | Total loss: 3.955 | Reg loss: 0.029 | Tree loss: 3.955 | Accuracy: 0.224609 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 029 | Total loss: 3.947 | Reg loss: 0.029 | Tree loss: 3.947 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 029 | Total loss: 3.955 | Reg loss: 0.030 | Tree loss: 3.955 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 029 | Total loss: 3.936 | Reg loss: 0.030 | Tree loss: 3.936 | Accuracy: 0.173828 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 029 | Total loss: 3.931 | Reg loss: 0.030 | Tree loss: 3.931 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 029 | Total loss: 3.860 | Reg loss: 0.030 | Tree loss: 3.860 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 029 | Total loss: 3.887 | Reg loss: 0.030 | Tree loss: 3.887 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 029 | Total loss: 3.893 | Reg loss: 0.030 | Tree loss: 3.893 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 029 | Total loss: 3.828 | Reg loss: 0.030 | Tree loss: 3.828 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 029 | Total loss: 3.831 | Reg loss: 0.030 | Tree loss: 3.831 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 029 | Total loss: 3.799 | Reg loss: 0.030 | Tree loss: 3.799 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 029 | Total loss: 3.766 | Reg loss: 0.030 | Tree loss: 3.766 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 029 | Total loss: 3.753 | Reg loss: 0.030 | Tree loss: 3.753 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 029 | Total loss: 3.745 | Reg loss: 0.031 | Tree loss: 3.745 | Accuracy: 0.214844 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 029 | Total loss: 3.696 | Reg loss: 0.031 | Tree loss: 3.696 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 029 | Total loss: 3.695 | Reg loss: 0.031 | Tree loss: 3.695 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 029 | Total loss: 3.727 | Reg loss: 0.031 | Tree loss: 3.727 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 029 | Total loss: 3.700 | Reg loss: 0.031 | Tree loss: 3.700 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 029 | Total loss: 3.715 | Reg loss: 0.031 | Tree loss: 3.715 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 029 | Total loss: 3.636 | Reg loss: 0.031 | Tree loss: 3.636 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 029 | Total loss: 3.657 | Reg loss: 0.031 | Tree loss: 3.657 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 029 | Total loss: 3.883 | Reg loss: 0.031 | Tree loss: 3.883 | Accuracy: 0.224138 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 029 | Total loss: 3.904 | Reg loss: 0.030 | Tree loss: 3.904 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 029 | Total loss: 3.895 | Reg loss: 0.030 | Tree loss: 3.895 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 029 | Total loss: 3.985 | Reg loss: 0.030 | Tree loss: 3.985 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 029 | Total loss: 3.867 | Reg loss: 0.030 | Tree loss: 3.867 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 029 | Total loss: 3.895 | Reg loss: 0.030 | Tree loss: 3.895 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 029 | Total loss: 3.832 | Reg loss: 0.030 | Tree loss: 3.832 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 029 | Total loss: 3.806 | Reg loss: 0.030 | Tree loss: 3.806 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 029 | Total loss: 3.807 | Reg loss: 0.030 | Tree loss: 3.807 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 029 | Total loss: 3.794 | Reg loss: 0.030 | Tree loss: 3.794 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 029 | Total loss: 3.796 | Reg loss: 0.030 | Tree loss: 3.796 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 029 | Total loss: 3.748 | Reg loss: 0.030 | Tree loss: 3.748 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 029 | Total loss: 3.739 | Reg loss: 0.030 | Tree loss: 3.739 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 029 | Total loss: 3.721 | Reg loss: 0.030 | Tree loss: 3.721 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 029 | Total loss: 3.641 | Reg loss: 0.030 | Tree loss: 3.641 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 029 | Total loss: 3.649 | Reg loss: 0.030 | Tree loss: 3.649 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 029 | Total loss: 3.654 | Reg loss: 0.030 | Tree loss: 3.654 | Accuracy: 0.197266 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 016 / 029 | Total loss: 3.678 | Reg loss: 0.030 | Tree loss: 3.678 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 029 | Total loss: 3.661 | Reg loss: 0.031 | Tree loss: 3.661 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 029 | Total loss: 3.649 | Reg loss: 0.031 | Tree loss: 3.649 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 029 | Total loss: 3.628 | Reg loss: 0.031 | Tree loss: 3.628 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 029 | Total loss: 3.630 | Reg loss: 0.031 | Tree loss: 3.630 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 029 | Total loss: 3.572 | Reg loss: 0.031 | Tree loss: 3.572 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 029 | Total loss: 3.498 | Reg loss: 0.031 | Tree loss: 3.498 | Accuracy: 0.226562 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 029 | Total loss: 3.596 | Reg loss: 0.031 | Tree loss: 3.596 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 029 | Total loss: 3.539 | Reg loss: 0.031 | Tree loss: 3.539 | Accuracy: 0.242188 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 029 | Total loss: 3.512 | Reg loss: 0.031 | Tree loss: 3.512 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 029 | Total loss: 3.502 | Reg loss: 0.031 | Tree loss: 3.502 | Accuracy: 0.203125 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 029 | Total loss: 3.451 | Reg loss: 0.032 | Tree loss: 3.451 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 029 | Total loss: 3.493 | Reg loss: 0.032 | Tree loss: 3.493 | Accuracy: 0.258621 | 0.861 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 029 | Total loss: 3.689 | Reg loss: 0.030 | Tree loss: 3.689 | Accuracy: 0.226562 | 0.862 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 029 | Total loss: 3.757 | Reg loss: 0.030 | Tree loss: 3.757 | Accuracy: 0.185547 | 0.862 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 029 | Total loss: 3.733 | Reg loss: 0.030 | Tree loss: 3.733 | Accuracy: 0.187500 | 0.862 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 029 | Total loss: 3.704 | Reg loss: 0.030 | Tree loss: 3.704 | Accuracy: 0.207031 | 0.862 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 029 | Total loss: 3.753 | Reg loss: 0.030 | Tree loss: 3.753 | Accuracy: 0.177734 | 0.862 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 029 | Total loss: 3.724 | Reg loss: 0.030 | Tree loss: 3.724 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 029 | Total loss: 3.604 | Reg loss: 0.030 | Tree loss: 3.604 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 029 | Total loss: 3.664 | Reg loss: 0.030 | Tree loss: 3.664 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 029 | Total loss: 3.641 | Reg loss: 0.030 | Tree loss: 3.641 | Accuracy: 0.166016 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 029 | Total loss: 3.573 | Reg loss: 0.030 | Tree loss: 3.573 | Accuracy: 0.228516 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 029 | Total loss: 3.590 | Reg loss: 0.030 | Tree loss: 3.590 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 029 | Total loss: 3.612 | Reg loss: 0.030 | Tree loss: 3.612 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 029 | Total loss: 3.530 | Reg loss: 0.031 | Tree loss: 3.530 | Accuracy: 0.232422 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 029 | Total loss: 3.524 | Reg loss: 0.031 | Tree loss: 3.524 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 029 | Total loss: 3.498 | Reg loss: 0.031 | Tree loss: 3.498 | Accuracy: 0.244141 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 029 | Total loss: 3.505 | Reg loss: 0.031 | Tree loss: 3.505 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 029 | Total loss: 3.515 | Reg loss: 0.031 | Tree loss: 3.515 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 029 | Total loss: 3.485 | Reg loss: 0.031 | Tree loss: 3.485 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 029 | Total loss: 3.494 | Reg loss: 0.031 | Tree loss: 3.494 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 029 | Total loss: 3.535 | Reg loss: 0.031 | Tree loss: 3.535 | Accuracy: 0.181641 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 029 | Total loss: 3.475 | Reg loss: 0.031 | Tree loss: 3.475 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 029 | Total loss: 3.395 | Reg loss: 0.031 | Tree loss: 3.395 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 029 | Total loss: 3.453 | Reg loss: 0.031 | Tree loss: 3.453 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 029 | Total loss: 3.422 | Reg loss: 0.031 | Tree loss: 3.422 | Accuracy: 0.226562 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 029 | Total loss: 3.418 | Reg loss: 0.031 | Tree loss: 3.418 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 029 | Total loss: 3.449 | Reg loss: 0.032 | Tree loss: 3.449 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 029 | Total loss: 3.410 | Reg loss: 0.032 | Tree loss: 3.410 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 029 | Total loss: 3.344 | Reg loss: 0.032 | Tree loss: 3.344 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 029 | Total loss: 3.333 | Reg loss: 0.032 | Tree loss: 3.333 | Accuracy: 0.310345 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 029 | Total loss: 3.574 | Reg loss: 0.030 | Tree loss: 3.574 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 029 | Total loss: 3.560 | Reg loss: 0.030 | Tree loss: 3.560 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 029 | Total loss: 3.552 | Reg loss: 0.031 | Tree loss: 3.552 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 029 | Total loss: 3.573 | Reg loss: 0.031 | Tree loss: 3.573 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 029 | Total loss: 3.582 | Reg loss: 0.031 | Tree loss: 3.582 | Accuracy: 0.224609 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 029 | Total loss: 3.560 | Reg loss: 0.031 | Tree loss: 3.560 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 029 | Total loss: 3.434 | Reg loss: 0.031 | Tree loss: 3.434 | Accuracy: 0.240234 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 029 | Total loss: 3.474 | Reg loss: 0.031 | Tree loss: 3.474 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 029 | Total loss: 3.472 | Reg loss: 0.031 | Tree loss: 3.472 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 029 | Total loss: 3.448 | Reg loss: 0.031 | Tree loss: 3.448 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 029 | Total loss: 3.498 | Reg loss: 0.031 | Tree loss: 3.498 | Accuracy: 0.183594 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 029 | Total loss: 3.434 | Reg loss: 0.031 | Tree loss: 3.434 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 029 | Total loss: 3.425 | Reg loss: 0.031 | Tree loss: 3.425 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 029 | Total loss: 3.460 | Reg loss: 0.031 | Tree loss: 3.460 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 029 | Total loss: 3.396 | Reg loss: 0.031 | Tree loss: 3.396 | Accuracy: 0.173828 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 029 | Total loss: 3.384 | Reg loss: 0.031 | Tree loss: 3.384 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 029 | Total loss: 3.370 | Reg loss: 0.031 | Tree loss: 3.370 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 029 | Total loss: 3.356 | Reg loss: 0.031 | Tree loss: 3.356 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 029 | Total loss: 3.410 | Reg loss: 0.031 | Tree loss: 3.410 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 029 | Total loss: 3.386 | Reg loss: 0.031 | Tree loss: 3.386 | Accuracy: 0.181641 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 020 / 029 | Total loss: 3.354 | Reg loss: 0.031 | Tree loss: 3.354 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 029 | Total loss: 3.278 | Reg loss: 0.031 | Tree loss: 3.278 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 029 | Total loss: 3.347 | Reg loss: 0.031 | Tree loss: 3.347 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 029 | Total loss: 3.321 | Reg loss: 0.032 | Tree loss: 3.321 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 029 | Total loss: 3.347 | Reg loss: 0.032 | Tree loss: 3.347 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 029 | Total loss: 3.255 | Reg loss: 0.032 | Tree loss: 3.255 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 029 | Total loss: 3.281 | Reg loss: 0.032 | Tree loss: 3.281 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 029 | Total loss: 3.208 | Reg loss: 0.032 | Tree loss: 3.208 | Accuracy: 0.240234 | 0.86 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 029 | Total loss: 3.258 | Reg loss: 0.032 | Tree loss: 3.258 | Accuracy: 0.206897 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 029 | Total loss: 3.359 | Reg loss: 0.031 | Tree loss: 3.359 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 029 | Total loss: 3.414 | Reg loss: 0.031 | Tree loss: 3.414 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 029 | Total loss: 3.437 | Reg loss: 0.031 | Tree loss: 3.437 | Accuracy: 0.197266 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 029 | Total loss: 3.428 | Reg loss: 0.031 | Tree loss: 3.428 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 029 | Total loss: 3.391 | Reg loss: 0.031 | Tree loss: 3.391 | Accuracy: 0.218750 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 029 | Total loss: 3.334 | Reg loss: 0.031 | Tree loss: 3.334 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 029 | Total loss: 3.426 | Reg loss: 0.031 | Tree loss: 3.426 | Accuracy: 0.169922 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 029 | Total loss: 3.319 | Reg loss: 0.031 | Tree loss: 3.319 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 029 | Total loss: 3.386 | Reg loss: 0.031 | Tree loss: 3.386 | Accuracy: 0.207031 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 029 | Total loss: 3.313 | Reg loss: 0.031 | Tree loss: 3.313 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 029 | Total loss: 3.312 | Reg loss: 0.031 | Tree loss: 3.312 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 029 | Total loss: 3.343 | Reg loss: 0.031 | Tree loss: 3.343 | Accuracy: 0.171875 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 029 | Total loss: 3.323 | Reg loss: 0.031 | Tree loss: 3.323 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 029 | Total loss: 3.335 | Reg loss: 0.031 | Tree loss: 3.335 | Accuracy: 0.189453 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 029 | Total loss: 3.320 | Reg loss: 0.031 | Tree loss: 3.320 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 029 | Total loss: 3.269 | Reg loss: 0.031 | Tree loss: 3.269 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 029 | Total loss: 3.235 | Reg loss: 0.031 | Tree loss: 3.235 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 029 | Total loss: 3.256 | Reg loss: 0.031 | Tree loss: 3.256 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 029 | Total loss: 3.297 | Reg loss: 0.031 | Tree loss: 3.297 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 029 | Total loss: 3.245 | Reg loss: 0.032 | Tree loss: 3.245 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 029 | Total loss: 3.213 | Reg loss: 0.032 | Tree loss: 3.213 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 029 | Total loss: 3.214 | Reg loss: 0.032 | Tree loss: 3.214 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 029 | Total loss: 3.180 | Reg loss: 0.032 | Tree loss: 3.180 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 029 | Total loss: 3.228 | Reg loss: 0.032 | Tree loss: 3.228 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 029 | Total loss: 3.207 | Reg loss: 0.032 | Tree loss: 3.207 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 029 | Total loss: 3.144 | Reg loss: 0.032 | Tree loss: 3.144 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 029 | Total loss: 3.207 | Reg loss: 0.032 | Tree loss: 3.207 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 029 | Total loss: 3.191 | Reg loss: 0.032 | Tree loss: 3.191 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 029 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.224138 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 029 | Total loss: 3.280 | Reg loss: 0.031 | Tree loss: 3.280 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 029 | Total loss: 3.312 | Reg loss: 0.031 | Tree loss: 3.312 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 029 | Total loss: 3.284 | Reg loss: 0.031 | Tree loss: 3.284 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 029 | Total loss: 3.300 | Reg loss: 0.031 | Tree loss: 3.300 | Accuracy: 0.199219 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 029 | Total loss: 3.302 | Reg loss: 0.031 | Tree loss: 3.302 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 029 | Total loss: 3.315 | Reg loss: 0.031 | Tree loss: 3.315 | Accuracy: 0.160156 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 029 | Total loss: 3.270 | Reg loss: 0.031 | Tree loss: 3.270 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 029 | Total loss: 3.254 | Reg loss: 0.031 | Tree loss: 3.254 | Accuracy: 0.208984 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 029 | Total loss: 3.248 | Reg loss: 0.031 | Tree loss: 3.248 | Accuracy: 0.210938 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 029 | Total loss: 3.253 | Reg loss: 0.031 | Tree loss: 3.253 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 029 | Total loss: 3.320 | Reg loss: 0.031 | Tree loss: 3.320 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 029 | Total loss: 3.147 | Reg loss: 0.031 | Tree loss: 3.147 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 029 | Total loss: 3.193 | Reg loss: 0.031 | Tree loss: 3.193 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 029 | Total loss: 3.141 | Reg loss: 0.031 | Tree loss: 3.141 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 029 | Total loss: 3.157 | Reg loss: 0.031 | Tree loss: 3.157 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 029 | Total loss: 3.237 | Reg loss: 0.032 | Tree loss: 3.237 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 029 | Total loss: 3.174 | Reg loss: 0.032 | Tree loss: 3.174 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 029 | Total loss: 3.148 | Reg loss: 0.032 | Tree loss: 3.148 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 029 | Total loss: 3.108 | Reg loss: 0.032 | Tree loss: 3.108 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 029 | Total loss: 3.161 | Reg loss: 0.032 | Tree loss: 3.161 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 029 | Total loss: 3.193 | Reg loss: 0.032 | Tree loss: 3.193 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 029 | Total loss: 3.063 | Reg loss: 0.032 | Tree loss: 3.063 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 029 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 029 | Total loss: 3.064 | Reg loss: 0.032 | Tree loss: 3.064 | Accuracy: 0.201172 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Batch: 024 / 029 | Total loss: 3.058 | Reg loss: 0.032 | Tree loss: 3.058 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 029 | Total loss: 3.042 | Reg loss: 0.032 | Tree loss: 3.042 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 029 | Total loss: 3.141 | Reg loss: 0.032 | Tree loss: 3.141 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 029 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 029 | Total loss: 3.081 | Reg loss: 0.032 | Tree loss: 3.081 | Accuracy: 0.155172 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 029 | Total loss: 3.196 | Reg loss: 0.031 | Tree loss: 3.196 | Accuracy: 0.238281 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 029 | Total loss: 3.146 | Reg loss: 0.031 | Tree loss: 3.146 | Accuracy: 0.224609 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 029 | Total loss: 3.184 | Reg loss: 0.031 | Tree loss: 3.184 | Accuracy: 0.220703 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 029 | Total loss: 3.152 | Reg loss: 0.031 | Tree loss: 3.152 | Accuracy: 0.242188 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 029 | Total loss: 3.222 | Reg loss: 0.031 | Tree loss: 3.222 | Accuracy: 0.179688 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 029 | Total loss: 3.147 | Reg loss: 0.031 | Tree loss: 3.147 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 029 | Total loss: 3.178 | Reg loss: 0.031 | Tree loss: 3.178 | Accuracy: 0.162109 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 029 | Total loss: 3.110 | Reg loss: 0.031 | Tree loss: 3.110 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 029 | Total loss: 3.133 | Reg loss: 0.031 | Tree loss: 3.133 | Accuracy: 0.185547 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 029 | Total loss: 3.092 | Reg loss: 0.032 | Tree loss: 3.092 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 029 | Total loss: 3.145 | Reg loss: 0.032 | Tree loss: 3.145 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 029 | Total loss: 3.108 | Reg loss: 0.032 | Tree loss: 3.108 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 029 | Total loss: 3.168 | Reg loss: 0.032 | Tree loss: 3.168 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 029 | Total loss: 3.066 | Reg loss: 0.032 | Tree loss: 3.066 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 029 | Total loss: 3.082 | Reg loss: 0.032 | Tree loss: 3.082 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 029 | Total loss: 3.054 | Reg loss: 0.032 | Tree loss: 3.054 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 029 | Total loss: 3.020 | Reg loss: 0.032 | Tree loss: 3.020 | Accuracy: 0.234375 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 029 | Total loss: 3.026 | Reg loss: 0.032 | Tree loss: 3.026 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 029 | Total loss: 3.051 | Reg loss: 0.032 | Tree loss: 3.051 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 029 | Total loss: 3.048 | Reg loss: 0.032 | Tree loss: 3.048 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 029 | Total loss: 3.014 | Reg loss: 0.032 | Tree loss: 3.014 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 029 | Total loss: 3.034 | Reg loss: 0.032 | Tree loss: 3.034 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 029 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 029 | Total loss: 3.021 | Reg loss: 0.032 | Tree loss: 3.021 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 029 | Total loss: 2.994 | Reg loss: 0.032 | Tree loss: 2.994 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 029 | Total loss: 3.048 | Reg loss: 0.032 | Tree loss: 3.048 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 029 | Total loss: 3.031 | Reg loss: 0.032 | Tree loss: 3.031 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 029 | Total loss: 2.987 | Reg loss: 0.032 | Tree loss: 2.987 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 029 | Total loss: 2.958 | Reg loss: 0.032 | Tree loss: 2.958 | Accuracy: 0.155172 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 029 | Total loss: 3.102 | Reg loss: 0.032 | Tree loss: 3.102 | Accuracy: 0.222656 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 029 | Total loss: 3.131 | Reg loss: 0.032 | Tree loss: 3.131 | Accuracy: 0.205078 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 029 | Total loss: 3.112 | Reg loss: 0.032 | Tree loss: 3.112 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 029 | Total loss: 3.083 | Reg loss: 0.032 | Tree loss: 3.083 | Accuracy: 0.193359 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 029 | Total loss: 3.075 | Reg loss: 0.032 | Tree loss: 3.075 | Accuracy: 0.232422 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 029 | Total loss: 3.043 | Reg loss: 0.032 | Tree loss: 3.043 | Accuracy: 0.216797 | 0.861 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 029 | Total loss: 3.047 | Reg loss: 0.032 | Tree loss: 3.047 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 029 | Total loss: 3.062 | Reg loss: 0.032 | Tree loss: 3.062 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 029 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 029 | Total loss: 3.071 | Reg loss: 0.032 | Tree loss: 3.071 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 029 | Total loss: 3.057 | Reg loss: 0.032 | Tree loss: 3.057 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 029 | Total loss: 2.943 | Reg loss: 0.032 | Tree loss: 2.943 | Accuracy: 0.248047 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 029 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 029 | Total loss: 3.029 | Reg loss: 0.032 | Tree loss: 3.029 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 029 | Total loss: 2.997 | Reg loss: 0.032 | Tree loss: 2.997 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 029 | Total loss: 3.008 | Reg loss: 0.032 | Tree loss: 3.008 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 029 | Total loss: 3.016 | Reg loss: 0.032 | Tree loss: 3.016 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 029 | Total loss: 2.910 | Reg loss: 0.032 | Tree loss: 2.910 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 029 | Total loss: 2.959 | Reg loss: 0.032 | Tree loss: 2.959 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 029 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.150391 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 029 | Total loss: 2.970 | Reg loss: 0.032 | Tree loss: 2.970 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 029 | Total loss: 2.985 | Reg loss: 0.032 | Tree loss: 2.985 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 029 | Total loss: 2.890 | Reg loss: 0.032 | Tree loss: 2.890 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 029 | Total loss: 2.928 | Reg loss: 0.032 | Tree loss: 2.928 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 029 | Total loss: 2.922 | Reg loss: 0.032 | Tree loss: 2.922 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 029 | Total loss: 2.904 | Reg loss: 0.032 | Tree loss: 2.904 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 029 | Total loss: 2.934 | Reg loss: 0.032 | Tree loss: 2.934 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 029 | Total loss: 2.861 | Reg loss: 0.032 | Tree loss: 2.861 | Accuracy: 0.205078 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 028 / 029 | Total loss: 2.838 | Reg loss: 0.032 | Tree loss: 2.838 | Accuracy: 0.224138 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 029 | Total loss: 3.072 | Reg loss: 0.032 | Tree loss: 3.072 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 029 | Total loss: 2.984 | Reg loss: 0.032 | Tree loss: 2.984 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 029 | Total loss: 2.979 | Reg loss: 0.032 | Tree loss: 2.979 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 029 | Total loss: 3.013 | Reg loss: 0.032 | Tree loss: 3.013 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 029 | Total loss: 3.004 | Reg loss: 0.032 | Tree loss: 3.004 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 029 | Total loss: 3.025 | Reg loss: 0.032 | Tree loss: 3.025 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 029 | Total loss: 3.022 | Reg loss: 0.032 | Tree loss: 3.022 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 029 | Total loss: 3.001 | Reg loss: 0.032 | Tree loss: 3.001 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 029 | Total loss: 2.894 | Reg loss: 0.032 | Tree loss: 2.894 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 029 | Total loss: 2.973 | Reg loss: 0.032 | Tree loss: 2.973 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 029 | Total loss: 2.969 | Reg loss: 0.032 | Tree loss: 2.969 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 029 | Total loss: 2.975 | Reg loss: 0.032 | Tree loss: 2.975 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 029 | Total loss: 2.901 | Reg loss: 0.032 | Tree loss: 2.901 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 029 | Total loss: 2.881 | Reg loss: 0.032 | Tree loss: 2.881 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 029 | Total loss: 2.856 | Reg loss: 0.032 | Tree loss: 2.856 | Accuracy: 0.240234 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 029 | Total loss: 2.878 | Reg loss: 0.032 | Tree loss: 2.878 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 029 | Total loss: 2.848 | Reg loss: 0.032 | Tree loss: 2.848 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 029 | Total loss: 2.935 | Reg loss: 0.032 | Tree loss: 2.935 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 029 | Total loss: 2.881 | Reg loss: 0.032 | Tree loss: 2.881 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 029 | Total loss: 2.882 | Reg loss: 0.032 | Tree loss: 2.882 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 029 | Total loss: 2.898 | Reg loss: 0.032 | Tree loss: 2.898 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 029 | Total loss: 2.825 | Reg loss: 0.032 | Tree loss: 2.825 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 029 | Total loss: 2.845 | Reg loss: 0.032 | Tree loss: 2.845 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 029 | Total loss: 2.843 | Reg loss: 0.032 | Tree loss: 2.843 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 029 | Total loss: 2.869 | Reg loss: 0.032 | Tree loss: 2.869 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 029 | Total loss: 2.783 | Reg loss: 0.032 | Tree loss: 2.783 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 029 | Total loss: 2.846 | Reg loss: 0.032 | Tree loss: 2.846 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 029 | Total loss: 2.842 | Reg loss: 0.032 | Tree loss: 2.842 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 029 | Total loss: 2.877 | Reg loss: 0.032 | Tree loss: 2.877 | Accuracy: 0.120690 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 029 | Total loss: 2.930 | Reg loss: 0.032 | Tree loss: 2.930 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 029 | Total loss: 2.936 | Reg loss: 0.032 | Tree loss: 2.936 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 029 | Total loss: 2.948 | Reg loss: 0.032 | Tree loss: 2.948 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 029 | Total loss: 2.923 | Reg loss: 0.032 | Tree loss: 2.923 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 029 | Total loss: 2.938 | Reg loss: 0.032 | Tree loss: 2.938 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 029 | Total loss: 2.942 | Reg loss: 0.032 | Tree loss: 2.942 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 029 | Total loss: 2.908 | Reg loss: 0.032 | Tree loss: 2.908 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 029 | Total loss: 2.881 | Reg loss: 0.032 | Tree loss: 2.881 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 029 | Total loss: 2.871 | Reg loss: 0.032 | Tree loss: 2.871 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 029 | Total loss: 2.853 | Reg loss: 0.032 | Tree loss: 2.853 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 029 | Total loss: 2.813 | Reg loss: 0.032 | Tree loss: 2.813 | Accuracy: 0.244141 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 029 | Total loss: 2.845 | Reg loss: 0.032 | Tree loss: 2.845 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 029 | Total loss: 2.842 | Reg loss: 0.032 | Tree loss: 2.842 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 029 | Total loss: 2.849 | Reg loss: 0.032 | Tree loss: 2.849 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 029 | Total loss: 2.831 | Reg loss: 0.032 | Tree loss: 2.831 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 029 | Total loss: 2.800 | Reg loss: 0.032 | Tree loss: 2.800 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 029 | Total loss: 2.766 | Reg loss: 0.032 | Tree loss: 2.766 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 029 | Total loss: 2.805 | Reg loss: 0.032 | Tree loss: 2.805 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 029 | Total loss: 2.826 | Reg loss: 0.032 | Tree loss: 2.826 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 029 | Total loss: 2.741 | Reg loss: 0.032 | Tree loss: 2.741 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 029 | Total loss: 2.794 | Reg loss: 0.032 | Tree loss: 2.794 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 029 | Total loss: 2.852 | Reg loss: 0.032 | Tree loss: 2.852 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 029 | Total loss: 2.831 | Reg loss: 0.032 | Tree loss: 2.831 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 029 | Total loss: 2.812 | Reg loss: 0.032 | Tree loss: 2.812 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 029 | Total loss: 2.782 | Reg loss: 0.032 | Tree loss: 2.782 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 029 | Total loss: 2.830 | Reg loss: 0.032 | Tree loss: 2.830 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 029 | Total loss: 2.765 | Reg loss: 0.032 | Tree loss: 2.765 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 029 | Total loss: 2.829 | Reg loss: 0.032 | Tree loss: 2.829 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 029 | Total loss: 2.707 | Reg loss: 0.032 | Tree loss: 2.707 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 029 | Total loss: 2.852 | Reg loss: 0.032 | Tree loss: 2.852 | Accuracy: 0.216797 | 0.861 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 001 / 029 | Total loss: 2.893 | Reg loss: 0.032 | Tree loss: 2.893 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 029 | Total loss: 2.856 | Reg loss: 0.032 | Tree loss: 2.856 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 029 | Total loss: 2.822 | Reg loss: 0.032 | Tree loss: 2.822 | Accuracy: 0.238281 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 029 | Total loss: 2.850 | Reg loss: 0.032 | Tree loss: 2.850 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 029 | Total loss: 2.824 | Reg loss: 0.032 | Tree loss: 2.824 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 029 | Total loss: 2.803 | Reg loss: 0.032 | Tree loss: 2.803 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 029 | Total loss: 2.887 | Reg loss: 0.032 | Tree loss: 2.887 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 029 | Total loss: 2.827 | Reg loss: 0.032 | Tree loss: 2.827 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 029 | Total loss: 2.813 | Reg loss: 0.032 | Tree loss: 2.813 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 029 | Total loss: 2.840 | Reg loss: 0.032 | Tree loss: 2.840 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 029 | Total loss: 2.801 | Reg loss: 0.032 | Tree loss: 2.801 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 029 | Total loss: 2.815 | Reg loss: 0.032 | Tree loss: 2.815 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 029 | Total loss: 2.783 | Reg loss: 0.032 | Tree loss: 2.783 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 029 | Total loss: 2.761 | Reg loss: 0.032 | Tree loss: 2.761 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 029 | Total loss: 2.760 | Reg loss: 0.032 | Tree loss: 2.760 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 029 | Total loss: 2.831 | Reg loss: 0.032 | Tree loss: 2.831 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 029 | Total loss: 2.744 | Reg loss: 0.032 | Tree loss: 2.744 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 029 | Total loss: 2.785 | Reg loss: 0.032 | Tree loss: 2.785 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 029 | Total loss: 2.725 | Reg loss: 0.032 | Tree loss: 2.725 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 029 | Total loss: 2.725 | Reg loss: 0.032 | Tree loss: 2.725 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 029 | Total loss: 2.800 | Reg loss: 0.032 | Tree loss: 2.800 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 029 | Total loss: 2.716 | Reg loss: 0.032 | Tree loss: 2.716 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 029 | Total loss: 2.716 | Reg loss: 0.032 | Tree loss: 2.716 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 029 | Total loss: 2.644 | Reg loss: 0.032 | Tree loss: 2.644 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 029 | Total loss: 2.665 | Reg loss: 0.032 | Tree loss: 2.665 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 029 | Total loss: 2.727 | Reg loss: 0.032 | Tree loss: 2.727 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 029 | Total loss: 2.697 | Reg loss: 0.032 | Tree loss: 2.697 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 029 | Total loss: 2.705 | Reg loss: 0.032 | Tree loss: 2.705 | Accuracy: 0.224138 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 029 | Total loss: 2.796 | Reg loss: 0.032 | Tree loss: 2.796 | Accuracy: 0.187500 | 0.861 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 029 | Total loss: 2.796 | Reg loss: 0.032 | Tree loss: 2.796 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 029 | Total loss: 2.741 | Reg loss: 0.032 | Tree loss: 2.741 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 029 | Total loss: 2.748 | Reg loss: 0.032 | Tree loss: 2.748 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 029 | Total loss: 2.784 | Reg loss: 0.032 | Tree loss: 2.784 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 029 | Total loss: 2.775 | Reg loss: 0.032 | Tree loss: 2.775 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 029 | Total loss: 2.851 | Reg loss: 0.032 | Tree loss: 2.851 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 029 | Total loss: 2.754 | Reg loss: 0.032 | Tree loss: 2.754 | Accuracy: 0.236328 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 029 | Total loss: 2.771 | Reg loss: 0.032 | Tree loss: 2.771 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 029 | Total loss: 2.764 | Reg loss: 0.032 | Tree loss: 2.764 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 029 | Total loss: 2.722 | Reg loss: 0.032 | Tree loss: 2.722 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 029 | Total loss: 2.739 | Reg loss: 0.032 | Tree loss: 2.739 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 029 | Total loss: 2.761 | Reg loss: 0.032 | Tree loss: 2.761 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 029 | Total loss: 2.734 | Reg loss: 0.032 | Tree loss: 2.734 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 029 | Total loss: 2.716 | Reg loss: 0.032 | Tree loss: 2.716 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 029 | Total loss: 2.737 | Reg loss: 0.032 | Tree loss: 2.737 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 029 | Total loss: 2.733 | Reg loss: 0.032 | Tree loss: 2.733 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 029 | Total loss: 2.739 | Reg loss: 0.032 | Tree loss: 2.739 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 029 | Total loss: 2.728 | Reg loss: 0.032 | Tree loss: 2.728 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 029 | Total loss: 2.688 | Reg loss: 0.032 | Tree loss: 2.688 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 029 | Total loss: 2.646 | Reg loss: 0.032 | Tree loss: 2.646 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 029 | Total loss: 2.697 | Reg loss: 0.032 | Tree loss: 2.697 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 029 | Total loss: 2.663 | Reg loss: 0.032 | Tree loss: 2.663 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 029 | Total loss: 2.658 | Reg loss: 0.032 | Tree loss: 2.658 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 029 | Total loss: 2.699 | Reg loss: 0.032 | Tree loss: 2.699 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 029 | Total loss: 2.675 | Reg loss: 0.032 | Tree loss: 2.675 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 029 | Total loss: 2.649 | Reg loss: 0.032 | Tree loss: 2.649 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 029 | Total loss: 2.621 | Reg loss: 0.032 | Tree loss: 2.621 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 029 | Total loss: 2.708 | Reg loss: 0.032 | Tree loss: 2.708 | Accuracy: 0.172414 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 029 | Total loss: 2.763 | Reg loss: 0.032 | Tree loss: 2.763 | Accuracy: 0.201172 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 029 | Total loss: 2.748 | Reg loss: 0.032 | Tree loss: 2.748 | Accuracy: 0.212891 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 029 | Total loss: 2.752 | Reg loss: 0.032 | Tree loss: 2.752 | Accuracy: 0.191406 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 029 | Total loss: 2.723 | Reg loss: 0.032 | Tree loss: 2.723 | Accuracy: 0.195312 | 0.861 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 029 | Total loss: 2.718 | Reg loss: 0.032 | Tree loss: 2.718 | Accuracy: 0.187500 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 005 / 029 | Total loss: 2.693 | Reg loss: 0.032 | Tree loss: 2.693 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 029 | Total loss: 2.711 | Reg loss: 0.032 | Tree loss: 2.711 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 029 | Total loss: 2.713 | Reg loss: 0.032 | Tree loss: 2.713 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 029 | Total loss: 2.681 | Reg loss: 0.032 | Tree loss: 2.681 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 029 | Total loss: 2.697 | Reg loss: 0.032 | Tree loss: 2.697 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 029 | Total loss: 2.702 | Reg loss: 0.032 | Tree loss: 2.702 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 029 | Total loss: 2.685 | Reg loss: 0.032 | Tree loss: 2.685 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 029 | Total loss: 2.681 | Reg loss: 0.032 | Tree loss: 2.681 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 029 | Total loss: 2.699 | Reg loss: 0.032 | Tree loss: 2.699 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 029 | Total loss: 2.711 | Reg loss: 0.032 | Tree loss: 2.711 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 029 | Total loss: 2.657 | Reg loss: 0.032 | Tree loss: 2.657 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 029 | Total loss: 2.689 | Reg loss: 0.032 | Tree loss: 2.689 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 029 | Total loss: 2.656 | Reg loss: 0.032 | Tree loss: 2.656 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 029 | Total loss: 2.698 | Reg loss: 0.032 | Tree loss: 2.698 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 029 | Total loss: 2.671 | Reg loss: 0.032 | Tree loss: 2.671 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 029 | Total loss: 2.600 | Reg loss: 0.032 | Tree loss: 2.600 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 029 | Total loss: 2.619 | Reg loss: 0.032 | Tree loss: 2.619 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 029 | Total loss: 2.640 | Reg loss: 0.032 | Tree loss: 2.640 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 029 | Total loss: 2.619 | Reg loss: 0.032 | Tree loss: 2.619 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 029 | Total loss: 2.637 | Reg loss: 0.032 | Tree loss: 2.637 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 029 | Total loss: 2.622 | Reg loss: 0.032 | Tree loss: 2.622 | Accuracy: 0.244141 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 029 | Total loss: 2.591 | Reg loss: 0.032 | Tree loss: 2.591 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 029 | Total loss: 2.607 | Reg loss: 0.032 | Tree loss: 2.607 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 029 | Total loss: 2.749 | Reg loss: 0.032 | Tree loss: 2.749 | Accuracy: 0.155172 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 029 | Total loss: 2.696 | Reg loss: 0.032 | Tree loss: 2.696 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 029 | Total loss: 2.668 | Reg loss: 0.032 | Tree loss: 2.668 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 029 | Total loss: 2.676 | Reg loss: 0.032 | Tree loss: 2.676 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 029 | Total loss: 2.692 | Reg loss: 0.032 | Tree loss: 2.692 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 029 | Total loss: 2.700 | Reg loss: 0.032 | Tree loss: 2.700 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 029 | Total loss: 2.699 | Reg loss: 0.032 | Tree loss: 2.699 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 029 | Total loss: 2.716 | Reg loss: 0.032 | Tree loss: 2.716 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 029 | Total loss: 2.689 | Reg loss: 0.032 | Tree loss: 2.689 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 029 | Total loss: 2.671 | Reg loss: 0.032 | Tree loss: 2.671 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 029 | Total loss: 2.682 | Reg loss: 0.032 | Tree loss: 2.682 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 029 | Total loss: 2.620 | Reg loss: 0.032 | Tree loss: 2.620 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 029 | Total loss: 2.587 | Reg loss: 0.032 | Tree loss: 2.587 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 029 | Total loss: 2.652 | Reg loss: 0.032 | Tree loss: 2.652 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 029 | Total loss: 2.642 | Reg loss: 0.032 | Tree loss: 2.642 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 029 | Total loss: 2.598 | Reg loss: 0.032 | Tree loss: 2.598 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 029 | Total loss: 2.621 | Reg loss: 0.032 | Tree loss: 2.621 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 029 | Total loss: 2.587 | Reg loss: 0.032 | Tree loss: 2.587 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 029 | Total loss: 2.600 | Reg loss: 0.032 | Tree loss: 2.600 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 029 | Total loss: 2.642 | Reg loss: 0.032 | Tree loss: 2.642 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 029 | Total loss: 2.647 | Reg loss: 0.032 | Tree loss: 2.647 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 029 | Total loss: 2.584 | Reg loss: 0.032 | Tree loss: 2.584 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 029 | Total loss: 2.577 | Reg loss: 0.032 | Tree loss: 2.577 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 029 | Total loss: 2.591 | Reg loss: 0.032 | Tree loss: 2.591 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 029 | Total loss: 2.571 | Reg loss: 0.032 | Tree loss: 2.571 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 029 | Total loss: 2.568 | Reg loss: 0.032 | Tree loss: 2.568 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 029 | Total loss: 2.561 | Reg loss: 0.032 | Tree loss: 2.561 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 029 | Total loss: 2.625 | Reg loss: 0.032 | Tree loss: 2.625 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 029 | Total loss: 2.586 | Reg loss: 0.032 | Tree loss: 2.586 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 029 | Total loss: 2.528 | Reg loss: 0.032 | Tree loss: 2.528 | Accuracy: 0.275862 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 029 | Total loss: 2.677 | Reg loss: 0.032 | Tree loss: 2.677 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 029 | Total loss: 2.641 | Reg loss: 0.032 | Tree loss: 2.641 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 029 | Total loss: 2.629 | Reg loss: 0.032 | Tree loss: 2.629 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 029 | Total loss: 2.634 | Reg loss: 0.032 | Tree loss: 2.634 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 029 | Total loss: 2.642 | Reg loss: 0.032 | Tree loss: 2.642 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 029 | Total loss: 2.616 | Reg loss: 0.032 | Tree loss: 2.616 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 029 | Total loss: 2.617 | Reg loss: 0.032 | Tree loss: 2.617 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 029 | Total loss: 2.649 | Reg loss: 0.032 | Tree loss: 2.649 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 029 | Total loss: 2.602 | Reg loss: 0.032 | Tree loss: 2.602 | Accuracy: 0.226562 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 009 / 029 | Total loss: 2.618 | Reg loss: 0.032 | Tree loss: 2.618 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 029 | Total loss: 2.620 | Reg loss: 0.032 | Tree loss: 2.620 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 029 | Total loss: 2.568 | Reg loss: 0.032 | Tree loss: 2.568 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 029 | Total loss: 2.634 | Reg loss: 0.032 | Tree loss: 2.634 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 029 | Total loss: 2.563 | Reg loss: 0.032 | Tree loss: 2.563 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 029 | Total loss: 2.577 | Reg loss: 0.032 | Tree loss: 2.577 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 029 | Total loss: 2.587 | Reg loss: 0.032 | Tree loss: 2.587 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 029 | Total loss: 2.639 | Reg loss: 0.032 | Tree loss: 2.639 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 029 | Total loss: 2.602 | Reg loss: 0.032 | Tree loss: 2.602 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 029 | Total loss: 2.542 | Reg loss: 0.032 | Tree loss: 2.542 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 029 | Total loss: 2.577 | Reg loss: 0.032 | Tree loss: 2.577 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 029 | Total loss: 2.549 | Reg loss: 0.032 | Tree loss: 2.549 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 029 | Total loss: 2.544 | Reg loss: 0.032 | Tree loss: 2.544 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 029 | Total loss: 2.546 | Reg loss: 0.032 | Tree loss: 2.546 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 029 | Total loss: 2.596 | Reg loss: 0.032 | Tree loss: 2.596 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 029 | Total loss: 2.540 | Reg loss: 0.032 | Tree loss: 2.540 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 029 | Total loss: 2.556 | Reg loss: 0.032 | Tree loss: 2.556 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 029 | Total loss: 2.519 | Reg loss: 0.032 | Tree loss: 2.519 | Accuracy: 0.226562 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 029 | Total loss: 2.564 | Reg loss: 0.032 | Tree loss: 2.564 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 029 | Total loss: 2.418 | Reg loss: 0.032 | Tree loss: 2.418 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 029 | Total loss: 2.647 | Reg loss: 0.032 | Tree loss: 2.647 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 029 | Total loss: 2.658 | Reg loss: 0.032 | Tree loss: 2.658 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 029 | Total loss: 2.557 | Reg loss: 0.032 | Tree loss: 2.557 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 029 | Total loss: 2.620 | Reg loss: 0.032 | Tree loss: 2.620 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 029 | Total loss: 2.577 | Reg loss: 0.032 | Tree loss: 2.577 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 029 | Total loss: 2.606 | Reg loss: 0.032 | Tree loss: 2.606 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 029 | Total loss: 2.643 | Reg loss: 0.032 | Tree loss: 2.643 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 029 | Total loss: 2.594 | Reg loss: 0.032 | Tree loss: 2.594 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 029 | Total loss: 2.601 | Reg loss: 0.032 | Tree loss: 2.601 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 029 | Total loss: 2.531 | Reg loss: 0.032 | Tree loss: 2.531 | Accuracy: 0.236328 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 029 | Total loss: 2.607 | Reg loss: 0.032 | Tree loss: 2.607 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 029 | Total loss: 2.539 | Reg loss: 0.032 | Tree loss: 2.539 | Accuracy: 0.248047 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 029 | Total loss: 2.532 | Reg loss: 0.032 | Tree loss: 2.532 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 029 | Total loss: 2.553 | Reg loss: 0.032 | Tree loss: 2.553 | Accuracy: 0.232422 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 029 | Total loss: 2.570 | Reg loss: 0.032 | Tree loss: 2.570 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 029 | Total loss: 2.516 | Reg loss: 0.032 | Tree loss: 2.516 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 029 | Total loss: 2.582 | Reg loss: 0.032 | Tree loss: 2.582 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 029 | Total loss: 2.515 | Reg loss: 0.032 | Tree loss: 2.515 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 029 | Total loss: 2.522 | Reg loss: 0.032 | Tree loss: 2.522 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 029 | Total loss: 2.543 | Reg loss: 0.032 | Tree loss: 2.543 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 029 | Total loss: 2.523 | Reg loss: 0.032 | Tree loss: 2.523 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 029 | Total loss: 2.499 | Reg loss: 0.032 | Tree loss: 2.499 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 029 | Total loss: 2.524 | Reg loss: 0.032 | Tree loss: 2.524 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 029 | Total loss: 2.527 | Reg loss: 0.032 | Tree loss: 2.527 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 029 | Total loss: 2.523 | Reg loss: 0.032 | Tree loss: 2.523 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 029 | Total loss: 2.542 | Reg loss: 0.032 | Tree loss: 2.542 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 029 | Total loss: 2.484 | Reg loss: 0.032 | Tree loss: 2.484 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 029 | Total loss: 2.514 | Reg loss: 0.032 | Tree loss: 2.514 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 029 | Total loss: 2.482 | Reg loss: 0.032 | Tree loss: 2.482 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 029 | Total loss: 2.601 | Reg loss: 0.032 | Tree loss: 2.601 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 029 | Total loss: 2.553 | Reg loss: 0.032 | Tree loss: 2.553 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 029 | Total loss: 2.575 | Reg loss: 0.032 | Tree loss: 2.575 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 029 | Total loss: 2.639 | Reg loss: 0.032 | Tree loss: 2.639 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 029 | Total loss: 2.575 | Reg loss: 0.032 | Tree loss: 2.575 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 029 | Total loss: 2.534 | Reg loss: 0.032 | Tree loss: 2.534 | Accuracy: 0.238281 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 029 | Total loss: 2.581 | Reg loss: 0.032 | Tree loss: 2.581 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 029 | Total loss: 2.533 | Reg loss: 0.032 | Tree loss: 2.533 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 029 | Total loss: 2.559 | Reg loss: 0.032 | Tree loss: 2.559 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 029 | Total loss: 2.493 | Reg loss: 0.032 | Tree loss: 2.493 | Accuracy: 0.240234 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 029 | Total loss: 2.552 | Reg loss: 0.032 | Tree loss: 2.552 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 029 | Total loss: 2.514 | Reg loss: 0.032 | Tree loss: 2.514 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 029 | Total loss: 2.545 | Reg loss: 0.032 | Tree loss: 2.545 | Accuracy: 0.199219 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 013 / 029 | Total loss: 2.539 | Reg loss: 0.032 | Tree loss: 2.539 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 029 | Total loss: 2.546 | Reg loss: 0.032 | Tree loss: 2.546 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 029 | Total loss: 2.506 | Reg loss: 0.032 | Tree loss: 2.506 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 029 | Total loss: 2.485 | Reg loss: 0.032 | Tree loss: 2.485 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 029 | Total loss: 2.568 | Reg loss: 0.032 | Tree loss: 2.568 | Accuracy: 0.166016 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 029 | Total loss: 2.528 | Reg loss: 0.032 | Tree loss: 2.528 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 029 | Total loss: 2.489 | Reg loss: 0.032 | Tree loss: 2.489 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 029 | Total loss: 2.513 | Reg loss: 0.032 | Tree loss: 2.513 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 029 | Total loss: 2.498 | Reg loss: 0.032 | Tree loss: 2.498 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 029 | Total loss: 2.456 | Reg loss: 0.032 | Tree loss: 2.456 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 029 | Total loss: 2.475 | Reg loss: 0.032 | Tree loss: 2.475 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 029 | Total loss: 2.471 | Reg loss: 0.032 | Tree loss: 2.471 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 029 | Total loss: 2.449 | Reg loss: 0.032 | Tree loss: 2.449 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 029 | Total loss: 2.501 | Reg loss: 0.032 | Tree loss: 2.501 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 029 | Total loss: 2.489 | Reg loss: 0.032 | Tree loss: 2.489 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 029 | Total loss: 2.417 | Reg loss: 0.032 | Tree loss: 2.417 | Accuracy: 0.172414 | 0.86 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 029 | Total loss: 2.579 | Reg loss: 0.032 | Tree loss: 2.579 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 029 | Total loss: 2.532 | Reg loss: 0.032 | Tree loss: 2.532 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 029 | Total loss: 2.513 | Reg loss: 0.032 | Tree loss: 2.513 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 029 | Total loss: 2.512 | Reg loss: 0.032 | Tree loss: 2.512 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 029 | Total loss: 2.590 | Reg loss: 0.032 | Tree loss: 2.590 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 029 | Total loss: 2.540 | Reg loss: 0.032 | Tree loss: 2.540 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 029 | Total loss: 2.535 | Reg loss: 0.032 | Tree loss: 2.535 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 029 | Total loss: 2.483 | Reg loss: 0.032 | Tree loss: 2.483 | Accuracy: 0.240234 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 029 | Total loss: 2.538 | Reg loss: 0.032 | Tree loss: 2.538 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 029 | Total loss: 2.495 | Reg loss: 0.032 | Tree loss: 2.495 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 029 | Total loss: 2.528 | Reg loss: 0.032 | Tree loss: 2.528 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 029 | Total loss: 2.472 | Reg loss: 0.032 | Tree loss: 2.472 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 029 | Total loss: 2.503 | Reg loss: 0.032 | Tree loss: 2.503 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 029 | Total loss: 2.503 | Reg loss: 0.032 | Tree loss: 2.503 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 029 | Total loss: 2.456 | Reg loss: 0.032 | Tree loss: 2.456 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 029 | Total loss: 2.481 | Reg loss: 0.032 | Tree loss: 2.481 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 029 | Total loss: 2.463 | Reg loss: 0.032 | Tree loss: 2.463 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 029 | Total loss: 2.548 | Reg loss: 0.032 | Tree loss: 2.548 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 029 | Total loss: 2.492 | Reg loss: 0.032 | Tree loss: 2.492 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 029 | Total loss: 2.464 | Reg loss: 0.032 | Tree loss: 2.464 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 029 | Total loss: 2.553 | Reg loss: 0.032 | Tree loss: 2.553 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 029 | Total loss: 2.455 | Reg loss: 0.032 | Tree loss: 2.455 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 029 | Total loss: 2.477 | Reg loss: 0.032 | Tree loss: 2.477 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 029 | Total loss: 2.452 | Reg loss: 0.032 | Tree loss: 2.452 | Accuracy: 0.238281 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 029 | Total loss: 2.470 | Reg loss: 0.032 | Tree loss: 2.470 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 029 | Total loss: 2.435 | Reg loss: 0.032 | Tree loss: 2.435 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 029 | Total loss: 2.447 | Reg loss: 0.032 | Tree loss: 2.447 | Accuracy: 0.238281 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 029 | Total loss: 2.459 | Reg loss: 0.032 | Tree loss: 2.459 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 029 | Total loss: 2.504 | Reg loss: 0.032 | Tree loss: 2.504 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 029 | Total loss: 2.532 | Reg loss: 0.032 | Tree loss: 2.532 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 029 | Total loss: 2.574 | Reg loss: 0.032 | Tree loss: 2.574 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 029 | Total loss: 2.481 | Reg loss: 0.032 | Tree loss: 2.481 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 029 | Total loss: 2.514 | Reg loss: 0.032 | Tree loss: 2.514 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 029 | Total loss: 2.522 | Reg loss: 0.032 | Tree loss: 2.522 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 029 | Total loss: 2.497 | Reg loss: 0.032 | Tree loss: 2.497 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 029 | Total loss: 2.512 | Reg loss: 0.032 | Tree loss: 2.512 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 029 | Total loss: 2.484 | Reg loss: 0.032 | Tree loss: 2.484 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 029 | Total loss: 2.539 | Reg loss: 0.032 | Tree loss: 2.539 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 029 | Total loss: 2.456 | Reg loss: 0.032 | Tree loss: 2.456 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 029 | Total loss: 2.480 | Reg loss: 0.032 | Tree loss: 2.480 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 029 | Total loss: 2.448 | Reg loss: 0.032 | Tree loss: 2.448 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 029 | Total loss: 2.535 | Reg loss: 0.032 | Tree loss: 2.535 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 029 | Total loss: 2.500 | Reg loss: 0.032 | Tree loss: 2.500 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 029 | Total loss: 2.405 | Reg loss: 0.032 | Tree loss: 2.405 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 029 | Total loss: 2.413 | Reg loss: 0.032 | Tree loss: 2.413 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 029 | Total loss: 2.499 | Reg loss: 0.032 | Tree loss: 2.499 | Accuracy: 0.214844 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 017 / 029 | Total loss: 2.403 | Reg loss: 0.032 | Tree loss: 2.403 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 029 | Total loss: 2.475 | Reg loss: 0.032 | Tree loss: 2.475 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 029 | Total loss: 2.446 | Reg loss: 0.032 | Tree loss: 2.446 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 029 | Total loss: 2.442 | Reg loss: 0.032 | Tree loss: 2.442 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 029 | Total loss: 2.504 | Reg loss: 0.032 | Tree loss: 2.504 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 029 | Total loss: 2.463 | Reg loss: 0.032 | Tree loss: 2.463 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 029 | Total loss: 2.438 | Reg loss: 0.032 | Tree loss: 2.438 | Accuracy: 0.226562 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 029 | Total loss: 2.426 | Reg loss: 0.032 | Tree loss: 2.426 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 029 | Total loss: 2.478 | Reg loss: 0.032 | Tree loss: 2.478 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 029 | Total loss: 2.417 | Reg loss: 0.032 | Tree loss: 2.417 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 029 | Total loss: 2.304 | Reg loss: 0.032 | Tree loss: 2.304 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 029 | Total loss: 2.468 | Reg loss: 0.032 | Tree loss: 2.468 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 029 | Total loss: 2.502 | Reg loss: 0.032 | Tree loss: 2.502 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 029 | Total loss: 2.496 | Reg loss: 0.032 | Tree loss: 2.496 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 029 | Total loss: 2.497 | Reg loss: 0.032 | Tree loss: 2.497 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 029 | Total loss: 2.464 | Reg loss: 0.032 | Tree loss: 2.464 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 029 | Total loss: 2.469 | Reg loss: 0.032 | Tree loss: 2.469 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 029 | Total loss: 2.499 | Reg loss: 0.032 | Tree loss: 2.499 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 029 | Total loss: 2.453 | Reg loss: 0.032 | Tree loss: 2.453 | Accuracy: 0.224609 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 029 | Total loss: 2.501 | Reg loss: 0.032 | Tree loss: 2.501 | Accuracy: 0.232422 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 029 | Total loss: 2.464 | Reg loss: 0.032 | Tree loss: 2.464 | Accuracy: 0.246094 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 029 | Total loss: 2.501 | Reg loss: 0.032 | Tree loss: 2.501 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 029 | Total loss: 2.459 | Reg loss: 0.032 | Tree loss: 2.459 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 029 | Total loss: 2.488 | Reg loss: 0.032 | Tree loss: 2.488 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 029 | Total loss: 2.447 | Reg loss: 0.032 | Tree loss: 2.447 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 029 | Total loss: 2.459 | Reg loss: 0.032 | Tree loss: 2.459 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 029 | Total loss: 2.449 | Reg loss: 0.032 | Tree loss: 2.449 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 029 | Total loss: 2.413 | Reg loss: 0.032 | Tree loss: 2.413 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 029 | Total loss: 2.438 | Reg loss: 0.032 | Tree loss: 2.438 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 029 | Total loss: 2.405 | Reg loss: 0.032 | Tree loss: 2.405 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 029 | Total loss: 2.426 | Reg loss: 0.032 | Tree loss: 2.426 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 029 | Total loss: 2.436 | Reg loss: 0.032 | Tree loss: 2.436 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 029 | Total loss: 2.437 | Reg loss: 0.032 | Tree loss: 2.437 | Accuracy: 0.167969 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 029 | Total loss: 2.409 | Reg loss: 0.032 | Tree loss: 2.409 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 029 | Total loss: 2.388 | Reg loss: 0.032 | Tree loss: 2.388 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 029 | Total loss: 2.411 | Reg loss: 0.032 | Tree loss: 2.411 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 029 | Total loss: 2.414 | Reg loss: 0.032 | Tree loss: 2.414 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 029 | Total loss: 2.417 | Reg loss: 0.032 | Tree loss: 2.417 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 029 | Total loss: 2.391 | Reg loss: 0.032 | Tree loss: 2.391 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 029 | Total loss: 2.533 | Reg loss: 0.032 | Tree loss: 2.533 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.236328 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 029 | Total loss: 2.443 | Reg loss: 0.032 | Tree loss: 2.443 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 029 | Total loss: 2.496 | Reg loss: 0.032 | Tree loss: 2.496 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 029 | Total loss: 2.436 | Reg loss: 0.032 | Tree loss: 2.436 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 029 | Total loss: 2.437 | Reg loss: 0.032 | Tree loss: 2.437 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 029 | Total loss: 2.503 | Reg loss: 0.032 | Tree loss: 2.503 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 029 | Total loss: 2.478 | Reg loss: 0.032 | Tree loss: 2.478 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 029 | Total loss: 2.472 | Reg loss: 0.032 | Tree loss: 2.472 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 029 | Total loss: 2.450 | Reg loss: 0.032 | Tree loss: 2.450 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 029 | Total loss: 2.437 | Reg loss: 0.032 | Tree loss: 2.437 | Accuracy: 0.193359 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 029 | Total loss: 2.460 | Reg loss: 0.032 | Tree loss: 2.460 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 029 | Total loss: 2.443 | Reg loss: 0.032 | Tree loss: 2.443 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 029 | Total loss: 2.453 | Reg loss: 0.032 | Tree loss: 2.453 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 029 | Total loss: 2.405 | Reg loss: 0.032 | Tree loss: 2.405 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 029 | Total loss: 2.436 | Reg loss: 0.032 | Tree loss: 2.436 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 029 | Total loss: 2.434 | Reg loss: 0.032 | Tree loss: 2.434 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 029 | Total loss: 2.417 | Reg loss: 0.032 | Tree loss: 2.417 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 029 | Total loss: 2.459 | Reg loss: 0.032 | Tree loss: 2.459 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 029 | Total loss: 2.403 | Reg loss: 0.032 | Tree loss: 2.403 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 029 | Total loss: 2.428 | Reg loss: 0.032 | Tree loss: 2.428 | Accuracy: 0.193359 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 021 / 029 | Total loss: 2.381 | Reg loss: 0.032 | Tree loss: 2.381 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 029 | Total loss: 2.425 | Reg loss: 0.032 | Tree loss: 2.425 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 029 | Total loss: 2.383 | Reg loss: 0.032 | Tree loss: 2.383 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 029 | Total loss: 2.374 | Reg loss: 0.032 | Tree loss: 2.374 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 029 | Total loss: 2.390 | Reg loss: 0.032 | Tree loss: 2.390 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 029 | Total loss: 2.428 | Reg loss: 0.032 | Tree loss: 2.428 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 029 | Total loss: 2.398 | Reg loss: 0.032 | Tree loss: 2.398 | Accuracy: 0.228516 | 0.859 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 029 | Total loss: 2.303 | Reg loss: 0.032 | Tree loss: 2.303 | Accuracy: 0.224138 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 029 | Total loss: 2.466 | Reg loss: 0.032 | Tree loss: 2.466 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 029 | Total loss: 2.462 | Reg loss: 0.032 | Tree loss: 2.462 | Accuracy: 0.179688 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 029 | Total loss: 2.442 | Reg loss: 0.032 | Tree loss: 2.442 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 029 | Total loss: 2.471 | Reg loss: 0.032 | Tree loss: 2.471 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 029 | Total loss: 2.445 | Reg loss: 0.032 | Tree loss: 2.445 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 029 | Total loss: 2.418 | Reg loss: 0.032 | Tree loss: 2.418 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 029 | Total loss: 2.448 | Reg loss: 0.032 | Tree loss: 2.448 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 029 | Total loss: 2.445 | Reg loss: 0.032 | Tree loss: 2.445 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 029 | Total loss: 2.423 | Reg loss: 0.032 | Tree loss: 2.423 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 029 | Total loss: 2.473 | Reg loss: 0.032 | Tree loss: 2.473 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 029 | Total loss: 2.422 | Reg loss: 0.032 | Tree loss: 2.422 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 029 | Total loss: 2.444 | Reg loss: 0.032 | Tree loss: 2.444 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 029 | Total loss: 2.393 | Reg loss: 0.032 | Tree loss: 2.393 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 029 | Total loss: 2.407 | Reg loss: 0.032 | Tree loss: 2.407 | Accuracy: 0.232422 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 029 | Total loss: 2.369 | Reg loss: 0.032 | Tree loss: 2.369 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 029 | Total loss: 2.435 | Reg loss: 0.032 | Tree loss: 2.435 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 029 | Total loss: 2.399 | Reg loss: 0.032 | Tree loss: 2.399 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 029 | Total loss: 2.406 | Reg loss: 0.032 | Tree loss: 2.406 | Accuracy: 0.156250 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 029 | Total loss: 2.388 | Reg loss: 0.032 | Tree loss: 2.388 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 029 | Total loss: 2.344 | Reg loss: 0.032 | Tree loss: 2.344 | Accuracy: 0.248047 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 029 | Total loss: 2.389 | Reg loss: 0.032 | Tree loss: 2.389 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 029 | Total loss: 2.366 | Reg loss: 0.032 | Tree loss: 2.366 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 029 | Total loss: 2.374 | Reg loss: 0.032 | Tree loss: 2.374 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 029 | Total loss: 2.381 | Reg loss: 0.032 | Tree loss: 2.381 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 029 | Total loss: 2.364 | Reg loss: 0.032 | Tree loss: 2.364 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 029 | Total loss: 2.347 | Reg loss: 0.032 | Tree loss: 2.347 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 029 | Total loss: 2.357 | Reg loss: 0.032 | Tree loss: 2.357 | Accuracy: 0.206897 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 029 | Total loss: 2.502 | Reg loss: 0.032 | Tree loss: 2.502 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 029 | Total loss: 2.449 | Reg loss: 0.032 | Tree loss: 2.449 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.234375 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 029 | Total loss: 2.461 | Reg loss: 0.032 | Tree loss: 2.461 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 029 | Total loss: 2.411 | Reg loss: 0.032 | Tree loss: 2.411 | Accuracy: 0.244141 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 029 | Total loss: 2.443 | Reg loss: 0.032 | Tree loss: 2.443 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 029 | Total loss: 2.387 | Reg loss: 0.032 | Tree loss: 2.387 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 029 | Total loss: 2.401 | Reg loss: 0.032 | Tree loss: 2.401 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 029 | Total loss: 2.370 | Reg loss: 0.032 | Tree loss: 2.370 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 029 | Total loss: 2.432 | Reg loss: 0.032 | Tree loss: 2.432 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 029 | Total loss: 2.419 | Reg loss: 0.032 | Tree loss: 2.419 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 029 | Total loss: 2.395 | Reg loss: 0.032 | Tree loss: 2.395 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 029 | Total loss: 2.415 | Reg loss: 0.032 | Tree loss: 2.415 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 029 | Total loss: 2.386 | Reg loss: 0.032 | Tree loss: 2.386 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 029 | Total loss: 2.383 | Reg loss: 0.032 | Tree loss: 2.383 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 029 | Total loss: 2.363 | Reg loss: 0.032 | Tree loss: 2.363 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 029 | Total loss: 2.402 | Reg loss: 0.032 | Tree loss: 2.402 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 029 | Total loss: 2.404 | Reg loss: 0.032 | Tree loss: 2.404 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 029 | Total loss: 2.332 | Reg loss: 0.032 | Tree loss: 2.332 | Accuracy: 0.167969 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 029 | Total loss: 2.346 | Reg loss: 0.032 | Tree loss: 2.346 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 029 | Total loss: 2.352 | Reg loss: 0.032 | Tree loss: 2.352 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 029 | Total loss: 2.369 | Reg loss: 0.032 | Tree loss: 2.369 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 029 | Total loss: 2.379 | Reg loss: 0.032 | Tree loss: 2.379 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 029 | Total loss: 2.340 | Reg loss: 0.032 | Tree loss: 2.340 | Accuracy: 0.224609 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 025 / 029 | Total loss: 2.395 | Reg loss: 0.032 | Tree loss: 2.395 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 029 | Total loss: 2.325 | Reg loss: 0.032 | Tree loss: 2.325 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 029 | Total loss: 2.367 | Reg loss: 0.032 | Tree loss: 2.367 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 029 | Total loss: 2.426 | Reg loss: 0.032 | Tree loss: 2.426 | Accuracy: 0.172414 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 029 | Total loss: 2.408 | Reg loss: 0.032 | Tree loss: 2.408 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 029 | Total loss: 2.442 | Reg loss: 0.032 | Tree loss: 2.442 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 029 | Total loss: 2.398 | Reg loss: 0.032 | Tree loss: 2.398 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 029 | Total loss: 2.420 | Reg loss: 0.032 | Tree loss: 2.420 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 029 | Total loss: 2.434 | Reg loss: 0.032 | Tree loss: 2.434 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 029 | Total loss: 2.421 | Reg loss: 0.032 | Tree loss: 2.421 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 029 | Total loss: 2.374 | Reg loss: 0.032 | Tree loss: 2.374 | Accuracy: 0.234375 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 029 | Total loss: 2.408 | Reg loss: 0.032 | Tree loss: 2.408 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 029 | Total loss: 2.359 | Reg loss: 0.032 | Tree loss: 2.359 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 029 | Total loss: 2.403 | Reg loss: 0.032 | Tree loss: 2.403 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 029 | Total loss: 2.427 | Reg loss: 0.032 | Tree loss: 2.427 | Accuracy: 0.160156 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 029 | Total loss: 2.414 | Reg loss: 0.032 | Tree loss: 2.414 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 029 | Total loss: 2.389 | Reg loss: 0.032 | Tree loss: 2.389 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 029 | Total loss: 2.334 | Reg loss: 0.032 | Tree loss: 2.334 | Accuracy: 0.216797 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 029 | Total loss: 2.429 | Reg loss: 0.032 | Tree loss: 2.429 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 029 | Total loss: 2.360 | Reg loss: 0.032 | Tree loss: 2.360 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 029 | Total loss: 2.361 | Reg loss: 0.032 | Tree loss: 2.361 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 029 | Total loss: 2.398 | Reg loss: 0.032 | Tree loss: 2.398 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 029 | Total loss: 2.363 | Reg loss: 0.032 | Tree loss: 2.363 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 029 | Total loss: 2.342 | Reg loss: 0.032 | Tree loss: 2.342 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 029 | Total loss: 2.345 | Reg loss: 0.032 | Tree loss: 2.345 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 029 | Total loss: 2.397 | Reg loss: 0.032 | Tree loss: 2.397 | Accuracy: 0.152344 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 029 | Total loss: 2.347 | Reg loss: 0.032 | Tree loss: 2.347 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 029 | Total loss: 2.305 | Reg loss: 0.032 | Tree loss: 2.305 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 029 | Total loss: 2.273 | Reg loss: 0.032 | Tree loss: 2.273 | Accuracy: 0.251953 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 029 | Total loss: 2.372 | Reg loss: 0.032 | Tree loss: 2.372 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 029 | Total loss: 2.352 | Reg loss: 0.032 | Tree loss: 2.352 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 029 | Total loss: 2.330 | Reg loss: 0.032 | Tree loss: 2.330 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 029 | Total loss: 2.275 | Reg loss: 0.032 | Tree loss: 2.275 | Accuracy: 0.224138 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 029 | Total loss: 2.376 | Reg loss: 0.032 | Tree loss: 2.376 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 029 | Total loss: 2.463 | Reg loss: 0.032 | Tree loss: 2.463 | Accuracy: 0.169922 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 029 | Total loss: 2.359 | Reg loss: 0.032 | Tree loss: 2.359 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 029 | Total loss: 2.431 | Reg loss: 0.032 | Tree loss: 2.431 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 029 | Total loss: 2.401 | Reg loss: 0.032 | Tree loss: 2.401 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 029 | Total loss: 2.373 | Reg loss: 0.032 | Tree loss: 2.373 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 029 | Total loss: 2.363 | Reg loss: 0.032 | Tree loss: 2.363 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 029 | Total loss: 2.348 | Reg loss: 0.032 | Tree loss: 2.348 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 029 | Total loss: 2.386 | Reg loss: 0.032 | Tree loss: 2.386 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 029 | Total loss: 2.394 | Reg loss: 0.032 | Tree loss: 2.394 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 029 | Total loss: 2.362 | Reg loss: 0.032 | Tree loss: 2.362 | Accuracy: 0.261719 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.248047 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 029 | Total loss: 2.343 | Reg loss: 0.032 | Tree loss: 2.343 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 029 | Total loss: 2.360 | Reg loss: 0.032 | Tree loss: 2.360 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 029 | Total loss: 2.385 | Reg loss: 0.032 | Tree loss: 2.385 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 029 | Total loss: 2.354 | Reg loss: 0.032 | Tree loss: 2.354 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 029 | Total loss: 2.357 | Reg loss: 0.032 | Tree loss: 2.357 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 029 | Total loss: 2.353 | Reg loss: 0.032 | Tree loss: 2.353 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 029 | Total loss: 2.404 | Reg loss: 0.032 | Tree loss: 2.404 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 029 | Total loss: 2.345 | Reg loss: 0.032 | Tree loss: 2.345 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 029 | Total loss: 2.409 | Reg loss: 0.032 | Tree loss: 2.409 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 029 | Total loss: 2.347 | Reg loss: 0.032 | Tree loss: 2.347 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 029 | Total loss: 2.323 | Reg loss: 0.032 | Tree loss: 2.323 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 029 | Total loss: 2.301 | Reg loss: 0.032 | Tree loss: 2.301 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 029 | Total loss: 2.345 | Reg loss: 0.032 | Tree loss: 2.345 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 029 | Total loss: 2.323 | Reg loss: 0.032 | Tree loss: 2.323 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 029 | Total loss: 2.336 | Reg loss: 0.032 | Tree loss: 2.336 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 029 | Total loss: 2.292 | Reg loss: 0.032 | Tree loss: 2.292 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 029 | Total loss: 2.229 | Reg loss: 0.032 | Tree loss: 2.229 | Accuracy: 0.258621 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 029 | Total loss: 2.415 | Reg loss: 0.032 | Tree loss: 2.415 | Accuracy: 0.171875 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 029 | Total loss: 2.382 | Reg loss: 0.032 | Tree loss: 2.382 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 029 | Total loss: 2.392 | Reg loss: 0.032 | Tree loss: 2.392 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 029 | Total loss: 2.367 | Reg loss: 0.032 | Tree loss: 2.367 | Accuracy: 0.230469 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 029 | Total loss: 2.350 | Reg loss: 0.032 | Tree loss: 2.350 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 029 | Total loss: 2.367 | Reg loss: 0.032 | Tree loss: 2.367 | Accuracy: 0.210938 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 029 | Total loss: 2.401 | Reg loss: 0.032 | Tree loss: 2.401 | Accuracy: 0.199219 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 029 | Total loss: 2.428 | Reg loss: 0.032 | Tree loss: 2.428 | Accuracy: 0.228516 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 029 | Total loss: 2.342 | Reg loss: 0.032 | Tree loss: 2.342 | Accuracy: 0.222656 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 029 | Total loss: 2.345 | Reg loss: 0.032 | Tree loss: 2.345 | Accuracy: 0.187500 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 029 | Total loss: 2.362 | Reg loss: 0.032 | Tree loss: 2.362 | Accuracy: 0.207031 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 029 | Total loss: 2.371 | Reg loss: 0.032 | Tree loss: 2.371 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 029 | Total loss: 2.334 | Reg loss: 0.032 | Tree loss: 2.334 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 029 | Total loss: 2.374 | Reg loss: 0.032 | Tree loss: 2.374 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 029 | Total loss: 2.360 | Reg loss: 0.032 | Tree loss: 2.360 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 029 | Total loss: 2.295 | Reg loss: 0.032 | Tree loss: 2.295 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 029 | Total loss: 2.353 | Reg loss: 0.032 | Tree loss: 2.353 | Accuracy: 0.175781 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 029 | Total loss: 2.329 | Reg loss: 0.032 | Tree loss: 2.329 | Accuracy: 0.220703 | 0.86 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 029 | Total loss: 2.328 | Reg loss: 0.032 | Tree loss: 2.328 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 029 | Total loss: 2.318 | Reg loss: 0.032 | Tree loss: 2.318 | Accuracy: 0.240234 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 029 | Total loss: 2.344 | Reg loss: 0.032 | Tree loss: 2.344 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 029 | Total loss: 2.316 | Reg loss: 0.032 | Tree loss: 2.316 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 029 | Total loss: 2.334 | Reg loss: 0.032 | Tree loss: 2.334 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 029 | Total loss: 2.374 | Reg loss: 0.032 | Tree loss: 2.374 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 029 | Total loss: 2.296 | Reg loss: 0.032 | Tree loss: 2.296 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 029 | Total loss: 2.337 | Reg loss: 0.032 | Tree loss: 2.337 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 029 | Total loss: 2.307 | Reg loss: 0.032 | Tree loss: 2.307 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 029 | Total loss: 2.329 | Reg loss: 0.032 | Tree loss: 2.329 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 029 | Total loss: 2.300 | Reg loss: 0.032 | Tree loss: 2.300 | Accuracy: 0.155172 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 029 | Total loss: 2.396 | Reg loss: 0.031 | Tree loss: 2.396 | Accuracy: 0.197266 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 029 | Total loss: 2.368 | Reg loss: 0.031 | Tree loss: 2.368 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 029 | Total loss: 2.381 | Reg loss: 0.031 | Tree loss: 2.381 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 029 | Total loss: 2.327 | Reg loss: 0.031 | Tree loss: 2.327 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 029 | Total loss: 2.373 | Reg loss: 0.031 | Tree loss: 2.373 | Accuracy: 0.183594 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 029 | Total loss: 2.383 | Reg loss: 0.031 | Tree loss: 2.383 | Accuracy: 0.191406 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 029 | Total loss: 2.366 | Reg loss: 0.031 | Tree loss: 2.366 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 029 | Total loss: 2.332 | Reg loss: 0.031 | Tree loss: 2.332 | Accuracy: 0.226562 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 029 | Total loss: 2.327 | Reg loss: 0.031 | Tree loss: 2.327 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 029 | Total loss: 2.353 | Reg loss: 0.031 | Tree loss: 2.353 | Accuracy: 0.212891 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 029 | Total loss: 2.364 | Reg loss: 0.031 | Tree loss: 2.364 | Accuracy: 0.201172 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 029 | Total loss: 2.354 | Reg loss: 0.032 | Tree loss: 2.354 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 029 | Total loss: 2.353 | Reg loss: 0.032 | Tree loss: 2.353 | Accuracy: 0.185547 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 029 | Total loss: 2.317 | Reg loss: 0.032 | Tree loss: 2.317 | Accuracy: 0.208984 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 029 | Total loss: 2.330 | Reg loss: 0.032 | Tree loss: 2.330 | Accuracy: 0.173828 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 029 | Total loss: 2.314 | Reg loss: 0.032 | Tree loss: 2.314 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 029 | Total loss: 2.315 | Reg loss: 0.032 | Tree loss: 2.315 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 029 | Total loss: 2.339 | Reg loss: 0.032 | Tree loss: 2.339 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 029 | Total loss: 2.310 | Reg loss: 0.032 | Tree loss: 2.310 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 029 | Total loss: 2.296 | Reg loss: 0.032 | Tree loss: 2.296 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 029 | Total loss: 2.378 | Reg loss: 0.032 | Tree loss: 2.378 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 029 | Total loss: 2.314 | Reg loss: 0.032 | Tree loss: 2.314 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 029 | Total loss: 2.298 | Reg loss: 0.032 | Tree loss: 2.298 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 029 | Total loss: 2.317 | Reg loss: 0.032 | Tree loss: 2.317 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 029 | Total loss: 2.316 | Reg loss: 0.032 | Tree loss: 2.316 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 029 | Total loss: 2.298 | Reg loss: 0.032 | Tree loss: 2.298 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 029 | Total loss: 2.305 | Reg loss: 0.032 | Tree loss: 2.305 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 029 | Total loss: 2.350 | Reg loss: 0.032 | Tree loss: 2.350 | Accuracy: 0.120690 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 029 | Total loss: 2.389 | Reg loss: 0.031 | Tree loss: 2.389 | Accuracy: 0.181641 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 029 | Total loss: 2.400 | Reg loss: 0.031 | Tree loss: 2.400 | Accuracy: 0.189453 | 0.86 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 002 / 029 | Total loss: 2.347 | Reg loss: 0.031 | Tree loss: 2.347 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 029 | Total loss: 2.385 | Reg loss: 0.031 | Tree loss: 2.385 | Accuracy: 0.162109 | 0.86 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 029 | Total loss: 2.391 | Reg loss: 0.031 | Tree loss: 2.391 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 029 | Total loss: 2.354 | Reg loss: 0.031 | Tree loss: 2.354 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 029 | Total loss: 2.369 | Reg loss: 0.031 | Tree loss: 2.369 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 029 | Total loss: 2.350 | Reg loss: 0.031 | Tree loss: 2.350 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 029 | Total loss: 2.377 | Reg loss: 0.031 | Tree loss: 2.377 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 029 | Total loss: 2.326 | Reg loss: 0.031 | Tree loss: 2.326 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 029 | Total loss: 2.348 | Reg loss: 0.031 | Tree loss: 2.348 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 029 | Total loss: 2.336 | Reg loss: 0.031 | Tree loss: 2.336 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.242188 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 029 | Total loss: 2.286 | Reg loss: 0.031 | Tree loss: 2.286 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 029 | Total loss: 2.294 | Reg loss: 0.031 | Tree loss: 2.294 | Accuracy: 0.236328 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 029 | Total loss: 2.329 | Reg loss: 0.031 | Tree loss: 2.329 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 029 | Total loss: 2.310 | Reg loss: 0.031 | Tree loss: 2.310 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 029 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 029 | Total loss: 2.305 | Reg loss: 0.031 | Tree loss: 2.305 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 029 | Total loss: 2.327 | Reg loss: 0.031 | Tree loss: 2.327 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 029 | Total loss: 2.320 | Reg loss: 0.031 | Tree loss: 2.320 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 029 | Total loss: 2.303 | Reg loss: 0.031 | Tree loss: 2.303 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 029 | Total loss: 2.322 | Reg loss: 0.031 | Tree loss: 2.322 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 029 | Total loss: 2.306 | Reg loss: 0.032 | Tree loss: 2.306 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 029 | Total loss: 2.302 | Reg loss: 0.032 | Tree loss: 2.302 | Accuracy: 0.234375 | 0.859 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 029 | Total loss: 2.200 | Reg loss: 0.032 | Tree loss: 2.200 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 029 | Total loss: 2.380 | Reg loss: 0.031 | Tree loss: 2.380 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 029 | Total loss: 2.347 | Reg loss: 0.031 | Tree loss: 2.347 | Accuracy: 0.203125 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.218750 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 029 | Total loss: 2.317 | Reg loss: 0.031 | Tree loss: 2.317 | Accuracy: 0.244141 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 029 | Total loss: 2.372 | Reg loss: 0.031 | Tree loss: 2.372 | Accuracy: 0.177734 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 029 | Total loss: 2.360 | Reg loss: 0.031 | Tree loss: 2.360 | Accuracy: 0.164062 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 029 | Total loss: 2.307 | Reg loss: 0.031 | Tree loss: 2.307 | Accuracy: 0.214844 | 0.86 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 029 | Total loss: 2.349 | Reg loss: 0.031 | Tree loss: 2.349 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 029 | Total loss: 2.299 | Reg loss: 0.031 | Tree loss: 2.299 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 029 | Total loss: 2.331 | Reg loss: 0.031 | Tree loss: 2.331 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 029 | Total loss: 2.325 | Reg loss: 0.031 | Tree loss: 2.325 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 029 | Total loss: 2.311 | Reg loss: 0.031 | Tree loss: 2.311 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 029 | Total loss: 2.324 | Reg loss: 0.031 | Tree loss: 2.324 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 029 | Total loss: 2.351 | Reg loss: 0.031 | Tree loss: 2.351 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 029 | Total loss: 2.335 | Reg loss: 0.031 | Tree loss: 2.335 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 029 | Total loss: 2.317 | Reg loss: 0.031 | Tree loss: 2.317 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 029 | Total loss: 2.371 | Reg loss: 0.031 | Tree loss: 2.371 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 029 | Total loss: 2.301 | Reg loss: 0.031 | Tree loss: 2.301 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 029 | Total loss: 2.302 | Reg loss: 0.031 | Tree loss: 2.302 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 029 | Total loss: 2.321 | Reg loss: 0.031 | Tree loss: 2.321 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 029 | Total loss: 2.326 | Reg loss: 0.031 | Tree loss: 2.326 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 029 | Total loss: 2.274 | Reg loss: 0.031 | Tree loss: 2.274 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 029 | Total loss: 2.258 | Reg loss: 0.031 | Tree loss: 2.258 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 029 | Total loss: 2.249 | Reg loss: 0.031 | Tree loss: 2.249 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 029 | Total loss: 2.273 | Reg loss: 0.031 | Tree loss: 2.273 | Accuracy: 0.155172 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 029 | Total loss: 2.339 | Reg loss: 0.031 | Tree loss: 2.339 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 029 | Total loss: 2.375 | Reg loss: 0.031 | Tree loss: 2.375 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 029 | Total loss: 2.344 | Reg loss: 0.031 | Tree loss: 2.344 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 029 | Total loss: 2.322 | Reg loss: 0.031 | Tree loss: 2.322 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 029 | Total loss: 2.350 | Reg loss: 0.031 | Tree loss: 2.350 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 029 | Total loss: 2.352 | Reg loss: 0.031 | Tree loss: 2.352 | Accuracy: 0.207031 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 006 / 029 | Total loss: 2.350 | Reg loss: 0.031 | Tree loss: 2.350 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 029 | Total loss: 2.328 | Reg loss: 0.031 | Tree loss: 2.328 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 029 | Total loss: 2.322 | Reg loss: 0.031 | Tree loss: 2.322 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 029 | Total loss: 2.277 | Reg loss: 0.031 | Tree loss: 2.277 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 029 | Total loss: 2.339 | Reg loss: 0.031 | Tree loss: 2.339 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 029 | Total loss: 2.320 | Reg loss: 0.031 | Tree loss: 2.320 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 029 | Total loss: 2.303 | Reg loss: 0.031 | Tree loss: 2.303 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 029 | Total loss: 2.305 | Reg loss: 0.031 | Tree loss: 2.305 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 029 | Total loss: 2.275 | Reg loss: 0.031 | Tree loss: 2.275 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 029 | Total loss: 2.323 | Reg loss: 0.031 | Tree loss: 2.323 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 029 | Total loss: 2.293 | Reg loss: 0.031 | Tree loss: 2.293 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 029 | Total loss: 2.277 | Reg loss: 0.031 | Tree loss: 2.277 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 029 | Total loss: 2.286 | Reg loss: 0.031 | Tree loss: 2.286 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 029 | Total loss: 2.250 | Reg loss: 0.031 | Tree loss: 2.250 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.175781 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 029 | Total loss: 2.304 | Reg loss: 0.031 | Tree loss: 2.304 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 029 | Total loss: 2.306 | Reg loss: 0.031 | Tree loss: 2.306 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 029 | Total loss: 2.268 | Reg loss: 0.031 | Tree loss: 2.268 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 029 | Total loss: 2.491 | Reg loss: 0.031 | Tree loss: 2.491 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 029 | Total loss: 2.406 | Reg loss: 0.031 | Tree loss: 2.406 | Accuracy: 0.205078 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 029 | Total loss: 2.349 | Reg loss: 0.031 | Tree loss: 2.349 | Accuracy: 0.195312 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 029 | Total loss: 2.407 | Reg loss: 0.031 | Tree loss: 2.407 | Accuracy: 0.189453 | 0.86 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 029 | Total loss: 2.362 | Reg loss: 0.031 | Tree loss: 2.362 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 029 | Total loss: 2.327 | Reg loss: 0.031 | Tree loss: 2.327 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 029 | Total loss: 2.284 | Reg loss: 0.031 | Tree loss: 2.284 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 029 | Total loss: 2.341 | Reg loss: 0.031 | Tree loss: 2.341 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 029 | Total loss: 2.362 | Reg loss: 0.031 | Tree loss: 2.362 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 029 | Total loss: 2.308 | Reg loss: 0.031 | Tree loss: 2.308 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 029 | Total loss: 2.331 | Reg loss: 0.031 | Tree loss: 2.331 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 029 | Total loss: 2.308 | Reg loss: 0.031 | Tree loss: 2.308 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 029 | Total loss: 2.317 | Reg loss: 0.031 | Tree loss: 2.317 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 029 | Total loss: 2.302 | Reg loss: 0.031 | Tree loss: 2.302 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.242188 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 029 | Total loss: 2.286 | Reg loss: 0.031 | Tree loss: 2.286 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 029 | Total loss: 2.294 | Reg loss: 0.031 | Tree loss: 2.294 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 029 | Total loss: 2.255 | Reg loss: 0.031 | Tree loss: 2.255 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 029 | Total loss: 2.266 | Reg loss: 0.031 | Tree loss: 2.266 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 029 | Total loss: 2.265 | Reg loss: 0.031 | Tree loss: 2.265 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 029 | Total loss: 2.227 | Reg loss: 0.031 | Tree loss: 2.227 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 029 | Total loss: 2.275 | Reg loss: 0.031 | Tree loss: 2.275 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 029 | Total loss: 2.278 | Reg loss: 0.031 | Tree loss: 2.278 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 029 | Total loss: 2.251 | Reg loss: 0.031 | Tree loss: 2.251 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 029 | Total loss: 2.295 | Reg loss: 0.031 | Tree loss: 2.295 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 025 / 029 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.228516 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 029 | Total loss: 2.225 | Reg loss: 0.031 | Tree loss: 2.225 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 029 | Total loss: 2.045 | Reg loss: 0.031 | Tree loss: 2.045 | Accuracy: 0.344828 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 029 | Total loss: 2.306 | Reg loss: 0.031 | Tree loss: 2.306 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 029 | Total loss: 2.321 | Reg loss: 0.031 | Tree loss: 2.321 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 029 | Total loss: 2.337 | Reg loss: 0.031 | Tree loss: 2.337 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 029 | Total loss: 2.310 | Reg loss: 0.031 | Tree loss: 2.310 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 029 | Total loss: 2.284 | Reg loss: 0.031 | Tree loss: 2.284 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 029 | Total loss: 2.328 | Reg loss: 0.031 | Tree loss: 2.328 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 029 | Total loss: 2.315 | Reg loss: 0.031 | Tree loss: 2.315 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 029 | Total loss: 2.347 | Reg loss: 0.031 | Tree loss: 2.347 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 029 | Total loss: 2.317 | Reg loss: 0.031 | Tree loss: 2.317 | Accuracy: 0.208984 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 010 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 029 | Total loss: 2.289 | Reg loss: 0.031 | Tree loss: 2.289 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 029 | Total loss: 2.303 | Reg loss: 0.031 | Tree loss: 2.303 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 029 | Total loss: 2.271 | Reg loss: 0.031 | Tree loss: 2.271 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 029 | Total loss: 2.278 | Reg loss: 0.031 | Tree loss: 2.278 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 029 | Total loss: 2.317 | Reg loss: 0.031 | Tree loss: 2.317 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 029 | Total loss: 2.338 | Reg loss: 0.031 | Tree loss: 2.338 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 029 | Total loss: 2.248 | Reg loss: 0.031 | Tree loss: 2.248 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 029 | Total loss: 2.263 | Reg loss: 0.031 | Tree loss: 2.263 | Accuracy: 0.251953 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 029 | Total loss: 2.333 | Reg loss: 0.031 | Tree loss: 2.333 | Accuracy: 0.166016 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 029 | Total loss: 2.223 | Reg loss: 0.031 | Tree loss: 2.223 | Accuracy: 0.234375 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 029 | Total loss: 2.272 | Reg loss: 0.031 | Tree loss: 2.272 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 029 | Total loss: 2.315 | Reg loss: 0.031 | Tree loss: 2.315 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 029 | Total loss: 2.247 | Reg loss: 0.031 | Tree loss: 2.247 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 029 | Total loss: 2.278 | Reg loss: 0.031 | Tree loss: 2.278 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 029 | Total loss: 2.137 | Reg loss: 0.031 | Tree loss: 2.137 | Accuracy: 0.189655 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 029 | Total loss: 2.270 | Reg loss: 0.031 | Tree loss: 2.270 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 029 | Total loss: 2.352 | Reg loss: 0.031 | Tree loss: 2.352 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 029 | Total loss: 2.328 | Reg loss: 0.031 | Tree loss: 2.328 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 029 | Total loss: 2.313 | Reg loss: 0.031 | Tree loss: 2.313 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 029 | Total loss: 2.356 | Reg loss: 0.031 | Tree loss: 2.356 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 029 | Total loss: 2.321 | Reg loss: 0.031 | Tree loss: 2.321 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 029 | Total loss: 2.323 | Reg loss: 0.031 | Tree loss: 2.323 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 029 | Total loss: 2.262 | Reg loss: 0.031 | Tree loss: 2.262 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 029 | Total loss: 2.313 | Reg loss: 0.031 | Tree loss: 2.313 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 029 | Total loss: 2.311 | Reg loss: 0.031 | Tree loss: 2.311 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 029 | Total loss: 2.263 | Reg loss: 0.031 | Tree loss: 2.263 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 029 | Total loss: 2.268 | Reg loss: 0.031 | Tree loss: 2.268 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 029 | Total loss: 2.275 | Reg loss: 0.031 | Tree loss: 2.275 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 029 | Total loss: 2.276 | Reg loss: 0.031 | Tree loss: 2.276 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 029 | Total loss: 2.311 | Reg loss: 0.031 | Tree loss: 2.311 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 029 | Total loss: 2.319 | Reg loss: 0.031 | Tree loss: 2.319 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 029 | Total loss: 2.293 | Reg loss: 0.031 | Tree loss: 2.293 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 029 | Total loss: 2.224 | Reg loss: 0.031 | Tree loss: 2.224 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 029 | Total loss: 2.248 | Reg loss: 0.031 | Tree loss: 2.248 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 029 | Total loss: 2.270 | Reg loss: 0.031 | Tree loss: 2.270 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 029 | Total loss: 2.296 | Reg loss: 0.031 | Tree loss: 2.296 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 029 | Total loss: 2.280 | Reg loss: 0.031 | Tree loss: 2.280 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.154297 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 026 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 029 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 029 | Total loss: 2.314 | Reg loss: 0.031 | Tree loss: 2.314 | Accuracy: 0.120690 | 0.859 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 029 | Total loss: 2.333 | Reg loss: 0.031 | Tree loss: 2.333 | Accuracy: 0.164062 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 029 | Total loss: 2.299 | Reg loss: 0.031 | Tree loss: 2.299 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.230469 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 029 | Total loss: 2.337 | Reg loss: 0.031 | Tree loss: 2.337 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 029 | Total loss: 2.277 | Reg loss: 0.031 | Tree loss: 2.277 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 029 | Total loss: 2.338 | Reg loss: 0.031 | Tree loss: 2.338 | Accuracy: 0.169922 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 029 | Total loss: 2.281 | Reg loss: 0.031 | Tree loss: 2.281 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 029 | Total loss: 2.309 | Reg loss: 0.031 | Tree loss: 2.309 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 029 | Total loss: 2.310 | Reg loss: 0.031 | Tree loss: 2.310 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 029 | Total loss: 2.325 | Reg loss: 0.031 | Tree loss: 2.325 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 029 | Total loss: 2.316 | Reg loss: 0.031 | Tree loss: 2.316 | Accuracy: 0.189453 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 | Batch: 014 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.248047 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 029 | Total loss: 2.244 | Reg loss: 0.031 | Tree loss: 2.244 | Accuracy: 0.226562 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 029 | Total loss: 2.269 | Reg loss: 0.031 | Tree loss: 2.269 | Accuracy: 0.173828 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 029 | Total loss: 2.258 | Reg loss: 0.031 | Tree loss: 2.258 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 029 | Total loss: 2.251 | Reg loss: 0.031 | Tree loss: 2.251 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 029 | Total loss: 2.252 | Reg loss: 0.031 | Tree loss: 2.252 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.228516 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.232422 | 0.859 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 029 | Total loss: 2.255 | Reg loss: 0.031 | Tree loss: 2.255 | Accuracy: 0.189453 | 0.858 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 029 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.195312 | 0.858 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 029 | Total loss: 2.199 | Reg loss: 0.031 | Tree loss: 2.199 | Accuracy: 0.258621 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 029 | Total loss: 2.318 | Reg loss: 0.031 | Tree loss: 2.318 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 029 | Total loss: 2.315 | Reg loss: 0.031 | Tree loss: 2.315 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 029 | Total loss: 2.308 | Reg loss: 0.031 | Tree loss: 2.308 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 029 | Total loss: 2.294 | Reg loss: 0.031 | Tree loss: 2.294 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 029 | Total loss: 2.340 | Reg loss: 0.031 | Tree loss: 2.340 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 029 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 029 | Total loss: 2.240 | Reg loss: 0.031 | Tree loss: 2.240 | Accuracy: 0.248047 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 029 | Total loss: 2.296 | Reg loss: 0.031 | Tree loss: 2.296 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 029 | Total loss: 2.293 | Reg loss: 0.031 | Tree loss: 2.293 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 029 | Total loss: 2.237 | Reg loss: 0.031 | Tree loss: 2.237 | Accuracy: 0.224609 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 029 | Total loss: 2.301 | Reg loss: 0.031 | Tree loss: 2.301 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 029 | Total loss: 2.281 | Reg loss: 0.031 | Tree loss: 2.281 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 029 | Total loss: 2.281 | Reg loss: 0.031 | Tree loss: 2.281 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 029 | Total loss: 2.254 | Reg loss: 0.031 | Tree loss: 2.254 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 029 | Total loss: 2.241 | Reg loss: 0.031 | Tree loss: 2.241 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 029 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 029 | Total loss: 2.252 | Reg loss: 0.031 | Tree loss: 2.252 | Accuracy: 0.214844 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 029 | Total loss: 2.282 | Reg loss: 0.031 | Tree loss: 2.282 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 029 | Total loss: 2.240 | Reg loss: 0.031 | Tree loss: 2.240 | Accuracy: 0.179688 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 029 | Total loss: 2.238 | Reg loss: 0.031 | Tree loss: 2.238 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 029 | Total loss: 2.255 | Reg loss: 0.031 | Tree loss: 2.255 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 029 | Total loss: 2.248 | Reg loss: 0.031 | Tree loss: 2.248 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 029 | Total loss: 2.237 | Reg loss: 0.031 | Tree loss: 2.237 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 029 | Total loss: 2.218 | Reg loss: 0.031 | Tree loss: 2.218 | Accuracy: 0.257812 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 027 / 029 | Total loss: 2.272 | Reg loss: 0.031 | Tree loss: 2.272 | Accuracy: 0.187500 | 0.859 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.189655 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 029 | Total loss: 2.330 | Reg loss: 0.031 | Tree loss: 2.330 | Accuracy: 0.173828 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 029 | Total loss: 2.273 | Reg loss: 0.031 | Tree loss: 2.273 | Accuracy: 0.273438 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 029 | Total loss: 2.312 | Reg loss: 0.031 | Tree loss: 2.312 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 029 | Total loss: 2.286 | Reg loss: 0.031 | Tree loss: 2.286 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.199219 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 029 | Total loss: 2.302 | Reg loss: 0.031 | Tree loss: 2.302 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.216797 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 029 | Total loss: 2.252 | Reg loss: 0.031 | Tree loss: 2.252 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 029 | Total loss: 2.281 | Reg loss: 0.031 | Tree loss: 2.281 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 029 | Total loss: 2.233 | Reg loss: 0.031 | Tree loss: 2.233 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 029 | Total loss: 2.300 | Reg loss: 0.031 | Tree loss: 2.300 | Accuracy: 0.226562 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 029 | Total loss: 2.275 | Reg loss: 0.031 | Tree loss: 2.275 | Accuracy: 0.208984 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 029 | Total loss: 2.285 | Reg loss: 0.031 | Tree loss: 2.285 | Accuracy: 0.218750 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 029 | Total loss: 2.266 | Reg loss: 0.031 | Tree loss: 2.266 | Accuracy: 0.203125 | 0.859 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 018 / 029 | Total loss: 2.216 | Reg loss: 0.031 | Tree loss: 2.216 | Accuracy: 0.189453 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 029 | Total loss: 2.312 | Reg loss: 0.031 | Tree loss: 2.312 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 029 | Total loss: 2.262 | Reg loss: 0.031 | Tree loss: 2.262 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 029 | Total loss: 2.319 | Reg loss: 0.031 | Tree loss: 2.319 | Accuracy: 0.162109 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 029 | Total loss: 2.218 | Reg loss: 0.031 | Tree loss: 2.218 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.197266 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.181641 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 029 | Total loss: 2.198 | Reg loss: 0.031 | Tree loss: 2.198 | Accuracy: 0.201172 | 0.859 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 029 | Total loss: 2.213 | Reg loss: 0.031 | Tree loss: 2.213 | Accuracy: 0.193359 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 029 | Total loss: 2.251 | Reg loss: 0.031 | Tree loss: 2.251 | Accuracy: 0.195312 | 0.858 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 029 | Total loss: 2.149 | Reg loss: 0.031 | Tree loss: 2.149 | Accuracy: 0.206897 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 029 | Total loss: 2.310 | Reg loss: 0.031 | Tree loss: 2.310 | Accuracy: 0.203125 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 029 | Total loss: 2.302 | Reg loss: 0.031 | Tree loss: 2.302 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 029 | Total loss: 2.322 | Reg loss: 0.031 | Tree loss: 2.322 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 029 | Total loss: 2.294 | Reg loss: 0.031 | Tree loss: 2.294 | Accuracy: 0.193359 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 029 | Total loss: 2.296 | Reg loss: 0.031 | Tree loss: 2.296 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 029 | Total loss: 2.240 | Reg loss: 0.031 | Tree loss: 2.240 | Accuracy: 0.228516 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.195312 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 029 | Total loss: 2.296 | Reg loss: 0.031 | Tree loss: 2.296 | Accuracy: 0.185547 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 029 | Total loss: 2.299 | Reg loss: 0.031 | Tree loss: 2.299 | Accuracy: 0.177734 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 029 | Total loss: 2.273 | Reg loss: 0.031 | Tree loss: 2.273 | Accuracy: 0.183594 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 029 | Total loss: 2.287 | Reg loss: 0.031 | Tree loss: 2.287 | Accuracy: 0.191406 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 029 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.205078 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 029 | Total loss: 2.248 | Reg loss: 0.031 | Tree loss: 2.248 | Accuracy: 0.222656 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.212891 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 029 | Total loss: 2.242 | Reg loss: 0.031 | Tree loss: 2.242 | Accuracy: 0.210938 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 029 | Total loss: 2.216 | Reg loss: 0.031 | Tree loss: 2.216 | Accuracy: 0.207031 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 029 | Total loss: 2.189 | Reg loss: 0.031 | Tree loss: 2.189 | Accuracy: 0.220703 | 0.859 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.177734 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 029 | Total loss: 2.315 | Reg loss: 0.031 | Tree loss: 2.315 | Accuracy: 0.179688 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 029 | Total loss: 2.255 | Reg loss: 0.031 | Tree loss: 2.255 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.224609 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 029 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.210938 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 029 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.244141 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 029 | Total loss: 2.271 | Reg loss: 0.031 | Tree loss: 2.271 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 029 | Total loss: 2.204 | Reg loss: 0.031 | Tree loss: 2.204 | Accuracy: 0.212891 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 029 | Total loss: 2.237 | Reg loss: 0.031 | Tree loss: 2.237 | Accuracy: 0.201172 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 029 | Total loss: 2.239 | Reg loss: 0.031 | Tree loss: 2.239 | Accuracy: 0.240234 | 0.858 sec/iter\n",
      "Epoch: 60 | Batch: 028 / 029 | Total loss: 2.191 | Reg loss: 0.031 | Tree loss: 2.191 | Accuracy: 0.155172 | 0.858 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 029 | Total loss: 2.340 | Reg loss: 0.031 | Tree loss: 2.340 | Accuracy: 0.177734 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 029 | Total loss: 2.296 | Reg loss: 0.031 | Tree loss: 2.296 | Accuracy: 0.187500 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.232422 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 029 | Total loss: 2.293 | Reg loss: 0.031 | Tree loss: 2.293 | Accuracy: 0.185547 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.222656 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 029 | Total loss: 2.283 | Reg loss: 0.031 | Tree loss: 2.283 | Accuracy: 0.205078 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 029 | Total loss: 2.300 | Reg loss: 0.031 | Tree loss: 2.300 | Accuracy: 0.167969 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 029 | Total loss: 2.237 | Reg loss: 0.031 | Tree loss: 2.237 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 029 | Total loss: 2.222 | Reg loss: 0.031 | Tree loss: 2.222 | Accuracy: 0.214844 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 029 | Total loss: 2.289 | Reg loss: 0.031 | Tree loss: 2.289 | Accuracy: 0.203125 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 029 | Total loss: 2.270 | Reg loss: 0.031 | Tree loss: 2.270 | Accuracy: 0.195312 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 029 | Total loss: 2.254 | Reg loss: 0.031 | Tree loss: 2.254 | Accuracy: 0.218750 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.232422 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 029 | Total loss: 2.236 | Reg loss: 0.031 | Tree loss: 2.236 | Accuracy: 0.195312 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 029 | Total loss: 2.257 | Reg loss: 0.031 | Tree loss: 2.257 | Accuracy: 0.230469 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 029 | Total loss: 2.229 | Reg loss: 0.031 | Tree loss: 2.229 | Accuracy: 0.216797 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 029 | Total loss: 2.261 | Reg loss: 0.031 | Tree loss: 2.261 | Accuracy: 0.228516 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 029 | Total loss: 2.254 | Reg loss: 0.031 | Tree loss: 2.254 | Accuracy: 0.199219 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 029 | Total loss: 2.204 | Reg loss: 0.031 | Tree loss: 2.204 | Accuracy: 0.220703 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 029 | Total loss: 2.245 | Reg loss: 0.031 | Tree loss: 2.245 | Accuracy: 0.201172 | 0.858 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 029 | Total loss: 2.197 | Reg loss: 0.031 | Tree loss: 2.197 | Accuracy: 0.220703 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.181641 | 0.857 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 022 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.199219 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 029 | Total loss: 2.214 | Reg loss: 0.031 | Tree loss: 2.214 | Accuracy: 0.195312 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.193359 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 029 | Total loss: 2.238 | Reg loss: 0.031 | Tree loss: 2.238 | Accuracy: 0.193359 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.201172 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.222656 | 0.857 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 029 | Total loss: 2.152 | Reg loss: 0.031 | Tree loss: 2.152 | Accuracy: 0.189655 | 0.857 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 029 | Total loss: 2.305 | Reg loss: 0.031 | Tree loss: 2.305 | Accuracy: 0.187500 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 029 | Total loss: 2.250 | Reg loss: 0.031 | Tree loss: 2.250 | Accuracy: 0.205078 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 029 | Total loss: 2.257 | Reg loss: 0.031 | Tree loss: 2.257 | Accuracy: 0.197266 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 029 | Total loss: 2.268 | Reg loss: 0.031 | Tree loss: 2.268 | Accuracy: 0.183594 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 029 | Total loss: 2.286 | Reg loss: 0.031 | Tree loss: 2.286 | Accuracy: 0.207031 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 029 | Total loss: 2.279 | Reg loss: 0.031 | Tree loss: 2.279 | Accuracy: 0.224609 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 029 | Total loss: 2.293 | Reg loss: 0.031 | Tree loss: 2.293 | Accuracy: 0.201172 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 029 | Total loss: 2.301 | Reg loss: 0.031 | Tree loss: 2.301 | Accuracy: 0.185547 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.224609 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 029 | Total loss: 2.236 | Reg loss: 0.031 | Tree loss: 2.236 | Accuracy: 0.222656 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 029 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.214844 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.179688 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.277344 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 029 | Total loss: 2.277 | Reg loss: 0.031 | Tree loss: 2.277 | Accuracy: 0.166016 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 029 | Total loss: 2.250 | Reg loss: 0.031 | Tree loss: 2.250 | Accuracy: 0.193359 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 029 | Total loss: 2.253 | Reg loss: 0.031 | Tree loss: 2.253 | Accuracy: 0.199219 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.166016 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 029 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.216797 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 029 | Total loss: 2.232 | Reg loss: 0.031 | Tree loss: 2.232 | Accuracy: 0.253906 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 029 | Total loss: 2.226 | Reg loss: 0.031 | Tree loss: 2.226 | Accuracy: 0.210938 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 029 | Total loss: 2.239 | Reg loss: 0.031 | Tree loss: 2.239 | Accuracy: 0.195312 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.195312 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 029 | Total loss: 2.271 | Reg loss: 0.031 | Tree loss: 2.271 | Accuracy: 0.199219 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 029 | Total loss: 2.257 | Reg loss: 0.031 | Tree loss: 2.257 | Accuracy: 0.177734 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.214844 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 029 | Total loss: 2.197 | Reg loss: 0.031 | Tree loss: 2.197 | Accuracy: 0.207031 | 0.857 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.234375 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 029 | Total loss: 2.242 | Reg loss: 0.031 | Tree loss: 2.242 | Accuracy: 0.205078 | 0.856 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.155172 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 029 | Total loss: 2.313 | Reg loss: 0.031 | Tree loss: 2.313 | Accuracy: 0.216797 | 0.857 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 029 | Total loss: 2.257 | Reg loss: 0.031 | Tree loss: 2.257 | Accuracy: 0.189453 | 0.857 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 029 | Total loss: 2.272 | Reg loss: 0.031 | Tree loss: 2.272 | Accuracy: 0.220703 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 029 | Total loss: 2.327 | Reg loss: 0.031 | Tree loss: 2.327 | Accuracy: 0.203125 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 029 | Total loss: 2.269 | Reg loss: 0.031 | Tree loss: 2.269 | Accuracy: 0.216797 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 029 | Total loss: 2.288 | Reg loss: 0.031 | Tree loss: 2.288 | Accuracy: 0.191406 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 029 | Total loss: 2.281 | Reg loss: 0.031 | Tree loss: 2.281 | Accuracy: 0.195312 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 029 | Total loss: 2.289 | Reg loss: 0.031 | Tree loss: 2.289 | Accuracy: 0.169922 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 029 | Total loss: 2.227 | Reg loss: 0.031 | Tree loss: 2.227 | Accuracy: 0.224609 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 029 | Total loss: 2.280 | Reg loss: 0.031 | Tree loss: 2.280 | Accuracy: 0.205078 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 029 | Total loss: 2.246 | Reg loss: 0.031 | Tree loss: 2.246 | Accuracy: 0.203125 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 029 | Total loss: 2.300 | Reg loss: 0.031 | Tree loss: 2.300 | Accuracy: 0.208984 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 029 | Total loss: 2.214 | Reg loss: 0.031 | Tree loss: 2.214 | Accuracy: 0.205078 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 029 | Total loss: 2.254 | Reg loss: 0.031 | Tree loss: 2.254 | Accuracy: 0.203125 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 029 | Total loss: 2.258 | Reg loss: 0.031 | Tree loss: 2.258 | Accuracy: 0.199219 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 029 | Total loss: 2.280 | Reg loss: 0.031 | Tree loss: 2.280 | Accuracy: 0.181641 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 029 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.199219 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 029 | Total loss: 2.211 | Reg loss: 0.031 | Tree loss: 2.211 | Accuracy: 0.251953 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.222656 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 029 | Total loss: 2.221 | Reg loss: 0.031 | Tree loss: 2.221 | Accuracy: 0.230469 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.191406 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 029 | Total loss: 2.152 | Reg loss: 0.031 | Tree loss: 2.152 | Accuracy: 0.187500 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 029 | Total loss: 2.201 | Reg loss: 0.031 | Tree loss: 2.201 | Accuracy: 0.220703 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.205078 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 029 | Total loss: 2.223 | Reg loss: 0.031 | Tree loss: 2.223 | Accuracy: 0.183594 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 029 | Total loss: 2.213 | Reg loss: 0.031 | Tree loss: 2.213 | Accuracy: 0.207031 | 0.856 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 026 / 029 | Total loss: 2.211 | Reg loss: 0.031 | Tree loss: 2.211 | Accuracy: 0.201172 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 029 | Total loss: 2.178 | Reg loss: 0.031 | Tree loss: 2.178 | Accuracy: 0.203125 | 0.856 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 029 | Total loss: 2.156 | Reg loss: 0.031 | Tree loss: 2.156 | Accuracy: 0.241379 | 0.856 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.208984 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 029 | Total loss: 2.300 | Reg loss: 0.031 | Tree loss: 2.300 | Accuracy: 0.244141 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 029 | Total loss: 2.255 | Reg loss: 0.031 | Tree loss: 2.255 | Accuracy: 0.240234 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 029 | Total loss: 2.319 | Reg loss: 0.031 | Tree loss: 2.319 | Accuracy: 0.197266 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 029 | Total loss: 2.305 | Reg loss: 0.031 | Tree loss: 2.305 | Accuracy: 0.173828 | 0.856 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.224609 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 029 | Total loss: 2.267 | Reg loss: 0.031 | Tree loss: 2.267 | Accuracy: 0.197266 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.226562 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 029 | Total loss: 2.222 | Reg loss: 0.031 | Tree loss: 2.222 | Accuracy: 0.199219 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 029 | Total loss: 2.223 | Reg loss: 0.031 | Tree loss: 2.223 | Accuracy: 0.218750 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 029 | Total loss: 2.250 | Reg loss: 0.031 | Tree loss: 2.250 | Accuracy: 0.181641 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 029 | Total loss: 2.244 | Reg loss: 0.031 | Tree loss: 2.244 | Accuracy: 0.179688 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.222656 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 029 | Total loss: 2.246 | Reg loss: 0.031 | Tree loss: 2.246 | Accuracy: 0.214844 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.173828 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 029 | Total loss: 2.200 | Reg loss: 0.031 | Tree loss: 2.200 | Accuracy: 0.199219 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.203125 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 029 | Total loss: 2.239 | Reg loss: 0.031 | Tree loss: 2.239 | Accuracy: 0.175781 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 029 | Total loss: 2.201 | Reg loss: 0.031 | Tree loss: 2.201 | Accuracy: 0.207031 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.216797 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 029 | Total loss: 2.239 | Reg loss: 0.031 | Tree loss: 2.239 | Accuracy: 0.205078 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 029 | Total loss: 2.224 | Reg loss: 0.031 | Tree loss: 2.224 | Accuracy: 0.203125 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 029 | Total loss: 2.238 | Reg loss: 0.031 | Tree loss: 2.238 | Accuracy: 0.201172 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 029 | Total loss: 2.210 | Reg loss: 0.031 | Tree loss: 2.210 | Accuracy: 0.183594 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.210938 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 029 | Total loss: 2.213 | Reg loss: 0.031 | Tree loss: 2.213 | Accuracy: 0.197266 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 029 | Total loss: 2.160 | Reg loss: 0.031 | Tree loss: 2.160 | Accuracy: 0.218750 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 029 | Total loss: 2.188 | Reg loss: 0.031 | Tree loss: 2.188 | Accuracy: 0.197266 | 0.855 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 029 | Total loss: 2.172 | Reg loss: 0.031 | Tree loss: 2.172 | Accuracy: 0.189655 | 0.855 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 029 | Total loss: 2.254 | Reg loss: 0.031 | Tree loss: 2.254 | Accuracy: 0.230469 | 0.855 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.220703 | 0.855 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 029 | Total loss: 2.275 | Reg loss: 0.031 | Tree loss: 2.275 | Accuracy: 0.244141 | 0.855 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.228516 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 029 | Total loss: 2.273 | Reg loss: 0.031 | Tree loss: 2.273 | Accuracy: 0.179688 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.203125 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 029 | Total loss: 2.258 | Reg loss: 0.031 | Tree loss: 2.258 | Accuracy: 0.226562 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 029 | Total loss: 2.279 | Reg loss: 0.031 | Tree loss: 2.279 | Accuracy: 0.162109 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 029 | Total loss: 2.262 | Reg loss: 0.031 | Tree loss: 2.262 | Accuracy: 0.197266 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 029 | Total loss: 2.273 | Reg loss: 0.031 | Tree loss: 2.273 | Accuracy: 0.173828 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 029 | Total loss: 2.233 | Reg loss: 0.031 | Tree loss: 2.233 | Accuracy: 0.193359 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.187500 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.238281 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 029 | Total loss: 2.260 | Reg loss: 0.031 | Tree loss: 2.260 | Accuracy: 0.195312 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 029 | Total loss: 2.292 | Reg loss: 0.031 | Tree loss: 2.292 | Accuracy: 0.203125 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 029 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.218750 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 029 | Total loss: 2.241 | Reg loss: 0.031 | Tree loss: 2.241 | Accuracy: 0.197266 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 029 | Total loss: 2.272 | Reg loss: 0.031 | Tree loss: 2.272 | Accuracy: 0.173828 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 029 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.197266 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.220703 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 029 | Total loss: 2.195 | Reg loss: 0.031 | Tree loss: 2.195 | Accuracy: 0.207031 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.189453 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 029 | Total loss: 2.195 | Reg loss: 0.031 | Tree loss: 2.195 | Accuracy: 0.191406 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.218750 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 029 | Total loss: 2.159 | Reg loss: 0.031 | Tree loss: 2.159 | Accuracy: 0.210938 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 029 | Total loss: 2.204 | Reg loss: 0.031 | Tree loss: 2.204 | Accuracy: 0.218750 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 029 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.195312 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.210938 | 0.854 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 029 | Total loss: 2.233 | Reg loss: 0.031 | Tree loss: 2.233 | Accuracy: 0.172414 | 0.854 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 000 / 029 | Total loss: 2.328 | Reg loss: 0.031 | Tree loss: 2.328 | Accuracy: 0.210938 | 0.854 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 029 | Total loss: 2.264 | Reg loss: 0.031 | Tree loss: 2.264 | Accuracy: 0.216797 | 0.854 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 029 | Total loss: 2.319 | Reg loss: 0.031 | Tree loss: 2.319 | Accuracy: 0.185547 | 0.854 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 029 | Total loss: 2.302 | Reg loss: 0.031 | Tree loss: 2.302 | Accuracy: 0.189453 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 029 | Total loss: 2.259 | Reg loss: 0.031 | Tree loss: 2.259 | Accuracy: 0.218750 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 029 | Total loss: 2.246 | Reg loss: 0.031 | Tree loss: 2.246 | Accuracy: 0.207031 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 029 | Total loss: 2.265 | Reg loss: 0.031 | Tree loss: 2.265 | Accuracy: 0.205078 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 029 | Total loss: 2.274 | Reg loss: 0.031 | Tree loss: 2.274 | Accuracy: 0.205078 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 029 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.173828 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.207031 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 029 | Total loss: 2.178 | Reg loss: 0.031 | Tree loss: 2.178 | Accuracy: 0.259766 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 029 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.201172 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 029 | Total loss: 2.242 | Reg loss: 0.031 | Tree loss: 2.242 | Accuracy: 0.191406 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 029 | Total loss: 2.219 | Reg loss: 0.031 | Tree loss: 2.219 | Accuracy: 0.218750 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 029 | Total loss: 2.276 | Reg loss: 0.031 | Tree loss: 2.276 | Accuracy: 0.197266 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 029 | Total loss: 2.243 | Reg loss: 0.031 | Tree loss: 2.243 | Accuracy: 0.193359 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 029 | Total loss: 2.189 | Reg loss: 0.031 | Tree loss: 2.189 | Accuracy: 0.210938 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 029 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.167969 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 029 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.222656 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 029 | Total loss: 2.229 | Reg loss: 0.031 | Tree loss: 2.229 | Accuracy: 0.203125 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 029 | Total loss: 2.224 | Reg loss: 0.031 | Tree loss: 2.224 | Accuracy: 0.201172 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.220703 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 029 | Total loss: 2.171 | Reg loss: 0.031 | Tree loss: 2.171 | Accuracy: 0.195312 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 029 | Total loss: 2.198 | Reg loss: 0.031 | Tree loss: 2.198 | Accuracy: 0.214844 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.197266 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 029 | Total loss: 2.200 | Reg loss: 0.031 | Tree loss: 2.200 | Accuracy: 0.222656 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 029 | Total loss: 2.171 | Reg loss: 0.031 | Tree loss: 2.171 | Accuracy: 0.205078 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.187500 | 0.853 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 029 | Total loss: 2.189 | Reg loss: 0.031 | Tree loss: 2.189 | Accuracy: 0.258621 | 0.852 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 029 | Total loss: 2.282 | Reg loss: 0.030 | Tree loss: 2.282 | Accuracy: 0.208984 | 0.853 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 029 | Total loss: 2.268 | Reg loss: 0.030 | Tree loss: 2.268 | Accuracy: 0.173828 | 0.853 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 029 | Total loss: 2.304 | Reg loss: 0.030 | Tree loss: 2.304 | Accuracy: 0.187500 | 0.853 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 029 | Total loss: 2.259 | Reg loss: 0.030 | Tree loss: 2.259 | Accuracy: 0.216797 | 0.853 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 029 | Total loss: 2.280 | Reg loss: 0.030 | Tree loss: 2.280 | Accuracy: 0.199219 | 0.853 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.205078 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 029 | Total loss: 2.248 | Reg loss: 0.030 | Tree loss: 2.248 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 029 | Total loss: 2.245 | Reg loss: 0.030 | Tree loss: 2.245 | Accuracy: 0.181641 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 029 | Total loss: 2.274 | Reg loss: 0.031 | Tree loss: 2.274 | Accuracy: 0.187500 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 029 | Total loss: 2.262 | Reg loss: 0.031 | Tree loss: 2.262 | Accuracy: 0.212891 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 029 | Total loss: 2.227 | Reg loss: 0.031 | Tree loss: 2.227 | Accuracy: 0.222656 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.203125 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 029 | Total loss: 2.232 | Reg loss: 0.031 | Tree loss: 2.232 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.185547 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 029 | Total loss: 2.221 | Reg loss: 0.031 | Tree loss: 2.221 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 029 | Total loss: 2.291 | Reg loss: 0.031 | Tree loss: 2.291 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 029 | Total loss: 2.209 | Reg loss: 0.031 | Tree loss: 2.209 | Accuracy: 0.210938 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 029 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.203125 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.216797 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.220703 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 029 | Total loss: 2.219 | Reg loss: 0.031 | Tree loss: 2.219 | Accuracy: 0.212891 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 029 | Total loss: 2.217 | Reg loss: 0.031 | Tree loss: 2.217 | Accuracy: 0.201172 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 029 | Total loss: 2.214 | Reg loss: 0.031 | Tree loss: 2.214 | Accuracy: 0.210938 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 029 | Total loss: 2.191 | Reg loss: 0.031 | Tree loss: 2.191 | Accuracy: 0.187500 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 029 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 029 | Total loss: 2.157 | Reg loss: 0.031 | Tree loss: 2.157 | Accuracy: 0.234375 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 029 | Total loss: 2.195 | Reg loss: 0.031 | Tree loss: 2.195 | Accuracy: 0.207031 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 029 | Total loss: 2.194 | Reg loss: 0.031 | Tree loss: 2.194 | Accuracy: 0.212891 | 0.852 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 029 | Total loss: 2.122 | Reg loss: 0.031 | Tree loss: 2.122 | Accuracy: 0.224138 | 0.852 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 029 | Total loss: 2.270 | Reg loss: 0.030 | Tree loss: 2.270 | Accuracy: 0.220703 | 0.852 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 029 | Total loss: 2.281 | Reg loss: 0.030 | Tree loss: 2.281 | Accuracy: 0.226562 | 0.852 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 029 | Total loss: 2.291 | Reg loss: 0.030 | Tree loss: 2.291 | Accuracy: 0.193359 | 0.852 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 029 | Total loss: 2.280 | Reg loss: 0.030 | Tree loss: 2.280 | Accuracy: 0.210938 | 0.852 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 004 / 029 | Total loss: 2.268 | Reg loss: 0.030 | Tree loss: 2.268 | Accuracy: 0.205078 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 029 | Total loss: 2.215 | Reg loss: 0.030 | Tree loss: 2.215 | Accuracy: 0.232422 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 029 | Total loss: 2.242 | Reg loss: 0.030 | Tree loss: 2.242 | Accuracy: 0.187500 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 029 | Total loss: 2.267 | Reg loss: 0.030 | Tree loss: 2.267 | Accuracy: 0.185547 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.203125 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 029 | Total loss: 2.214 | Reg loss: 0.030 | Tree loss: 2.214 | Accuracy: 0.224609 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 029 | Total loss: 2.222 | Reg loss: 0.031 | Tree loss: 2.222 | Accuracy: 0.208984 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 029 | Total loss: 2.241 | Reg loss: 0.031 | Tree loss: 2.241 | Accuracy: 0.214844 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 029 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.222656 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 029 | Total loss: 2.213 | Reg loss: 0.031 | Tree loss: 2.213 | Accuracy: 0.214844 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 029 | Total loss: 2.220 | Reg loss: 0.031 | Tree loss: 2.220 | Accuracy: 0.203125 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 029 | Total loss: 2.252 | Reg loss: 0.031 | Tree loss: 2.252 | Accuracy: 0.191406 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 029 | Total loss: 2.203 | Reg loss: 0.031 | Tree loss: 2.203 | Accuracy: 0.173828 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 029 | Total loss: 2.200 | Reg loss: 0.031 | Tree loss: 2.200 | Accuracy: 0.222656 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 029 | Total loss: 2.208 | Reg loss: 0.031 | Tree loss: 2.208 | Accuracy: 0.193359 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 029 | Total loss: 2.179 | Reg loss: 0.031 | Tree loss: 2.179 | Accuracy: 0.203125 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 029 | Total loss: 2.219 | Reg loss: 0.031 | Tree loss: 2.219 | Accuracy: 0.220703 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.199219 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 029 | Total loss: 2.205 | Reg loss: 0.031 | Tree loss: 2.205 | Accuracy: 0.191406 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 029 | Total loss: 2.236 | Reg loss: 0.031 | Tree loss: 2.236 | Accuracy: 0.236328 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 029 | Total loss: 2.238 | Reg loss: 0.031 | Tree loss: 2.238 | Accuracy: 0.187500 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 029 | Total loss: 2.212 | Reg loss: 0.031 | Tree loss: 2.212 | Accuracy: 0.191406 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 029 | Total loss: 2.175 | Reg loss: 0.031 | Tree loss: 2.175 | Accuracy: 0.177734 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 029 | Total loss: 2.179 | Reg loss: 0.031 | Tree loss: 2.179 | Accuracy: 0.201172 | 0.851 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 029 | Total loss: 2.070 | Reg loss: 0.031 | Tree loss: 2.070 | Accuracy: 0.189655 | 0.851 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 029 | Total loss: 2.252 | Reg loss: 0.030 | Tree loss: 2.252 | Accuracy: 0.207031 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 029 | Total loss: 2.277 | Reg loss: 0.030 | Tree loss: 2.277 | Accuracy: 0.199219 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 029 | Total loss: 2.295 | Reg loss: 0.030 | Tree loss: 2.295 | Accuracy: 0.199219 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 029 | Total loss: 2.258 | Reg loss: 0.030 | Tree loss: 2.258 | Accuracy: 0.216797 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 029 | Total loss: 2.318 | Reg loss: 0.030 | Tree loss: 2.318 | Accuracy: 0.191406 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 029 | Total loss: 2.247 | Reg loss: 0.030 | Tree loss: 2.247 | Accuracy: 0.214844 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.187500 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 029 | Total loss: 2.243 | Reg loss: 0.030 | Tree loss: 2.243 | Accuracy: 0.197266 | 0.851 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 029 | Total loss: 2.276 | Reg loss: 0.030 | Tree loss: 2.276 | Accuracy: 0.193359 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.207031 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 029 | Total loss: 2.242 | Reg loss: 0.030 | Tree loss: 2.242 | Accuracy: 0.218750 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 029 | Total loss: 2.249 | Reg loss: 0.031 | Tree loss: 2.249 | Accuracy: 0.164062 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 029 | Total loss: 2.253 | Reg loss: 0.031 | Tree loss: 2.253 | Accuracy: 0.160156 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 029 | Total loss: 2.240 | Reg loss: 0.031 | Tree loss: 2.240 | Accuracy: 0.169922 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 029 | Total loss: 2.188 | Reg loss: 0.031 | Tree loss: 2.188 | Accuracy: 0.214844 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 029 | Total loss: 2.168 | Reg loss: 0.031 | Tree loss: 2.168 | Accuracy: 0.257812 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 029 | Total loss: 2.218 | Reg loss: 0.031 | Tree loss: 2.218 | Accuracy: 0.210938 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 029 | Total loss: 2.176 | Reg loss: 0.031 | Tree loss: 2.176 | Accuracy: 0.224609 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 029 | Total loss: 2.211 | Reg loss: 0.031 | Tree loss: 2.211 | Accuracy: 0.187500 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.232422 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.201172 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.214844 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.226562 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.210938 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 029 | Total loss: 2.226 | Reg loss: 0.031 | Tree loss: 2.226 | Accuracy: 0.216797 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.191406 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 029 | Total loss: 2.175 | Reg loss: 0.031 | Tree loss: 2.175 | Accuracy: 0.214844 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 029 | Total loss: 2.185 | Reg loss: 0.031 | Tree loss: 2.185 | Accuracy: 0.189453 | 0.85 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 029 | Total loss: 2.137 | Reg loss: 0.031 | Tree loss: 2.137 | Accuracy: 0.293103 | 0.85 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 029 | Total loss: 2.261 | Reg loss: 0.030 | Tree loss: 2.261 | Accuracy: 0.212891 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.230469 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 029 | Total loss: 2.288 | Reg loss: 0.030 | Tree loss: 2.288 | Accuracy: 0.201172 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 029 | Total loss: 2.250 | Reg loss: 0.030 | Tree loss: 2.250 | Accuracy: 0.199219 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 029 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.210938 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 029 | Total loss: 2.241 | Reg loss: 0.030 | Tree loss: 2.241 | Accuracy: 0.220703 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 029 | Total loss: 2.264 | Reg loss: 0.030 | Tree loss: 2.264 | Accuracy: 0.183594 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 029 | Total loss: 2.265 | Reg loss: 0.030 | Tree loss: 2.265 | Accuracy: 0.185547 | 0.85 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 008 / 029 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.214844 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 029 | Total loss: 2.213 | Reg loss: 0.030 | Tree loss: 2.213 | Accuracy: 0.181641 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.191406 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.226562 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 029 | Total loss: 2.200 | Reg loss: 0.030 | Tree loss: 2.200 | Accuracy: 0.236328 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 029 | Total loss: 2.230 | Reg loss: 0.031 | Tree loss: 2.230 | Accuracy: 0.195312 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 029 | Total loss: 2.232 | Reg loss: 0.031 | Tree loss: 2.232 | Accuracy: 0.207031 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 029 | Total loss: 2.252 | Reg loss: 0.031 | Tree loss: 2.252 | Accuracy: 0.166016 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.222656 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 029 | Total loss: 2.203 | Reg loss: 0.031 | Tree loss: 2.203 | Accuracy: 0.203125 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.205078 | 0.85 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.222656 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 029 | Total loss: 2.198 | Reg loss: 0.031 | Tree loss: 2.198 | Accuracy: 0.187500 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.220703 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 029 | Total loss: 2.194 | Reg loss: 0.031 | Tree loss: 2.194 | Accuracy: 0.207031 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 029 | Total loss: 2.198 | Reg loss: 0.031 | Tree loss: 2.198 | Accuracy: 0.189453 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 029 | Total loss: 2.186 | Reg loss: 0.031 | Tree loss: 2.186 | Accuracy: 0.222656 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.191406 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 029 | Total loss: 2.210 | Reg loss: 0.031 | Tree loss: 2.210 | Accuracy: 0.183594 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 029 | Total loss: 2.205 | Reg loss: 0.031 | Tree loss: 2.205 | Accuracy: 0.203125 | 0.849 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 029 | Total loss: 2.155 | Reg loss: 0.031 | Tree loss: 2.155 | Accuracy: 0.275862 | 0.849 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 029 | Total loss: 2.263 | Reg loss: 0.030 | Tree loss: 2.263 | Accuracy: 0.234375 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.218750 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 029 | Total loss: 2.307 | Reg loss: 0.030 | Tree loss: 2.307 | Accuracy: 0.187500 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 029 | Total loss: 2.243 | Reg loss: 0.030 | Tree loss: 2.243 | Accuracy: 0.214844 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 029 | Total loss: 2.234 | Reg loss: 0.030 | Tree loss: 2.234 | Accuracy: 0.191406 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 029 | Total loss: 2.230 | Reg loss: 0.030 | Tree loss: 2.230 | Accuracy: 0.224609 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 029 | Total loss: 2.264 | Reg loss: 0.030 | Tree loss: 2.264 | Accuracy: 0.181641 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 029 | Total loss: 2.266 | Reg loss: 0.030 | Tree loss: 2.266 | Accuracy: 0.197266 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 029 | Total loss: 2.222 | Reg loss: 0.030 | Tree loss: 2.222 | Accuracy: 0.238281 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 029 | Total loss: 2.260 | Reg loss: 0.030 | Tree loss: 2.260 | Accuracy: 0.205078 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.208984 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.205078 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.212891 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 029 | Total loss: 2.216 | Reg loss: 0.031 | Tree loss: 2.216 | Accuracy: 0.181641 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 029 | Total loss: 2.211 | Reg loss: 0.031 | Tree loss: 2.211 | Accuracy: 0.216797 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 029 | Total loss: 2.167 | Reg loss: 0.031 | Tree loss: 2.167 | Accuracy: 0.226562 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 029 | Total loss: 2.234 | Reg loss: 0.031 | Tree loss: 2.234 | Accuracy: 0.208984 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 029 | Total loss: 2.245 | Reg loss: 0.031 | Tree loss: 2.245 | Accuracy: 0.205078 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 029 | Total loss: 2.231 | Reg loss: 0.031 | Tree loss: 2.231 | Accuracy: 0.214844 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.189453 | 0.849 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 029 | Total loss: 2.210 | Reg loss: 0.031 | Tree loss: 2.210 | Accuracy: 0.189453 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.208984 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.210938 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 029 | Total loss: 2.150 | Reg loss: 0.031 | Tree loss: 2.150 | Accuracy: 0.183594 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.177734 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 029 | Total loss: 2.191 | Reg loss: 0.031 | Tree loss: 2.191 | Accuracy: 0.193359 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 029 | Total loss: 2.110 | Reg loss: 0.031 | Tree loss: 2.110 | Accuracy: 0.218750 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 029 | Total loss: 2.167 | Reg loss: 0.031 | Tree loss: 2.167 | Accuracy: 0.193359 | 0.848 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 029 | Total loss: 2.178 | Reg loss: 0.031 | Tree loss: 2.178 | Accuracy: 0.206897 | 0.848 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 029 | Total loss: 2.251 | Reg loss: 0.030 | Tree loss: 2.251 | Accuracy: 0.205078 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 029 | Total loss: 2.254 | Reg loss: 0.030 | Tree loss: 2.254 | Accuracy: 0.203125 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 029 | Total loss: 2.214 | Reg loss: 0.030 | Tree loss: 2.214 | Accuracy: 0.205078 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 029 | Total loss: 2.266 | Reg loss: 0.030 | Tree loss: 2.266 | Accuracy: 0.197266 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 029 | Total loss: 2.279 | Reg loss: 0.030 | Tree loss: 2.279 | Accuracy: 0.171875 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 029 | Total loss: 2.288 | Reg loss: 0.030 | Tree loss: 2.288 | Accuracy: 0.203125 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 029 | Total loss: 2.281 | Reg loss: 0.030 | Tree loss: 2.281 | Accuracy: 0.228516 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 029 | Total loss: 2.275 | Reg loss: 0.030 | Tree loss: 2.275 | Accuracy: 0.185547 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 029 | Total loss: 2.239 | Reg loss: 0.030 | Tree loss: 2.239 | Accuracy: 0.214844 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 029 | Total loss: 2.237 | Reg loss: 0.030 | Tree loss: 2.237 | Accuracy: 0.185547 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 029 | Total loss: 2.253 | Reg loss: 0.030 | Tree loss: 2.253 | Accuracy: 0.222656 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.216797 | 0.848 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 012 / 029 | Total loss: 2.210 | Reg loss: 0.030 | Tree loss: 2.210 | Accuracy: 0.230469 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.234375 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 029 | Total loss: 2.201 | Reg loss: 0.031 | Tree loss: 2.201 | Accuracy: 0.205078 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.183594 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 029 | Total loss: 2.159 | Reg loss: 0.031 | Tree loss: 2.159 | Accuracy: 0.220703 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 029 | Total loss: 2.214 | Reg loss: 0.031 | Tree loss: 2.214 | Accuracy: 0.197266 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 029 | Total loss: 2.164 | Reg loss: 0.031 | Tree loss: 2.164 | Accuracy: 0.191406 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.205078 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 029 | Total loss: 2.187 | Reg loss: 0.031 | Tree loss: 2.187 | Accuracy: 0.195312 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 029 | Total loss: 2.162 | Reg loss: 0.031 | Tree loss: 2.162 | Accuracy: 0.208984 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 029 | Total loss: 2.180 | Reg loss: 0.031 | Tree loss: 2.180 | Accuracy: 0.218750 | 0.848 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.148438 | 0.847 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 029 | Total loss: 2.157 | Reg loss: 0.031 | Tree loss: 2.157 | Accuracy: 0.201172 | 0.847 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 029 | Total loss: 2.164 | Reg loss: 0.031 | Tree loss: 2.164 | Accuracy: 0.210938 | 0.847 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 029 | Total loss: 2.169 | Reg loss: 0.031 | Tree loss: 2.169 | Accuracy: 0.224609 | 0.847 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 029 | Total loss: 2.206 | Reg loss: 0.031 | Tree loss: 2.206 | Accuracy: 0.212891 | 0.847 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 029 | Total loss: 2.207 | Reg loss: 0.031 | Tree loss: 2.207 | Accuracy: 0.310345 | 0.847 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 029 | Total loss: 2.245 | Reg loss: 0.030 | Tree loss: 2.245 | Accuracy: 0.197266 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 029 | Total loss: 2.246 | Reg loss: 0.030 | Tree loss: 2.246 | Accuracy: 0.228516 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 029 | Total loss: 2.267 | Reg loss: 0.030 | Tree loss: 2.267 | Accuracy: 0.216797 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 029 | Total loss: 2.246 | Reg loss: 0.030 | Tree loss: 2.246 | Accuracy: 0.199219 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 029 | Total loss: 2.259 | Reg loss: 0.030 | Tree loss: 2.259 | Accuracy: 0.197266 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 029 | Total loss: 2.251 | Reg loss: 0.030 | Tree loss: 2.251 | Accuracy: 0.199219 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.193359 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 029 | Total loss: 2.281 | Reg loss: 0.030 | Tree loss: 2.281 | Accuracy: 0.193359 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 029 | Total loss: 2.230 | Reg loss: 0.030 | Tree loss: 2.230 | Accuracy: 0.199219 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 029 | Total loss: 2.221 | Reg loss: 0.030 | Tree loss: 2.221 | Accuracy: 0.207031 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.228516 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 029 | Total loss: 2.231 | Reg loss: 0.030 | Tree loss: 2.231 | Accuracy: 0.195312 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.214844 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 029 | Total loss: 2.211 | Reg loss: 0.030 | Tree loss: 2.211 | Accuracy: 0.195312 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 029 | Total loss: 2.190 | Reg loss: 0.030 | Tree loss: 2.190 | Accuracy: 0.187500 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.230469 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 029 | Total loss: 2.200 | Reg loss: 0.031 | Tree loss: 2.200 | Accuracy: 0.212891 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 029 | Total loss: 2.220 | Reg loss: 0.031 | Tree loss: 2.220 | Accuracy: 0.218750 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 029 | Total loss: 2.168 | Reg loss: 0.031 | Tree loss: 2.168 | Accuracy: 0.199219 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 029 | Total loss: 2.141 | Reg loss: 0.031 | Tree loss: 2.141 | Accuracy: 0.226562 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 029 | Total loss: 2.220 | Reg loss: 0.031 | Tree loss: 2.220 | Accuracy: 0.216797 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.187500 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 029 | Total loss: 2.190 | Reg loss: 0.031 | Tree loss: 2.190 | Accuracy: 0.210938 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.203125 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 029 | Total loss: 2.189 | Reg loss: 0.031 | Tree loss: 2.189 | Accuracy: 0.208984 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 029 | Total loss: 2.195 | Reg loss: 0.031 | Tree loss: 2.195 | Accuracy: 0.187500 | 0.847 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.195312 | 0.846 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 029 | Total loss: 2.168 | Reg loss: 0.031 | Tree loss: 2.168 | Accuracy: 0.193359 | 0.846 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 029 | Total loss: 2.058 | Reg loss: 0.031 | Tree loss: 2.058 | Accuracy: 0.189655 | 0.846 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.207031 | 0.847 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 029 | Total loss: 2.282 | Reg loss: 0.030 | Tree loss: 2.282 | Accuracy: 0.203125 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.205078 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 029 | Total loss: 2.269 | Reg loss: 0.030 | Tree loss: 2.269 | Accuracy: 0.232422 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.216797 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 029 | Total loss: 2.242 | Reg loss: 0.030 | Tree loss: 2.242 | Accuracy: 0.214844 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.197266 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 029 | Total loss: 2.220 | Reg loss: 0.030 | Tree loss: 2.220 | Accuracy: 0.201172 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 029 | Total loss: 2.210 | Reg loss: 0.030 | Tree loss: 2.210 | Accuracy: 0.248047 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.238281 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 029 | Total loss: 2.258 | Reg loss: 0.030 | Tree loss: 2.258 | Accuracy: 0.199219 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.185547 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.187500 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 029 | Total loss: 2.212 | Reg loss: 0.030 | Tree loss: 2.212 | Accuracy: 0.203125 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 029 | Total loss: 2.224 | Reg loss: 0.030 | Tree loss: 2.224 | Accuracy: 0.220703 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.187500 | 0.846 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 016 / 029 | Total loss: 2.223 | Reg loss: 0.030 | Tree loss: 2.223 | Accuracy: 0.207031 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 029 | Total loss: 2.203 | Reg loss: 0.031 | Tree loss: 2.203 | Accuracy: 0.218750 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 029 | Total loss: 2.179 | Reg loss: 0.031 | Tree loss: 2.179 | Accuracy: 0.193359 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.214844 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.201172 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 029 | Total loss: 2.178 | Reg loss: 0.031 | Tree loss: 2.178 | Accuracy: 0.179688 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 029 | Total loss: 2.201 | Reg loss: 0.031 | Tree loss: 2.201 | Accuracy: 0.214844 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 029 | Total loss: 2.223 | Reg loss: 0.031 | Tree loss: 2.223 | Accuracy: 0.164062 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 029 | Total loss: 2.178 | Reg loss: 0.031 | Tree loss: 2.178 | Accuracy: 0.164062 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 029 | Total loss: 2.202 | Reg loss: 0.031 | Tree loss: 2.202 | Accuracy: 0.212891 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 029 | Total loss: 2.145 | Reg loss: 0.031 | Tree loss: 2.145 | Accuracy: 0.228516 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.193359 | 0.846 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 029 | Total loss: 2.043 | Reg loss: 0.031 | Tree loss: 2.043 | Accuracy: 0.206897 | 0.846 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 029 | Total loss: 2.273 | Reg loss: 0.030 | Tree loss: 2.273 | Accuracy: 0.199219 | 0.846 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 029 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.222656 | 0.846 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 029 | Total loss: 2.278 | Reg loss: 0.030 | Tree loss: 2.278 | Accuracy: 0.181641 | 0.846 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 029 | Total loss: 2.242 | Reg loss: 0.030 | Tree loss: 2.242 | Accuracy: 0.222656 | 0.846 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.208984 | 0.846 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.185547 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.199219 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 029 | Total loss: 2.247 | Reg loss: 0.030 | Tree loss: 2.247 | Accuracy: 0.205078 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 029 | Total loss: 2.201 | Reg loss: 0.030 | Tree loss: 2.201 | Accuracy: 0.240234 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 029 | Total loss: 2.234 | Reg loss: 0.030 | Tree loss: 2.234 | Accuracy: 0.201172 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 029 | Total loss: 2.249 | Reg loss: 0.030 | Tree loss: 2.249 | Accuracy: 0.160156 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.189453 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.212891 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.203125 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.201172 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 029 | Total loss: 2.248 | Reg loss: 0.030 | Tree loss: 2.248 | Accuracy: 0.195312 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 029 | Total loss: 2.199 | Reg loss: 0.030 | Tree loss: 2.199 | Accuracy: 0.199219 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 029 | Total loss: 2.184 | Reg loss: 0.030 | Tree loss: 2.184 | Accuracy: 0.214844 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 029 | Total loss: 2.164 | Reg loss: 0.031 | Tree loss: 2.164 | Accuracy: 0.199219 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.228516 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 029 | Total loss: 2.147 | Reg loss: 0.031 | Tree loss: 2.147 | Accuracy: 0.216797 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 029 | Total loss: 2.168 | Reg loss: 0.031 | Tree loss: 2.168 | Accuracy: 0.212891 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 029 | Total loss: 2.159 | Reg loss: 0.031 | Tree loss: 2.159 | Accuracy: 0.197266 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 029 | Total loss: 2.157 | Reg loss: 0.031 | Tree loss: 2.157 | Accuracy: 0.203125 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 029 | Total loss: 2.188 | Reg loss: 0.031 | Tree loss: 2.188 | Accuracy: 0.212891 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 029 | Total loss: 2.203 | Reg loss: 0.031 | Tree loss: 2.203 | Accuracy: 0.207031 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 029 | Total loss: 2.174 | Reg loss: 0.031 | Tree loss: 2.174 | Accuracy: 0.193359 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 029 | Total loss: 2.163 | Reg loss: 0.031 | Tree loss: 2.163 | Accuracy: 0.224609 | 0.845 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 029 | Total loss: 2.105 | Reg loss: 0.031 | Tree loss: 2.105 | Accuracy: 0.172414 | 0.845 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 029 | Total loss: 2.238 | Reg loss: 0.030 | Tree loss: 2.238 | Accuracy: 0.236328 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 029 | Total loss: 2.269 | Reg loss: 0.030 | Tree loss: 2.269 | Accuracy: 0.214844 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.179688 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 029 | Total loss: 2.285 | Reg loss: 0.030 | Tree loss: 2.285 | Accuracy: 0.175781 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 029 | Total loss: 2.293 | Reg loss: 0.030 | Tree loss: 2.293 | Accuracy: 0.199219 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 029 | Total loss: 2.307 | Reg loss: 0.030 | Tree loss: 2.307 | Accuracy: 0.189453 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 029 | Total loss: 2.230 | Reg loss: 0.030 | Tree loss: 2.230 | Accuracy: 0.222656 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.220703 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.173828 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 029 | Total loss: 2.230 | Reg loss: 0.030 | Tree loss: 2.230 | Accuracy: 0.214844 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 029 | Total loss: 2.267 | Reg loss: 0.030 | Tree loss: 2.267 | Accuracy: 0.179688 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.207031 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.228516 | 0.845 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 029 | Total loss: 2.173 | Reg loss: 0.030 | Tree loss: 2.173 | Accuracy: 0.208984 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.199219 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 029 | Total loss: 2.185 | Reg loss: 0.030 | Tree loss: 2.185 | Accuracy: 0.205078 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.185547 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.220703 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 029 | Total loss: 2.173 | Reg loss: 0.031 | Tree loss: 2.173 | Accuracy: 0.207031 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.214844 | 0.844 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 020 / 029 | Total loss: 2.182 | Reg loss: 0.031 | Tree loss: 2.182 | Accuracy: 0.212891 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 029 | Total loss: 2.165 | Reg loss: 0.031 | Tree loss: 2.165 | Accuracy: 0.203125 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 029 | Total loss: 2.172 | Reg loss: 0.031 | Tree loss: 2.172 | Accuracy: 0.228516 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 029 | Total loss: 2.154 | Reg loss: 0.031 | Tree loss: 2.154 | Accuracy: 0.216797 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 029 | Total loss: 2.166 | Reg loss: 0.031 | Tree loss: 2.166 | Accuracy: 0.228516 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.195312 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 029 | Total loss: 2.133 | Reg loss: 0.031 | Tree loss: 2.133 | Accuracy: 0.195312 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 029 | Total loss: 2.145 | Reg loss: 0.031 | Tree loss: 2.145 | Accuracy: 0.167969 | 0.844 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 029 | Total loss: 2.218 | Reg loss: 0.031 | Tree loss: 2.218 | Accuracy: 0.189655 | 0.844 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 029 | Total loss: 2.273 | Reg loss: 0.030 | Tree loss: 2.273 | Accuracy: 0.216797 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 029 | Total loss: 2.251 | Reg loss: 0.030 | Tree loss: 2.251 | Accuracy: 0.210938 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.244141 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.220703 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 029 | Total loss: 2.259 | Reg loss: 0.030 | Tree loss: 2.259 | Accuracy: 0.201172 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 029 | Total loss: 2.253 | Reg loss: 0.030 | Tree loss: 2.253 | Accuracy: 0.183594 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.167969 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 029 | Total loss: 2.213 | Reg loss: 0.030 | Tree loss: 2.213 | Accuracy: 0.201172 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 029 | Total loss: 2.201 | Reg loss: 0.030 | Tree loss: 2.201 | Accuracy: 0.193359 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 029 | Total loss: 2.199 | Reg loss: 0.030 | Tree loss: 2.199 | Accuracy: 0.255859 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 029 | Total loss: 2.194 | Reg loss: 0.030 | Tree loss: 2.194 | Accuracy: 0.203125 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 029 | Total loss: 2.250 | Reg loss: 0.030 | Tree loss: 2.250 | Accuracy: 0.187500 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.183594 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 029 | Total loss: 2.194 | Reg loss: 0.030 | Tree loss: 2.194 | Accuracy: 0.244141 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.193359 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.208984 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.207031 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.189453 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.187500 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 029 | Total loss: 2.227 | Reg loss: 0.031 | Tree loss: 2.227 | Accuracy: 0.199219 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 029 | Total loss: 2.172 | Reg loss: 0.031 | Tree loss: 2.172 | Accuracy: 0.207031 | 0.844 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 029 | Total loss: 2.160 | Reg loss: 0.031 | Tree loss: 2.160 | Accuracy: 0.191406 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 029 | Total loss: 2.146 | Reg loss: 0.031 | Tree loss: 2.146 | Accuracy: 0.205078 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.222656 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 029 | Total loss: 2.177 | Reg loss: 0.031 | Tree loss: 2.177 | Accuracy: 0.189453 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 029 | Total loss: 2.174 | Reg loss: 0.031 | Tree loss: 2.174 | Accuracy: 0.199219 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 029 | Total loss: 2.194 | Reg loss: 0.031 | Tree loss: 2.194 | Accuracy: 0.199219 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 029 | Total loss: 2.162 | Reg loss: 0.031 | Tree loss: 2.162 | Accuracy: 0.212891 | 0.843 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 029 | Total loss: 2.101 | Reg loss: 0.031 | Tree loss: 2.101 | Accuracy: 0.275862 | 0.843 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 029 | Total loss: 2.294 | Reg loss: 0.030 | Tree loss: 2.294 | Accuracy: 0.181641 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 029 | Total loss: 2.231 | Reg loss: 0.030 | Tree loss: 2.231 | Accuracy: 0.234375 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 029 | Total loss: 2.276 | Reg loss: 0.030 | Tree loss: 2.276 | Accuracy: 0.208984 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 029 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.208984 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 029 | Total loss: 2.269 | Reg loss: 0.030 | Tree loss: 2.269 | Accuracy: 0.195312 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 029 | Total loss: 2.254 | Reg loss: 0.030 | Tree loss: 2.254 | Accuracy: 0.185547 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.238281 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 029 | Total loss: 2.277 | Reg loss: 0.030 | Tree loss: 2.277 | Accuracy: 0.208984 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.201172 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 029 | Total loss: 2.248 | Reg loss: 0.030 | Tree loss: 2.248 | Accuracy: 0.142578 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 029 | Total loss: 2.244 | Reg loss: 0.030 | Tree loss: 2.244 | Accuracy: 0.167969 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.175781 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 029 | Total loss: 2.183 | Reg loss: 0.030 | Tree loss: 2.183 | Accuracy: 0.244141 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.212891 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.181641 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.212891 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.230469 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.187500 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.201172 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 029 | Total loss: 2.173 | Reg loss: 0.030 | Tree loss: 2.173 | Accuracy: 0.230469 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 029 | Total loss: 2.124 | Reg loss: 0.031 | Tree loss: 2.124 | Accuracy: 0.195312 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 029 | Total loss: 2.186 | Reg loss: 0.031 | Tree loss: 2.186 | Accuracy: 0.216797 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 029 | Total loss: 2.221 | Reg loss: 0.031 | Tree loss: 2.221 | Accuracy: 0.189453 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 029 | Total loss: 2.162 | Reg loss: 0.031 | Tree loss: 2.162 | Accuracy: 0.226562 | 0.843 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 | Batch: 024 / 029 | Total loss: 2.163 | Reg loss: 0.031 | Tree loss: 2.163 | Accuracy: 0.191406 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 029 | Total loss: 2.105 | Reg loss: 0.031 | Tree loss: 2.105 | Accuracy: 0.236328 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 029 | Total loss: 2.161 | Reg loss: 0.031 | Tree loss: 2.161 | Accuracy: 0.207031 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 029 | Total loss: 2.146 | Reg loss: 0.031 | Tree loss: 2.146 | Accuracy: 0.214844 | 0.843 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 029 | Total loss: 2.047 | Reg loss: 0.031 | Tree loss: 2.047 | Accuracy: 0.310345 | 0.843 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 029 | Total loss: 2.299 | Reg loss: 0.030 | Tree loss: 2.299 | Accuracy: 0.191406 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 029 | Total loss: 2.249 | Reg loss: 0.030 | Tree loss: 2.249 | Accuracy: 0.236328 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 029 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.214844 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 029 | Total loss: 2.245 | Reg loss: 0.030 | Tree loss: 2.245 | Accuracy: 0.224609 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 029 | Total loss: 2.264 | Reg loss: 0.030 | Tree loss: 2.264 | Accuracy: 0.185547 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 029 | Total loss: 2.221 | Reg loss: 0.030 | Tree loss: 2.221 | Accuracy: 0.189453 | 0.843 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.191406 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 029 | Total loss: 2.213 | Reg loss: 0.030 | Tree loss: 2.213 | Accuracy: 0.191406 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 029 | Total loss: 2.247 | Reg loss: 0.030 | Tree loss: 2.247 | Accuracy: 0.212891 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 029 | Total loss: 2.210 | Reg loss: 0.030 | Tree loss: 2.210 | Accuracy: 0.205078 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 029 | Total loss: 2.206 | Reg loss: 0.030 | Tree loss: 2.206 | Accuracy: 0.226562 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.181641 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 029 | Total loss: 2.205 | Reg loss: 0.030 | Tree loss: 2.205 | Accuracy: 0.175781 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.207031 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.214844 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.218750 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.197266 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.232422 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.210938 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 029 | Total loss: 2.163 | Reg loss: 0.030 | Tree loss: 2.163 | Accuracy: 0.185547 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 029 | Total loss: 2.146 | Reg loss: 0.031 | Tree loss: 2.146 | Accuracy: 0.212891 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 029 | Total loss: 2.156 | Reg loss: 0.031 | Tree loss: 2.156 | Accuracy: 0.201172 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.212891 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 029 | Total loss: 2.191 | Reg loss: 0.031 | Tree loss: 2.191 | Accuracy: 0.193359 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 029 | Total loss: 2.122 | Reg loss: 0.031 | Tree loss: 2.122 | Accuracy: 0.224609 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 029 | Total loss: 2.205 | Reg loss: 0.031 | Tree loss: 2.205 | Accuracy: 0.183594 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 029 | Total loss: 2.165 | Reg loss: 0.031 | Tree loss: 2.165 | Accuracy: 0.218750 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 029 | Total loss: 2.179 | Reg loss: 0.031 | Tree loss: 2.179 | Accuracy: 0.201172 | 0.842 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 029 | Total loss: 2.043 | Reg loss: 0.031 | Tree loss: 2.043 | Accuracy: 0.241379 | 0.842 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 029 | Total loss: 2.267 | Reg loss: 0.030 | Tree loss: 2.267 | Accuracy: 0.222656 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 029 | Total loss: 2.205 | Reg loss: 0.030 | Tree loss: 2.205 | Accuracy: 0.234375 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 029 | Total loss: 2.218 | Reg loss: 0.030 | Tree loss: 2.218 | Accuracy: 0.201172 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 029 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.191406 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 029 | Total loss: 2.193 | Reg loss: 0.030 | Tree loss: 2.193 | Accuracy: 0.216797 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 029 | Total loss: 2.239 | Reg loss: 0.030 | Tree loss: 2.239 | Accuracy: 0.210938 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 029 | Total loss: 2.253 | Reg loss: 0.030 | Tree loss: 2.253 | Accuracy: 0.179688 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 029 | Total loss: 2.223 | Reg loss: 0.030 | Tree loss: 2.223 | Accuracy: 0.208984 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 029 | Total loss: 2.209 | Reg loss: 0.030 | Tree loss: 2.209 | Accuracy: 0.228516 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 029 | Total loss: 2.183 | Reg loss: 0.030 | Tree loss: 2.183 | Accuracy: 0.185547 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 029 | Total loss: 2.268 | Reg loss: 0.030 | Tree loss: 2.268 | Accuracy: 0.197266 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.210938 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.226562 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.214844 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.214844 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.193359 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 029 | Total loss: 2.236 | Reg loss: 0.030 | Tree loss: 2.236 | Accuracy: 0.164062 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 029 | Total loss: 2.184 | Reg loss: 0.030 | Tree loss: 2.184 | Accuracy: 0.230469 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.234375 | 0.842 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.216797 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 029 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.203125 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 029 | Total loss: 2.170 | Reg loss: 0.031 | Tree loss: 2.170 | Accuracy: 0.207031 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 029 | Total loss: 2.172 | Reg loss: 0.031 | Tree loss: 2.172 | Accuracy: 0.191406 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 029 | Total loss: 2.153 | Reg loss: 0.031 | Tree loss: 2.153 | Accuracy: 0.201172 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 029 | Total loss: 2.177 | Reg loss: 0.031 | Tree loss: 2.177 | Accuracy: 0.175781 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 029 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.162109 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 029 | Total loss: 2.185 | Reg loss: 0.031 | Tree loss: 2.185 | Accuracy: 0.212891 | 0.841 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 029 | Total loss: 2.135 | Reg loss: 0.031 | Tree loss: 2.135 | Accuracy: 0.205078 | 0.841 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 028 / 029 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.172414 | 0.841 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 029 | Total loss: 2.249 | Reg loss: 0.030 | Tree loss: 2.249 | Accuracy: 0.216797 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 029 | Total loss: 2.253 | Reg loss: 0.030 | Tree loss: 2.253 | Accuracy: 0.199219 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 029 | Total loss: 2.289 | Reg loss: 0.030 | Tree loss: 2.289 | Accuracy: 0.203125 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.195312 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.222656 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.228516 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 029 | Total loss: 2.206 | Reg loss: 0.030 | Tree loss: 2.206 | Accuracy: 0.216797 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 029 | Total loss: 2.221 | Reg loss: 0.030 | Tree loss: 2.221 | Accuracy: 0.228516 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 029 | Total loss: 2.220 | Reg loss: 0.030 | Tree loss: 2.220 | Accuracy: 0.193359 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 029 | Total loss: 2.256 | Reg loss: 0.030 | Tree loss: 2.256 | Accuracy: 0.183594 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.214844 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 029 | Total loss: 2.196 | Reg loss: 0.030 | Tree loss: 2.196 | Accuracy: 0.203125 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.191406 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 029 | Total loss: 2.212 | Reg loss: 0.030 | Tree loss: 2.212 | Accuracy: 0.195312 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.179688 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 029 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.185547 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.193359 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 029 | Total loss: 2.133 | Reg loss: 0.030 | Tree loss: 2.133 | Accuracy: 0.205078 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 029 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.220703 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.236328 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.195312 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 029 | Total loss: 2.209 | Reg loss: 0.031 | Tree loss: 2.209 | Accuracy: 0.175781 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 029 | Total loss: 2.140 | Reg loss: 0.031 | Tree loss: 2.140 | Accuracy: 0.224609 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 029 | Total loss: 2.143 | Reg loss: 0.031 | Tree loss: 2.143 | Accuracy: 0.195312 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 029 | Total loss: 2.146 | Reg loss: 0.031 | Tree loss: 2.146 | Accuracy: 0.201172 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 029 | Total loss: 2.209 | Reg loss: 0.031 | Tree loss: 2.209 | Accuracy: 0.199219 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.218750 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 029 | Total loss: 2.115 | Reg loss: 0.031 | Tree loss: 2.115 | Accuracy: 0.218750 | 0.841 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 029 | Total loss: 2.156 | Reg loss: 0.031 | Tree loss: 2.156 | Accuracy: 0.206897 | 0.841 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 029 | Total loss: 2.273 | Reg loss: 0.030 | Tree loss: 2.273 | Accuracy: 0.191406 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 029 | Total loss: 2.290 | Reg loss: 0.030 | Tree loss: 2.290 | Accuracy: 0.216797 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 029 | Total loss: 2.227 | Reg loss: 0.030 | Tree loss: 2.227 | Accuracy: 0.222656 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.201172 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.224609 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.220703 | 0.841 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 029 | Total loss: 2.249 | Reg loss: 0.030 | Tree loss: 2.249 | Accuracy: 0.189453 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 029 | Total loss: 2.262 | Reg loss: 0.030 | Tree loss: 2.262 | Accuracy: 0.175781 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 029 | Total loss: 2.237 | Reg loss: 0.030 | Tree loss: 2.237 | Accuracy: 0.208984 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 029 | Total loss: 2.238 | Reg loss: 0.030 | Tree loss: 2.238 | Accuracy: 0.208984 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 029 | Total loss: 2.209 | Reg loss: 0.030 | Tree loss: 2.209 | Accuracy: 0.218750 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 029 | Total loss: 2.211 | Reg loss: 0.030 | Tree loss: 2.211 | Accuracy: 0.199219 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 029 | Total loss: 2.164 | Reg loss: 0.030 | Tree loss: 2.164 | Accuracy: 0.193359 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 029 | Total loss: 2.197 | Reg loss: 0.030 | Tree loss: 2.197 | Accuracy: 0.201172 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 029 | Total loss: 2.158 | Reg loss: 0.030 | Tree loss: 2.158 | Accuracy: 0.210938 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 029 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.201172 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 029 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.226562 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.171875 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 029 | Total loss: 2.160 | Reg loss: 0.030 | Tree loss: 2.160 | Accuracy: 0.230469 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 029 | Total loss: 2.157 | Reg loss: 0.030 | Tree loss: 2.157 | Accuracy: 0.201172 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.197266 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 029 | Total loss: 2.132 | Reg loss: 0.030 | Tree loss: 2.132 | Accuracy: 0.203125 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 029 | Total loss: 2.175 | Reg loss: 0.031 | Tree loss: 2.175 | Accuracy: 0.197266 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 029 | Total loss: 2.161 | Reg loss: 0.031 | Tree loss: 2.161 | Accuracy: 0.203125 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 029 | Total loss: 2.160 | Reg loss: 0.031 | Tree loss: 2.160 | Accuracy: 0.216797 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 029 | Total loss: 2.167 | Reg loss: 0.031 | Tree loss: 2.167 | Accuracy: 0.189453 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 029 | Total loss: 2.177 | Reg loss: 0.031 | Tree loss: 2.177 | Accuracy: 0.199219 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 029 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.216797 | 0.84 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 029 | Total loss: 2.077 | Reg loss: 0.031 | Tree loss: 2.077 | Accuracy: 0.224138 | 0.84 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 029 | Total loss: 2.282 | Reg loss: 0.030 | Tree loss: 2.282 | Accuracy: 0.216797 | 0.84 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 001 / 029 | Total loss: 2.259 | Reg loss: 0.030 | Tree loss: 2.259 | Accuracy: 0.208984 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 029 | Total loss: 2.222 | Reg loss: 0.030 | Tree loss: 2.222 | Accuracy: 0.218750 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 029 | Total loss: 2.231 | Reg loss: 0.030 | Tree loss: 2.231 | Accuracy: 0.203125 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 029 | Total loss: 2.222 | Reg loss: 0.030 | Tree loss: 2.222 | Accuracy: 0.210938 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 029 | Total loss: 2.196 | Reg loss: 0.030 | Tree loss: 2.196 | Accuracy: 0.208984 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 029 | Total loss: 2.215 | Reg loss: 0.030 | Tree loss: 2.215 | Accuracy: 0.220703 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 029 | Total loss: 2.234 | Reg loss: 0.030 | Tree loss: 2.234 | Accuracy: 0.218750 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 029 | Total loss: 2.190 | Reg loss: 0.030 | Tree loss: 2.190 | Accuracy: 0.197266 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.240234 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 029 | Total loss: 2.185 | Reg loss: 0.030 | Tree loss: 2.185 | Accuracy: 0.191406 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.181641 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.183594 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 029 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.230469 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.181641 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.195312 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.220703 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.216797 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 029 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.218750 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 029 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.216797 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 029 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.208984 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 029 | Total loss: 2.208 | Reg loss: 0.030 | Tree loss: 2.208 | Accuracy: 0.167969 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 029 | Total loss: 2.123 | Reg loss: 0.030 | Tree loss: 2.123 | Accuracy: 0.195312 | 0.84 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 029 | Total loss: 2.182 | Reg loss: 0.031 | Tree loss: 2.182 | Accuracy: 0.214844 | 0.839 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 029 | Total loss: 2.158 | Reg loss: 0.031 | Tree loss: 2.158 | Accuracy: 0.164062 | 0.839 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 029 | Total loss: 2.176 | Reg loss: 0.031 | Tree loss: 2.176 | Accuracy: 0.205078 | 0.839 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 029 | Total loss: 2.128 | Reg loss: 0.031 | Tree loss: 2.128 | Accuracy: 0.193359 | 0.839 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 029 | Total loss: 2.181 | Reg loss: 0.031 | Tree loss: 2.181 | Accuracy: 0.218750 | 0.839 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 029 | Total loss: 2.107 | Reg loss: 0.031 | Tree loss: 2.107 | Accuracy: 0.137931 | 0.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 029 | Total loss: 2.270 | Reg loss: 0.030 | Tree loss: 2.270 | Accuracy: 0.230469 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 029 | Total loss: 2.223 | Reg loss: 0.030 | Tree loss: 2.223 | Accuracy: 0.208984 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 029 | Total loss: 2.236 | Reg loss: 0.030 | Tree loss: 2.236 | Accuracy: 0.183594 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 029 | Total loss: 2.278 | Reg loss: 0.030 | Tree loss: 2.278 | Accuracy: 0.210938 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 029 | Total loss: 2.278 | Reg loss: 0.030 | Tree loss: 2.278 | Accuracy: 0.197266 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.205078 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.234375 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.183594 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 029 | Total loss: 2.183 | Reg loss: 0.030 | Tree loss: 2.183 | Accuracy: 0.224609 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.207031 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.185547 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.177734 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.201172 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.224609 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.228516 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.218750 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 029 | Total loss: 2.209 | Reg loss: 0.030 | Tree loss: 2.209 | Accuracy: 0.210938 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.197266 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.220703 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 029 | Total loss: 2.123 | Reg loss: 0.030 | Tree loss: 2.123 | Accuracy: 0.203125 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 029 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.218750 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.183594 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 029 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.193359 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 029 | Total loss: 2.146 | Reg loss: 0.030 | Tree loss: 2.146 | Accuracy: 0.220703 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 029 | Total loss: 2.157 | Reg loss: 0.031 | Tree loss: 2.157 | Accuracy: 0.177734 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 029 | Total loss: 2.186 | Reg loss: 0.031 | Tree loss: 2.186 | Accuracy: 0.208984 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 029 | Total loss: 2.133 | Reg loss: 0.031 | Tree loss: 2.133 | Accuracy: 0.228516 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 029 | Total loss: 2.196 | Reg loss: 0.031 | Tree loss: 2.196 | Accuracy: 0.156250 | 0.839 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 029 | Total loss: 2.147 | Reg loss: 0.031 | Tree loss: 2.147 | Accuracy: 0.189655 | 0.839 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 029 | Total loss: 2.236 | Reg loss: 0.030 | Tree loss: 2.236 | Accuracy: 0.218750 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.197266 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 029 | Total loss: 2.259 | Reg loss: 0.030 | Tree loss: 2.259 | Accuracy: 0.210938 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 029 | Total loss: 2.269 | Reg loss: 0.030 | Tree loss: 2.269 | Accuracy: 0.203125 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.218750 | 0.839 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 005 / 029 | Total loss: 2.212 | Reg loss: 0.030 | Tree loss: 2.212 | Accuracy: 0.201172 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 029 | Total loss: 2.246 | Reg loss: 0.030 | Tree loss: 2.246 | Accuracy: 0.203125 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 029 | Total loss: 2.315 | Reg loss: 0.030 | Tree loss: 2.315 | Accuracy: 0.183594 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 029 | Total loss: 2.218 | Reg loss: 0.030 | Tree loss: 2.218 | Accuracy: 0.226562 | 0.839 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.207031 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 029 | Total loss: 2.243 | Reg loss: 0.030 | Tree loss: 2.243 | Accuracy: 0.212891 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 029 | Total loss: 2.176 | Reg loss: 0.030 | Tree loss: 2.176 | Accuracy: 0.214844 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.208984 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.203125 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.199219 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.189453 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.207031 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.195312 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.207031 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.218750 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 029 | Total loss: 2.126 | Reg loss: 0.030 | Tree loss: 2.126 | Accuracy: 0.210938 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 029 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.212891 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 029 | Total loss: 2.092 | Reg loss: 0.030 | Tree loss: 2.092 | Accuracy: 0.214844 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 029 | Total loss: 2.145 | Reg loss: 0.030 | Tree loss: 2.145 | Accuracy: 0.197266 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 029 | Total loss: 2.131 | Reg loss: 0.031 | Tree loss: 2.131 | Accuracy: 0.187500 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 029 | Total loss: 2.120 | Reg loss: 0.031 | Tree loss: 2.120 | Accuracy: 0.205078 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 029 | Total loss: 2.147 | Reg loss: 0.031 | Tree loss: 2.147 | Accuracy: 0.205078 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 029 | Total loss: 2.150 | Reg loss: 0.031 | Tree loss: 2.150 | Accuracy: 0.179688 | 0.838 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 029 | Total loss: 2.153 | Reg loss: 0.031 | Tree loss: 2.153 | Accuracy: 0.189655 | 0.838 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 029 | Total loss: 2.262 | Reg loss: 0.030 | Tree loss: 2.262 | Accuracy: 0.181641 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 029 | Total loss: 2.263 | Reg loss: 0.030 | Tree loss: 2.263 | Accuracy: 0.197266 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 029 | Total loss: 2.231 | Reg loss: 0.030 | Tree loss: 2.231 | Accuracy: 0.205078 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 029 | Total loss: 2.247 | Reg loss: 0.030 | Tree loss: 2.247 | Accuracy: 0.185547 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 029 | Total loss: 2.239 | Reg loss: 0.030 | Tree loss: 2.239 | Accuracy: 0.210938 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 029 | Total loss: 2.208 | Reg loss: 0.030 | Tree loss: 2.208 | Accuracy: 0.224609 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.191406 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 029 | Total loss: 2.194 | Reg loss: 0.030 | Tree loss: 2.194 | Accuracy: 0.212891 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 029 | Total loss: 2.214 | Reg loss: 0.030 | Tree loss: 2.214 | Accuracy: 0.199219 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.214844 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 029 | Total loss: 2.234 | Reg loss: 0.030 | Tree loss: 2.234 | Accuracy: 0.160156 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 029 | Total loss: 2.180 | Reg loss: 0.030 | Tree loss: 2.180 | Accuracy: 0.236328 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 029 | Total loss: 2.157 | Reg loss: 0.030 | Tree loss: 2.157 | Accuracy: 0.220703 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 029 | Total loss: 2.210 | Reg loss: 0.030 | Tree loss: 2.210 | Accuracy: 0.220703 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.203125 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 029 | Total loss: 2.165 | Reg loss: 0.030 | Tree loss: 2.165 | Accuracy: 0.191406 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 029 | Total loss: 2.220 | Reg loss: 0.030 | Tree loss: 2.220 | Accuracy: 0.193359 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 029 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.199219 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.208984 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 029 | Total loss: 2.144 | Reg loss: 0.030 | Tree loss: 2.144 | Accuracy: 0.218750 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 029 | Total loss: 2.143 | Reg loss: 0.030 | Tree loss: 2.143 | Accuracy: 0.228516 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 029 | Total loss: 2.147 | Reg loss: 0.030 | Tree loss: 2.147 | Accuracy: 0.191406 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.187500 | 0.838 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 029 | Total loss: 2.161 | Reg loss: 0.030 | Tree loss: 2.161 | Accuracy: 0.212891 | 0.837 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.203125 | 0.837 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 029 | Total loss: 2.146 | Reg loss: 0.031 | Tree loss: 2.146 | Accuracy: 0.238281 | 0.837 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 029 | Total loss: 2.158 | Reg loss: 0.031 | Tree loss: 2.158 | Accuracy: 0.210938 | 0.837 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 029 | Total loss: 2.147 | Reg loss: 0.031 | Tree loss: 2.147 | Accuracy: 0.189453 | 0.837 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 029 | Total loss: 2.003 | Reg loss: 0.031 | Tree loss: 2.003 | Accuracy: 0.224138 | 0.837 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 029 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.201172 | 0.838 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 029 | Total loss: 2.254 | Reg loss: 0.030 | Tree loss: 2.254 | Accuracy: 0.201172 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 029 | Total loss: 2.219 | Reg loss: 0.030 | Tree loss: 2.219 | Accuracy: 0.208984 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 029 | Total loss: 2.252 | Reg loss: 0.030 | Tree loss: 2.252 | Accuracy: 0.197266 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 029 | Total loss: 2.208 | Reg loss: 0.030 | Tree loss: 2.208 | Accuracy: 0.214844 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 029 | Total loss: 2.260 | Reg loss: 0.030 | Tree loss: 2.260 | Accuracy: 0.210938 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 029 | Total loss: 2.205 | Reg loss: 0.030 | Tree loss: 2.205 | Accuracy: 0.195312 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 029 | Total loss: 2.223 | Reg loss: 0.030 | Tree loss: 2.223 | Accuracy: 0.216797 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 029 | Total loss: 2.200 | Reg loss: 0.030 | Tree loss: 2.200 | Accuracy: 0.226562 | 0.837 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 009 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.187500 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 029 | Total loss: 2.236 | Reg loss: 0.030 | Tree loss: 2.236 | Accuracy: 0.189453 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.195312 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 029 | Total loss: 2.212 | Reg loss: 0.030 | Tree loss: 2.212 | Accuracy: 0.208984 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 029 | Total loss: 2.211 | Reg loss: 0.030 | Tree loss: 2.211 | Accuracy: 0.171875 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.175781 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.205078 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.220703 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 029 | Total loss: 2.160 | Reg loss: 0.030 | Tree loss: 2.160 | Accuracy: 0.207031 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.203125 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 029 | Total loss: 2.168 | Reg loss: 0.030 | Tree loss: 2.168 | Accuracy: 0.216797 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 029 | Total loss: 2.152 | Reg loss: 0.030 | Tree loss: 2.152 | Accuracy: 0.224609 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 029 | Total loss: 2.116 | Reg loss: 0.030 | Tree loss: 2.116 | Accuracy: 0.238281 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 029 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.189453 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 029 | Total loss: 2.149 | Reg loss: 0.030 | Tree loss: 2.149 | Accuracy: 0.201172 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 029 | Total loss: 2.104 | Reg loss: 0.030 | Tree loss: 2.104 | Accuracy: 0.208984 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 029 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.201172 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 029 | Total loss: 2.143 | Reg loss: 0.031 | Tree loss: 2.143 | Accuracy: 0.220703 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 029 | Total loss: 2.148 | Reg loss: 0.031 | Tree loss: 2.148 | Accuracy: 0.203125 | 0.837 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 029 | Total loss: 2.117 | Reg loss: 0.031 | Tree loss: 2.117 | Accuracy: 0.172414 | 0.837 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 029 | Total loss: 2.261 | Reg loss: 0.030 | Tree loss: 2.261 | Accuracy: 0.189453 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 029 | Total loss: 2.226 | Reg loss: 0.030 | Tree loss: 2.226 | Accuracy: 0.216797 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 029 | Total loss: 2.257 | Reg loss: 0.030 | Tree loss: 2.257 | Accuracy: 0.232422 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 029 | Total loss: 2.261 | Reg loss: 0.030 | Tree loss: 2.261 | Accuracy: 0.169922 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.210938 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 029 | Total loss: 2.289 | Reg loss: 0.030 | Tree loss: 2.289 | Accuracy: 0.189453 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.224609 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.242188 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 029 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.191406 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.193359 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.216797 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 029 | Total loss: 2.265 | Reg loss: 0.030 | Tree loss: 2.265 | Accuracy: 0.183594 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 029 | Total loss: 2.150 | Reg loss: 0.030 | Tree loss: 2.150 | Accuracy: 0.205078 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.197266 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 029 | Total loss: 2.164 | Reg loss: 0.030 | Tree loss: 2.164 | Accuracy: 0.201172 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.205078 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 029 | Total loss: 2.184 | Reg loss: 0.030 | Tree loss: 2.184 | Accuracy: 0.195312 | 0.837 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.169922 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.226562 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 029 | Total loss: 2.136 | Reg loss: 0.030 | Tree loss: 2.136 | Accuracy: 0.203125 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 029 | Total loss: 2.163 | Reg loss: 0.030 | Tree loss: 2.163 | Accuracy: 0.226562 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 029 | Total loss: 2.143 | Reg loss: 0.030 | Tree loss: 2.143 | Accuracy: 0.220703 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 029 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.201172 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.210938 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 029 | Total loss: 2.152 | Reg loss: 0.030 | Tree loss: 2.152 | Accuracy: 0.216797 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.222656 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 029 | Total loss: 2.125 | Reg loss: 0.030 | Tree loss: 2.125 | Accuracy: 0.193359 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 029 | Total loss: 2.135 | Reg loss: 0.031 | Tree loss: 2.135 | Accuracy: 0.193359 | 0.836 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 029 | Total loss: 2.271 | Reg loss: 0.031 | Tree loss: 2.271 | Accuracy: 0.155172 | 0.836 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 029 | Total loss: 2.263 | Reg loss: 0.030 | Tree loss: 2.263 | Accuracy: 0.224609 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 029 | Total loss: 2.249 | Reg loss: 0.030 | Tree loss: 2.249 | Accuracy: 0.201172 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 029 | Total loss: 2.292 | Reg loss: 0.030 | Tree loss: 2.292 | Accuracy: 0.181641 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 029 | Total loss: 2.265 | Reg loss: 0.030 | Tree loss: 2.265 | Accuracy: 0.222656 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 029 | Total loss: 2.239 | Reg loss: 0.030 | Tree loss: 2.239 | Accuracy: 0.193359 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.208984 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.255859 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 029 | Total loss: 2.159 | Reg loss: 0.030 | Tree loss: 2.159 | Accuracy: 0.203125 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.207031 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 029 | Total loss: 2.190 | Reg loss: 0.030 | Tree loss: 2.190 | Accuracy: 0.226562 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.195312 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 029 | Total loss: 2.203 | Reg loss: 0.030 | Tree loss: 2.203 | Accuracy: 0.189453 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.193359 | 0.836 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 013 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.205078 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 029 | Total loss: 2.143 | Reg loss: 0.030 | Tree loss: 2.143 | Accuracy: 0.199219 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 029 | Total loss: 2.180 | Reg loss: 0.030 | Tree loss: 2.180 | Accuracy: 0.222656 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 029 | Total loss: 2.165 | Reg loss: 0.030 | Tree loss: 2.165 | Accuracy: 0.197266 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.189453 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.177734 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 029 | Total loss: 2.139 | Reg loss: 0.030 | Tree loss: 2.139 | Accuracy: 0.191406 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.210938 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 029 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.207031 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 029 | Total loss: 2.122 | Reg loss: 0.030 | Tree loss: 2.122 | Accuracy: 0.203125 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 029 | Total loss: 2.144 | Reg loss: 0.030 | Tree loss: 2.144 | Accuracy: 0.222656 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 029 | Total loss: 2.120 | Reg loss: 0.030 | Tree loss: 2.120 | Accuracy: 0.207031 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 029 | Total loss: 2.123 | Reg loss: 0.030 | Tree loss: 2.123 | Accuracy: 0.208984 | 0.836 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 029 | Total loss: 2.149 | Reg loss: 0.030 | Tree loss: 2.149 | Accuracy: 0.201172 | 0.835 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 029 | Total loss: 2.150 | Reg loss: 0.031 | Tree loss: 2.150 | Accuracy: 0.197266 | 0.835 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 029 | Total loss: 2.189 | Reg loss: 0.031 | Tree loss: 2.189 | Accuracy: 0.189655 | 0.835 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 029 | Total loss: 2.242 | Reg loss: 0.030 | Tree loss: 2.242 | Accuracy: 0.214844 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.205078 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 029 | Total loss: 2.218 | Reg loss: 0.030 | Tree loss: 2.218 | Accuracy: 0.222656 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.232422 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 029 | Total loss: 2.254 | Reg loss: 0.030 | Tree loss: 2.254 | Accuracy: 0.169922 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.191406 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 029 | Total loss: 2.261 | Reg loss: 0.030 | Tree loss: 2.261 | Accuracy: 0.183594 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.207031 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.191406 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 029 | Total loss: 2.223 | Reg loss: 0.030 | Tree loss: 2.223 | Accuracy: 0.199219 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 029 | Total loss: 2.199 | Reg loss: 0.030 | Tree loss: 2.199 | Accuracy: 0.199219 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 029 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.224609 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 029 | Total loss: 2.183 | Reg loss: 0.030 | Tree loss: 2.183 | Accuracy: 0.242188 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 029 | Total loss: 2.173 | Reg loss: 0.030 | Tree loss: 2.173 | Accuracy: 0.226562 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 029 | Total loss: 2.220 | Reg loss: 0.030 | Tree loss: 2.220 | Accuracy: 0.195312 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 029 | Total loss: 2.198 | Reg loss: 0.030 | Tree loss: 2.198 | Accuracy: 0.191406 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.203125 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 029 | Total loss: 2.145 | Reg loss: 0.030 | Tree loss: 2.145 | Accuracy: 0.218750 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 029 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.236328 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 029 | Total loss: 2.128 | Reg loss: 0.030 | Tree loss: 2.128 | Accuracy: 0.218750 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.185547 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 029 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.234375 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 029 | Total loss: 2.143 | Reg loss: 0.030 | Tree loss: 2.143 | Accuracy: 0.214844 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 029 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.179688 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.191406 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 029 | Total loss: 2.148 | Reg loss: 0.030 | Tree loss: 2.148 | Accuracy: 0.193359 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.185547 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.181641 | 0.835 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 029 | Total loss: 2.066 | Reg loss: 0.031 | Tree loss: 2.066 | Accuracy: 0.172414 | 0.835 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 029 | Total loss: 2.261 | Reg loss: 0.030 | Tree loss: 2.261 | Accuracy: 0.195312 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.207031 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 029 | Total loss: 2.292 | Reg loss: 0.030 | Tree loss: 2.292 | Accuracy: 0.212891 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 029 | Total loss: 2.246 | Reg loss: 0.030 | Tree loss: 2.246 | Accuracy: 0.191406 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.230469 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 029 | Total loss: 2.232 | Reg loss: 0.030 | Tree loss: 2.232 | Accuracy: 0.216797 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.224609 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 029 | Total loss: 2.237 | Reg loss: 0.030 | Tree loss: 2.237 | Accuracy: 0.207031 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 029 | Total loss: 2.168 | Reg loss: 0.030 | Tree loss: 2.168 | Accuracy: 0.232422 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 029 | Total loss: 2.197 | Reg loss: 0.030 | Tree loss: 2.197 | Accuracy: 0.181641 | 0.835 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 029 | Total loss: 2.181 | Reg loss: 0.030 | Tree loss: 2.181 | Accuracy: 0.169922 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.173828 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 029 | Total loss: 2.224 | Reg loss: 0.030 | Tree loss: 2.224 | Accuracy: 0.212891 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.199219 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 029 | Total loss: 2.159 | Reg loss: 0.030 | Tree loss: 2.159 | Accuracy: 0.212891 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 029 | Total loss: 2.180 | Reg loss: 0.030 | Tree loss: 2.180 | Accuracy: 0.203125 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 029 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.238281 | 0.834 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 017 / 029 | Total loss: 2.161 | Reg loss: 0.030 | Tree loss: 2.161 | Accuracy: 0.214844 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 029 | Total loss: 2.129 | Reg loss: 0.030 | Tree loss: 2.129 | Accuracy: 0.208984 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 029 | Total loss: 2.124 | Reg loss: 0.030 | Tree loss: 2.124 | Accuracy: 0.244141 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 029 | Total loss: 2.139 | Reg loss: 0.030 | Tree loss: 2.139 | Accuracy: 0.207031 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 029 | Total loss: 2.152 | Reg loss: 0.030 | Tree loss: 2.152 | Accuracy: 0.193359 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.189453 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 029 | Total loss: 2.136 | Reg loss: 0.030 | Tree loss: 2.136 | Accuracy: 0.208984 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 029 | Total loss: 2.181 | Reg loss: 0.030 | Tree loss: 2.181 | Accuracy: 0.201172 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 029 | Total loss: 2.184 | Reg loss: 0.030 | Tree loss: 2.184 | Accuracy: 0.167969 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 029 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.169922 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 029 | Total loss: 2.122 | Reg loss: 0.030 | Tree loss: 2.122 | Accuracy: 0.230469 | 0.834 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 029 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.155172 | 0.834 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 029 | Total loss: 2.285 | Reg loss: 0.030 | Tree loss: 2.285 | Accuracy: 0.212891 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 029 | Total loss: 2.260 | Reg loss: 0.030 | Tree loss: 2.260 | Accuracy: 0.201172 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.208984 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 029 | Total loss: 2.281 | Reg loss: 0.030 | Tree loss: 2.281 | Accuracy: 0.189453 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 029 | Total loss: 2.266 | Reg loss: 0.030 | Tree loss: 2.266 | Accuracy: 0.208984 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 029 | Total loss: 2.190 | Reg loss: 0.030 | Tree loss: 2.190 | Accuracy: 0.218750 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 029 | Total loss: 2.213 | Reg loss: 0.030 | Tree loss: 2.213 | Accuracy: 0.189453 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.191406 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 029 | Total loss: 2.193 | Reg loss: 0.030 | Tree loss: 2.193 | Accuracy: 0.216797 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.205078 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 029 | Total loss: 2.215 | Reg loss: 0.030 | Tree loss: 2.215 | Accuracy: 0.220703 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 029 | Total loss: 2.180 | Reg loss: 0.030 | Tree loss: 2.180 | Accuracy: 0.230469 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 029 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.220703 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.197266 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 029 | Total loss: 2.159 | Reg loss: 0.030 | Tree loss: 2.159 | Accuracy: 0.201172 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.232422 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 029 | Total loss: 2.185 | Reg loss: 0.030 | Tree loss: 2.185 | Accuracy: 0.193359 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 029 | Total loss: 2.181 | Reg loss: 0.030 | Tree loss: 2.181 | Accuracy: 0.183594 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.177734 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 029 | Total loss: 2.185 | Reg loss: 0.030 | Tree loss: 2.185 | Accuracy: 0.171875 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 029 | Total loss: 2.121 | Reg loss: 0.030 | Tree loss: 2.121 | Accuracy: 0.244141 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.203125 | 0.834 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 029 | Total loss: 2.148 | Reg loss: 0.030 | Tree loss: 2.148 | Accuracy: 0.214844 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 029 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.214844 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.203125 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 029 | Total loss: 2.158 | Reg loss: 0.030 | Tree loss: 2.158 | Accuracy: 0.179688 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 029 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.197266 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 029 | Total loss: 2.119 | Reg loss: 0.030 | Tree loss: 2.119 | Accuracy: 0.208984 | 0.833 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 029 | Total loss: 2.065 | Reg loss: 0.030 | Tree loss: 2.065 | Accuracy: 0.258621 | 0.833 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.201172 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 029 | Total loss: 2.240 | Reg loss: 0.030 | Tree loss: 2.240 | Accuracy: 0.189453 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 029 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.236328 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 029 | Total loss: 2.233 | Reg loss: 0.030 | Tree loss: 2.233 | Accuracy: 0.197266 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 029 | Total loss: 2.262 | Reg loss: 0.030 | Tree loss: 2.262 | Accuracy: 0.207031 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 029 | Total loss: 2.229 | Reg loss: 0.030 | Tree loss: 2.229 | Accuracy: 0.205078 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 029 | Total loss: 2.204 | Reg loss: 0.030 | Tree loss: 2.204 | Accuracy: 0.201172 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 029 | Total loss: 2.194 | Reg loss: 0.030 | Tree loss: 2.194 | Accuracy: 0.222656 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 029 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.205078 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.224609 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 029 | Total loss: 2.237 | Reg loss: 0.030 | Tree loss: 2.237 | Accuracy: 0.189453 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 029 | Total loss: 2.187 | Reg loss: 0.030 | Tree loss: 2.187 | Accuracy: 0.197266 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 029 | Total loss: 2.197 | Reg loss: 0.030 | Tree loss: 2.197 | Accuracy: 0.197266 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 029 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.205078 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.216797 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.199219 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 029 | Total loss: 2.139 | Reg loss: 0.030 | Tree loss: 2.139 | Accuracy: 0.185547 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 029 | Total loss: 2.169 | Reg loss: 0.030 | Tree loss: 2.169 | Accuracy: 0.203125 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 029 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.203125 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 029 | Total loss: 2.161 | Reg loss: 0.030 | Tree loss: 2.161 | Accuracy: 0.203125 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.185547 | 0.833 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 021 / 029 | Total loss: 2.144 | Reg loss: 0.030 | Tree loss: 2.144 | Accuracy: 0.212891 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 029 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.244141 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 029 | Total loss: 2.118 | Reg loss: 0.030 | Tree loss: 2.118 | Accuracy: 0.224609 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.179688 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 029 | Total loss: 2.112 | Reg loss: 0.030 | Tree loss: 2.112 | Accuracy: 0.214844 | 0.833 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 029 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.199219 | 0.832 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.185547 | 0.832 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 029 | Total loss: 2.145 | Reg loss: 0.030 | Tree loss: 2.145 | Accuracy: 0.206897 | 0.832 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 029 | Total loss: 2.224 | Reg loss: 0.030 | Tree loss: 2.224 | Accuracy: 0.193359 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 029 | Total loss: 2.278 | Reg loss: 0.030 | Tree loss: 2.278 | Accuracy: 0.181641 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 029 | Total loss: 2.250 | Reg loss: 0.030 | Tree loss: 2.250 | Accuracy: 0.203125 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 029 | Total loss: 2.244 | Reg loss: 0.030 | Tree loss: 2.244 | Accuracy: 0.208984 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 029 | Total loss: 2.215 | Reg loss: 0.030 | Tree loss: 2.215 | Accuracy: 0.195312 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 029 | Total loss: 2.250 | Reg loss: 0.030 | Tree loss: 2.250 | Accuracy: 0.199219 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 029 | Total loss: 2.187 | Reg loss: 0.030 | Tree loss: 2.187 | Accuracy: 0.230469 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 029 | Total loss: 2.246 | Reg loss: 0.030 | Tree loss: 2.246 | Accuracy: 0.189453 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 029 | Total loss: 2.191 | Reg loss: 0.030 | Tree loss: 2.191 | Accuracy: 0.208984 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 029 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.222656 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.214844 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.212891 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.212891 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 029 | Total loss: 2.217 | Reg loss: 0.030 | Tree loss: 2.217 | Accuracy: 0.212891 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 029 | Total loss: 2.176 | Reg loss: 0.030 | Tree loss: 2.176 | Accuracy: 0.205078 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 029 | Total loss: 2.114 | Reg loss: 0.030 | Tree loss: 2.114 | Accuracy: 0.220703 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 029 | Total loss: 2.206 | Reg loss: 0.030 | Tree loss: 2.206 | Accuracy: 0.169922 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 029 | Total loss: 2.188 | Reg loss: 0.030 | Tree loss: 2.188 | Accuracy: 0.210938 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 029 | Total loss: 2.129 | Reg loss: 0.030 | Tree loss: 2.129 | Accuracy: 0.246094 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 029 | Total loss: 2.182 | Reg loss: 0.030 | Tree loss: 2.182 | Accuracy: 0.208984 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.210938 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 029 | Total loss: 2.152 | Reg loss: 0.030 | Tree loss: 2.152 | Accuracy: 0.175781 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.214844 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 029 | Total loss: 2.131 | Reg loss: 0.030 | Tree loss: 2.131 | Accuracy: 0.173828 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 029 | Total loss: 2.132 | Reg loss: 0.030 | Tree loss: 2.132 | Accuracy: 0.236328 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 029 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.169922 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 029 | Total loss: 2.120 | Reg loss: 0.030 | Tree loss: 2.120 | Accuracy: 0.191406 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 029 | Total loss: 2.169 | Reg loss: 0.030 | Tree loss: 2.169 | Accuracy: 0.214844 | 0.832 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 029 | Total loss: 2.114 | Reg loss: 0.030 | Tree loss: 2.114 | Accuracy: 0.241379 | 0.831 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 029 | Total loss: 2.239 | Reg loss: 0.030 | Tree loss: 2.239 | Accuracy: 0.187500 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 029 | Total loss: 2.304 | Reg loss: 0.030 | Tree loss: 2.304 | Accuracy: 0.199219 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 029 | Total loss: 2.248 | Reg loss: 0.030 | Tree loss: 2.248 | Accuracy: 0.181641 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 029 | Total loss: 2.228 | Reg loss: 0.030 | Tree loss: 2.228 | Accuracy: 0.197266 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 029 | Total loss: 2.247 | Reg loss: 0.030 | Tree loss: 2.247 | Accuracy: 0.189453 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.216797 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 029 | Total loss: 2.205 | Reg loss: 0.030 | Tree loss: 2.205 | Accuracy: 0.242188 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 029 | Total loss: 2.224 | Reg loss: 0.030 | Tree loss: 2.224 | Accuracy: 0.212891 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 029 | Total loss: 2.214 | Reg loss: 0.030 | Tree loss: 2.214 | Accuracy: 0.201172 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 029 | Total loss: 2.208 | Reg loss: 0.030 | Tree loss: 2.208 | Accuracy: 0.216797 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 029 | Total loss: 2.205 | Reg loss: 0.030 | Tree loss: 2.205 | Accuracy: 0.193359 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 029 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.222656 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 029 | Total loss: 2.165 | Reg loss: 0.030 | Tree loss: 2.165 | Accuracy: 0.207031 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 029 | Total loss: 2.179 | Reg loss: 0.030 | Tree loss: 2.179 | Accuracy: 0.218750 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.214844 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.195312 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.208984 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 029 | Total loss: 2.154 | Reg loss: 0.030 | Tree loss: 2.154 | Accuracy: 0.203125 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 029 | Total loss: 2.192 | Reg loss: 0.030 | Tree loss: 2.192 | Accuracy: 0.185547 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 029 | Total loss: 2.163 | Reg loss: 0.030 | Tree loss: 2.163 | Accuracy: 0.183594 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.187500 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 029 | Total loss: 2.138 | Reg loss: 0.030 | Tree loss: 2.138 | Accuracy: 0.238281 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 029 | Total loss: 2.133 | Reg loss: 0.030 | Tree loss: 2.133 | Accuracy: 0.193359 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 029 | Total loss: 2.109 | Reg loss: 0.030 | Tree loss: 2.109 | Accuracy: 0.228516 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 029 | Total loss: 2.102 | Reg loss: 0.030 | Tree loss: 2.102 | Accuracy: 0.216797 | 0.831 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 025 / 029 | Total loss: 2.148 | Reg loss: 0.030 | Tree loss: 2.148 | Accuracy: 0.179688 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 029 | Total loss: 2.130 | Reg loss: 0.030 | Tree loss: 2.130 | Accuracy: 0.228516 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 029 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.189453 | 0.831 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 029 | Total loss: 2.106 | Reg loss: 0.030 | Tree loss: 2.106 | Accuracy: 0.206897 | 0.831 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 029 | Total loss: 2.241 | Reg loss: 0.029 | Tree loss: 2.241 | Accuracy: 0.205078 | 0.831 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 029 | Total loss: 2.257 | Reg loss: 0.029 | Tree loss: 2.257 | Accuracy: 0.208984 | 0.831 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 029 | Total loss: 2.225 | Reg loss: 0.029 | Tree loss: 2.225 | Accuracy: 0.193359 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 029 | Total loss: 2.217 | Reg loss: 0.029 | Tree loss: 2.217 | Accuracy: 0.208984 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 029 | Total loss: 2.241 | Reg loss: 0.029 | Tree loss: 2.241 | Accuracy: 0.216797 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 029 | Total loss: 2.236 | Reg loss: 0.030 | Tree loss: 2.236 | Accuracy: 0.203125 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 029 | Total loss: 2.255 | Reg loss: 0.030 | Tree loss: 2.255 | Accuracy: 0.183594 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 029 | Total loss: 2.216 | Reg loss: 0.030 | Tree loss: 2.216 | Accuracy: 0.191406 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 029 | Total loss: 2.212 | Reg loss: 0.030 | Tree loss: 2.212 | Accuracy: 0.171875 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 029 | Total loss: 2.202 | Reg loss: 0.030 | Tree loss: 2.202 | Accuracy: 0.216797 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 029 | Total loss: 2.187 | Reg loss: 0.030 | Tree loss: 2.187 | Accuracy: 0.183594 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.224609 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 029 | Total loss: 2.149 | Reg loss: 0.030 | Tree loss: 2.149 | Accuracy: 0.234375 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 029 | Total loss: 2.180 | Reg loss: 0.030 | Tree loss: 2.180 | Accuracy: 0.193359 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 029 | Total loss: 2.159 | Reg loss: 0.030 | Tree loss: 2.159 | Accuracy: 0.179688 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 029 | Total loss: 2.157 | Reg loss: 0.030 | Tree loss: 2.157 | Accuracy: 0.222656 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 029 | Total loss: 2.140 | Reg loss: 0.030 | Tree loss: 2.140 | Accuracy: 0.234375 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.228516 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.230469 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 029 | Total loss: 2.106 | Reg loss: 0.030 | Tree loss: 2.106 | Accuracy: 0.218750 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 029 | Total loss: 2.168 | Reg loss: 0.030 | Tree loss: 2.168 | Accuracy: 0.207031 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 029 | Total loss: 2.114 | Reg loss: 0.030 | Tree loss: 2.114 | Accuracy: 0.230469 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.228516 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 029 | Total loss: 2.159 | Reg loss: 0.030 | Tree loss: 2.159 | Accuracy: 0.193359 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 029 | Total loss: 2.183 | Reg loss: 0.030 | Tree loss: 2.183 | Accuracy: 0.166016 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 029 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.179688 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 029 | Total loss: 2.170 | Reg loss: 0.030 | Tree loss: 2.170 | Accuracy: 0.205078 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 029 | Total loss: 2.141 | Reg loss: 0.030 | Tree loss: 2.141 | Accuracy: 0.185547 | 0.83 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 029 | Total loss: 2.194 | Reg loss: 0.030 | Tree loss: 2.194 | Accuracy: 0.155172 | 0.83 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 029 | Total loss: 2.273 | Reg loss: 0.029 | Tree loss: 2.273 | Accuracy: 0.212891 | 0.83 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 029 | Total loss: 2.260 | Reg loss: 0.029 | Tree loss: 2.260 | Accuracy: 0.226562 | 0.83 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 029 | Total loss: 2.230 | Reg loss: 0.029 | Tree loss: 2.230 | Accuracy: 0.185547 | 0.83 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 029 | Total loss: 2.264 | Reg loss: 0.029 | Tree loss: 2.264 | Accuracy: 0.173828 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 029 | Total loss: 2.209 | Reg loss: 0.029 | Tree loss: 2.209 | Accuracy: 0.214844 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 029 | Total loss: 2.221 | Reg loss: 0.029 | Tree loss: 2.221 | Accuracy: 0.218750 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 029 | Total loss: 2.174 | Reg loss: 0.029 | Tree loss: 2.174 | Accuracy: 0.212891 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 029 | Total loss: 2.210 | Reg loss: 0.029 | Tree loss: 2.210 | Accuracy: 0.187500 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 029 | Total loss: 2.225 | Reg loss: 0.030 | Tree loss: 2.225 | Accuracy: 0.232422 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 029 | Total loss: 2.189 | Reg loss: 0.030 | Tree loss: 2.189 | Accuracy: 0.197266 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 029 | Total loss: 2.208 | Reg loss: 0.030 | Tree loss: 2.208 | Accuracy: 0.199219 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.216797 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 029 | Total loss: 2.193 | Reg loss: 0.030 | Tree loss: 2.193 | Accuracy: 0.199219 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 029 | Total loss: 2.171 | Reg loss: 0.030 | Tree loss: 2.171 | Accuracy: 0.199219 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 029 | Total loss: 2.165 | Reg loss: 0.030 | Tree loss: 2.165 | Accuracy: 0.210938 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 029 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.179688 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 029 | Total loss: 2.200 | Reg loss: 0.030 | Tree loss: 2.200 | Accuracy: 0.193359 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 029 | Total loss: 2.147 | Reg loss: 0.030 | Tree loss: 2.147 | Accuracy: 0.214844 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 029 | Total loss: 2.176 | Reg loss: 0.030 | Tree loss: 2.176 | Accuracy: 0.218750 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 029 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.193359 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 029 | Total loss: 2.160 | Reg loss: 0.030 | Tree loss: 2.160 | Accuracy: 0.212891 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 029 | Total loss: 2.141 | Reg loss: 0.030 | Tree loss: 2.141 | Accuracy: 0.193359 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 029 | Total loss: 2.146 | Reg loss: 0.030 | Tree loss: 2.146 | Accuracy: 0.166016 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 029 | Total loss: 2.140 | Reg loss: 0.030 | Tree loss: 2.140 | Accuracy: 0.222656 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 029 | Total loss: 2.118 | Reg loss: 0.030 | Tree loss: 2.118 | Accuracy: 0.201172 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 029 | Total loss: 2.084 | Reg loss: 0.030 | Tree loss: 2.084 | Accuracy: 0.242188 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 029 | Total loss: 2.123 | Reg loss: 0.030 | Tree loss: 2.123 | Accuracy: 0.201172 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 029 | Total loss: 2.163 | Reg loss: 0.030 | Tree loss: 2.163 | Accuracy: 0.214844 | 0.829 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 029 | Total loss: 2.055 | Reg loss: 0.030 | Tree loss: 2.055 | Accuracy: 0.189655 | 0.829 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 000 / 029 | Total loss: 2.238 | Reg loss: 0.029 | Tree loss: 2.238 | Accuracy: 0.210938 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 029 | Total loss: 2.290 | Reg loss: 0.029 | Tree loss: 2.290 | Accuracy: 0.177734 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 029 | Total loss: 2.235 | Reg loss: 0.029 | Tree loss: 2.235 | Accuracy: 0.199219 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 029 | Total loss: 2.235 | Reg loss: 0.029 | Tree loss: 2.235 | Accuracy: 0.226562 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 029 | Total loss: 2.217 | Reg loss: 0.029 | Tree loss: 2.217 | Accuracy: 0.199219 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 029 | Total loss: 2.212 | Reg loss: 0.029 | Tree loss: 2.212 | Accuracy: 0.214844 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 029 | Total loss: 2.232 | Reg loss: 0.029 | Tree loss: 2.232 | Accuracy: 0.195312 | 0.829 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 029 | Total loss: 2.207 | Reg loss: 0.029 | Tree loss: 2.207 | Accuracy: 0.208984 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 029 | Total loss: 2.210 | Reg loss: 0.029 | Tree loss: 2.210 | Accuracy: 0.228516 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 029 | Total loss: 2.195 | Reg loss: 0.029 | Tree loss: 2.195 | Accuracy: 0.187500 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 029 | Total loss: 2.166 | Reg loss: 0.030 | Tree loss: 2.166 | Accuracy: 0.224609 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 029 | Total loss: 2.199 | Reg loss: 0.030 | Tree loss: 2.199 | Accuracy: 0.216797 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 029 | Total loss: 2.164 | Reg loss: 0.030 | Tree loss: 2.164 | Accuracy: 0.199219 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 029 | Total loss: 2.186 | Reg loss: 0.030 | Tree loss: 2.186 | Accuracy: 0.199219 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 029 | Total loss: 2.218 | Reg loss: 0.030 | Tree loss: 2.218 | Accuracy: 0.181641 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 029 | Total loss: 2.123 | Reg loss: 0.030 | Tree loss: 2.123 | Accuracy: 0.203125 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 029 | Total loss: 2.195 | Reg loss: 0.030 | Tree loss: 2.195 | Accuracy: 0.197266 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 029 | Total loss: 2.184 | Reg loss: 0.030 | Tree loss: 2.184 | Accuracy: 0.208984 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.199219 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 029 | Total loss: 2.173 | Reg loss: 0.030 | Tree loss: 2.173 | Accuracy: 0.201172 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 029 | Total loss: 2.138 | Reg loss: 0.030 | Tree loss: 2.138 | Accuracy: 0.232422 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 029 | Total loss: 2.114 | Reg loss: 0.030 | Tree loss: 2.114 | Accuracy: 0.250000 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 029 | Total loss: 2.160 | Reg loss: 0.030 | Tree loss: 2.160 | Accuracy: 0.189453 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 029 | Total loss: 2.157 | Reg loss: 0.030 | Tree loss: 2.157 | Accuracy: 0.203125 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 029 | Total loss: 2.081 | Reg loss: 0.030 | Tree loss: 2.081 | Accuracy: 0.207031 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 029 | Total loss: 2.141 | Reg loss: 0.030 | Tree loss: 2.141 | Accuracy: 0.187500 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 029 | Total loss: 2.101 | Reg loss: 0.030 | Tree loss: 2.101 | Accuracy: 0.195312 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 029 | Total loss: 2.173 | Reg loss: 0.030 | Tree loss: 2.173 | Accuracy: 0.191406 | 0.828 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 029 | Total loss: 2.073 | Reg loss: 0.030 | Tree loss: 2.073 | Accuracy: 0.241379 | 0.828 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 029 | Total loss: 2.267 | Reg loss: 0.029 | Tree loss: 2.267 | Accuracy: 0.222656 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 029 | Total loss: 2.215 | Reg loss: 0.029 | Tree loss: 2.215 | Accuracy: 0.187500 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 029 | Total loss: 2.263 | Reg loss: 0.029 | Tree loss: 2.263 | Accuracy: 0.185547 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 029 | Total loss: 2.233 | Reg loss: 0.029 | Tree loss: 2.233 | Accuracy: 0.171875 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 029 | Total loss: 2.230 | Reg loss: 0.029 | Tree loss: 2.230 | Accuracy: 0.208984 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 029 | Total loss: 2.242 | Reg loss: 0.029 | Tree loss: 2.242 | Accuracy: 0.195312 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 029 | Total loss: 2.202 | Reg loss: 0.029 | Tree loss: 2.202 | Accuracy: 0.236328 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 029 | Total loss: 2.244 | Reg loss: 0.029 | Tree loss: 2.244 | Accuracy: 0.208984 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 029 | Total loss: 2.209 | Reg loss: 0.029 | Tree loss: 2.209 | Accuracy: 0.210938 | 0.828 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 029 | Total loss: 2.186 | Reg loss: 0.029 | Tree loss: 2.186 | Accuracy: 0.201172 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 029 | Total loss: 2.171 | Reg loss: 0.029 | Tree loss: 2.171 | Accuracy: 0.244141 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.205078 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 029 | Total loss: 2.234 | Reg loss: 0.030 | Tree loss: 2.234 | Accuracy: 0.201172 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 029 | Total loss: 2.167 | Reg loss: 0.030 | Tree loss: 2.167 | Accuracy: 0.210938 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 029 | Total loss: 2.178 | Reg loss: 0.030 | Tree loss: 2.178 | Accuracy: 0.191406 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 029 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.189453 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 029 | Total loss: 2.177 | Reg loss: 0.030 | Tree loss: 2.177 | Accuracy: 0.199219 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 029 | Total loss: 2.147 | Reg loss: 0.030 | Tree loss: 2.147 | Accuracy: 0.218750 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.208984 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 029 | Total loss: 2.162 | Reg loss: 0.030 | Tree loss: 2.162 | Accuracy: 0.224609 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 029 | Total loss: 2.155 | Reg loss: 0.030 | Tree loss: 2.155 | Accuracy: 0.224609 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 029 | Total loss: 2.138 | Reg loss: 0.030 | Tree loss: 2.138 | Accuracy: 0.197266 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 029 | Total loss: 2.129 | Reg loss: 0.030 | Tree loss: 2.129 | Accuracy: 0.203125 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 029 | Total loss: 2.151 | Reg loss: 0.030 | Tree loss: 2.151 | Accuracy: 0.203125 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 029 | Total loss: 2.127 | Reg loss: 0.030 | Tree loss: 2.127 | Accuracy: 0.177734 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 029 | Total loss: 2.122 | Reg loss: 0.030 | Tree loss: 2.122 | Accuracy: 0.197266 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 029 | Total loss: 2.090 | Reg loss: 0.030 | Tree loss: 2.090 | Accuracy: 0.208984 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 029 | Total loss: 2.146 | Reg loss: 0.030 | Tree loss: 2.146 | Accuracy: 0.203125 | 0.827 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 029 | Total loss: 2.047 | Reg loss: 0.030 | Tree loss: 2.047 | Accuracy: 0.224138 | 0.827 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b4267ee5374f788b1e2e5656e944c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00714649ace04979ba516146254e221e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f007b3794e954632b1310cb0d2630271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb9db44aee04010a94d2c438d5b7d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 9.895196506550219\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 916\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "13420\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "Average comprehensibility: 51.6768558951965\n",
      "std comprehensibility: 4.179429421497326\n",
      "var comprehensibility: 17.467630289277473\n",
      "minimum comprehensibility: 36\n",
      "maximum comprehensibility: 58\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
